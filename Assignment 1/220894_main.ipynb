{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "yap5aeYCxPhd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed 's/,$//' <(curl 'https://drive.google.com/uc?export=download&id=1K1XD1sETaCkO147uzSoqOadWFsFdGs2r') > test_df.csv\n",
        "!sed 's/,$//' <(curl 'https://drive.google.com/uc?export=download&id=1kffWWtIBd-pDeLp3ulJbiiYFpaTSLE2e') > train_df.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTl6do5tgY-7",
        "outputId": "26614bb1-9f4e-41ec-93fb-0b317e539e1d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprcoessing\n"
      ],
      "metadata": {
        "id": "6shble8D1Ba5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "label_encoder_1=preprocessing.LabelEncoder()\n",
        "label_encoder_2=preprocessing.LabelEncoder()\n",
        "label_encoder_3=preprocessing.LabelEncoder()"
      ],
      "metadata": {
        "id": "UG3y3Qod1C0S"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline(dataset):\n",
        "  dataset=dataset.drop(columns=['PassengerId','Name','Cabin','Ticket'])\n",
        "  dataset['Sex']= label_encoder_1.fit_transform(dataset['Sex'])\n",
        "  dataset['Embarked']= label_encoder_2.fit_transform(dataset['Embarked'])\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "1wTyQp4W1Yl8"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=pipeline(train_df)\n",
        "dataset=dataset.fillna(dataset.mean(numeric_only=True))\n"
      ],
      "metadata": {
        "id": "yzy4xDuW26IT"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.astype(float)\n",
        "y=dataset['Survived']\n",
        "X=dataset.drop(columns=['Survived'])\n",
        "X2=X[601:890]\n",
        "y2=y[601:890]\n",
        "X=X[0:600]\n",
        "y=y[0:600]"
      ],
      "metadata": {
        "id": "NKaK07yf3Xr0"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "792F8dNlggi3",
        "outputId": "06e4b7bd-befb-4fe3-f2b8-48ae24c100e7"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(600, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=X.to_numpy() #Feature Matrix(Training Input)\n",
        "y=y.to_numpy() #Training Labels\n",
        "X2=X2.to_numpy()\n",
        "y2=y2.to_numpy()\n",
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn4Xnd0s3jX-",
        "outputId": "0fbed4b5-407f-4ca3-d9df-4bbdc8e9f250"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3.          1.         22.         ...  0.          7.25\n",
            "   2.        ]\n",
            " [ 1.          0.         38.         ...  0.         71.2833\n",
            "   0.        ]\n",
            " [ 3.          0.         26.         ...  0.          7.925\n",
            "   2.        ]\n",
            " ...\n",
            " [ 3.          1.         49.         ...  0.          0.\n",
            "   2.        ]\n",
            " [ 3.          1.         29.69911765 ...  0.          7.225\n",
            "   0.        ]\n",
            " [ 1.          1.         49.         ...  0.         56.9292\n",
            "   0.        ]]\n",
            "[0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1.\n",
            " 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0.\n",
            " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
            " 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
            " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0.\n",
            " 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1.\n",
            " 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
            " 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1.\n",
            " 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0.\n",
            " 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymNeUdxx4b9w",
        "outputId": "d6e144fb-82e0-4598-ac1f-5c32b51fb3ea"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(600, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.reshape(y,(-1,1))\n",
        "y2=np.reshape(y2,(-1,1))"
      ],
      "metadata": {
        "id": "PylkUeCj4hUz"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  z=np.float128(z)\n",
        "  return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "BlSRm5oE4ooG"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_array=[];\n",
        "def cost(y, h):\n",
        "    n=y.shape[0]\n",
        "    cost = (-np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)))/n\n",
        "    return(cost)"
      ],
      "metadata": {
        "id": "SsDd7-r2nEtm"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(w,b):\n",
        "  y2_cap=np.dot(X2,w)+b\n",
        "#  print(b.shape)\n",
        "  h2=sigmoid(y2_cap)\n",
        "  h2=np.where(h2>0.5,1,0)\n",
        "  print(f'val_test cost = {cost(y2,h2)}', end=\" \")\n",
        "  print(f' accuracy = {accuracy(h2,y2)}\\n')\n"
      ],
      "metadata": {
        "id": "RooiS4UL55Zj"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  cost_array=[];"
      ],
      "metadata": {
        "id": "Mjh__0v5-zWP"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(X,y,lr,num_iterations,lamda_reg):\n",
        "  n,d=X.shape\n",
        "  w =np.reshape(np.random.rand(d),(-1,1))\n",
        "  b= np.random.rand()   #d is the numnbe rof columns to ue for predction\n",
        "  for i in range(num_iterations):           #w is basically a coumn vector\n",
        "    print(f\"Iteration Number :{i+1}\\n\")\n",
        "    y_cap=np.dot(X,w)+b # returns essentially a 1 D array each elemte of which is the net y for each passenger\n",
        "    h=sigmoid(y_cap)\n",
        "    gradient=(1/n)*(np.dot(X.T,h-y))\n",
        "    grad2=(1/n)*(np.sum(h-y))\n",
        "    b=b-lr*grad2\n",
        "    w=w-lr*gradient\n",
        "    print(f'train cost = {cost(y,h)}', end=\" \")\n",
        "    print(f'train accuracy = {accuracy(h,y)}\\n')\n",
        "\n",
        "    cost_array.append(cost(y,h))\n",
        "    val(w,b)\n",
        "    print('─' * 50)\n",
        "  return w\n"
      ],
      "metadata": {
        "id": "b-TKt_v345XK"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def accuracy(h,w):\n",
        "#   n=h.shape[0]\n",
        "#   acc=0\n",
        "#   for i in range(n):\n",
        "#     if((h[i]>=0.5 and y[i]==1) or (h[i]<0.5 and y[i]==0)):\n",
        "#       acc+=1\n",
        "#   print(f\"Accuracy= {acc/n}\\n\")"
      ],
      "metadata": {
        "id": "83ki4C-57qPN"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hwQR5vTFz01h"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(h,w):\n",
        "    n=h.shape[0]\n",
        "    acc=0\n",
        "    for i in range(0,n-1):\n",
        "      acc+=np.abs(h[i]-w[i])\n",
        "    return (1-acc/n)"
      ],
      "metadata": {
        "id": "IpcuAHTO-62X"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w=logistic_regression(X,y,0.004,10000,5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOs1M5X-7wZw",
        "outputId": "c1ec5931-a158-409d-840c-847645d83edf"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration Number :1\n",
            "\n",
            "train cost = nan train accuracy = [0.39333319]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.3633218]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :2\n",
            "\n",
            "train cost = nan train accuracy = [0.39333294]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.3633218]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :3\n",
            "\n",
            "train cost = nan train accuracy = [0.39333222]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.3633218]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :4\n",
            "\n",
            "train cost = nan train accuracy = [0.39333007]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.3633218]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :5\n",
            "\n",
            "train cost = nan train accuracy = [0.39332314]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.3633218]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :6\n",
            "\n",
            "train cost = inf train accuracy = [0.39329802]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.3633218]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :7\n",
            "\n",
            "train cost = 9.18687042235674 train accuracy = [0.39317895]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.33564014]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :8\n",
            "\n",
            "train cost = 7.07828351630011 train accuracy = [0.38799833]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.30795848]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9\n",
            "\n",
            "train cost = 5.641262064065739 train accuracy = [0.35646049]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.29065744]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :10\n",
            "\n",
            "train cost = 4.5856021282947355 train accuracy = [0.33836976]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.27681661]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :11\n",
            "\n",
            "train cost = 3.691994136155335 train accuracy = [0.32882022]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.26643599]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :12\n",
            "\n",
            "train cost = 2.9525292742907685 train accuracy = [0.33425196]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.23875433]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :13\n",
            "\n",
            "train cost = 2.368438115538812 train accuracy = [0.34917691]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.24913495]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :14\n",
            "\n",
            "train cost = 1.928865332200623 train accuracy = [0.37011354]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.28719723]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :15\n",
            "\n",
            "train cost = 1.602976033376869 train accuracy = [0.39453983]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.33217993]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :16\n",
            "\n",
            "train cost = 1.417754164468653 train accuracy = [0.42177876]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.35986159]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :17\n",
            "\n",
            "train cost = 1.3441727492761726 train accuracy = [0.44408567]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.39792388]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :18\n",
            "\n",
            "train cost = 1.3153198683663188 train accuracy = [0.45785377]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.42906574]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :19\n",
            "\n",
            "train cost = 1.3032437989043846 train accuracy = [0.46717489]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.44290657]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :20\n",
            "\n",
            "train cost = 1.2971127135810232 train accuracy = [0.47245858]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.44636678]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :21\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-87-cf4d8d6f18a6>:4: RuntimeWarning: divide by zero encountered in log\n",
            "  cost = (-np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)))/n\n",
            "<ipython-input-87-cf4d8d6f18a6>:4: RuntimeWarning: invalid value encountered in multiply\n",
            "  cost = (-np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)))/n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9287\n",
            "\n",
            "train cost = 0.5128730468333902 train accuracy = [0.65211272]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9288\n",
            "\n",
            "train cost = 0.5099201083659646 train accuracy = [0.65627567]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9289\n",
            "\n",
            "train cost = 0.512860091048138 train accuracy = [0.65212255]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9290\n",
            "\n",
            "train cost = 0.5099092713240982 train accuracy = [0.6562847]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9291\n",
            "\n",
            "train cost = 0.5128471416514858 train accuracy = [0.65213237]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9292\n",
            "\n",
            "train cost = 0.5098984392926408 train accuracy = [0.65629373]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9293\n",
            "\n",
            "train cost = 0.5128341986395591 train accuracy = [0.65214219]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9294\n",
            "\n",
            "train cost = 0.5098876122684917 train accuracy = [0.65630276]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9295\n",
            "\n",
            "train cost = 0.5128212620084862 train accuracy = [0.652152]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9296\n",
            "\n",
            "train cost = 0.5098767902485525 train accuracy = [0.65631179]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9297\n",
            "\n",
            "train cost = 0.5128083317543972 train accuracy = [0.65216182]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9298\n",
            "\n",
            "train cost = 0.5098659732297269 train accuracy = [0.65632081]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9299\n",
            "\n",
            "train cost = 0.512795407873425 train accuracy = [0.65217162]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9300\n",
            "\n",
            "train cost = 0.509855161208921 train accuracy = [0.65632983]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9301\n",
            "\n",
            "train cost = 0.512782490361705 train accuracy = [0.65218143]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9302\n",
            "\n",
            "train cost = 0.5098443541830426 train accuracy = [0.65633884]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9303\n",
            "\n",
            "train cost = 0.5127695792153749 train accuracy = [0.65219123]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9304\n",
            "\n",
            "train cost = 0.509833552149002 train accuracy = [0.65634786]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9305\n",
            "\n",
            "train cost = 0.5127566744305749 train accuracy = [0.65220103]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9306\n",
            "\n",
            "train cost = 0.5098227551037111 train accuracy = [0.65635687]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9307\n",
            "\n",
            "train cost = 0.5127437760034477 train accuracy = [0.65221082]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9308\n",
            "\n",
            "train cost = 0.5098119630440845 train accuracy = [0.65636587]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9309\n",
            "\n",
            "train cost = 0.5127308839301381 train accuracy = [0.65222061]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9310\n",
            "\n",
            "train cost = 0.5098011759670388 train accuracy = [0.65637488]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9311\n",
            "\n",
            "train cost = 0.5127179982067938 train accuracy = [0.6522304]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9312\n",
            "\n",
            "train cost = 0.5097903938694924 train accuracy = [0.65638388]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9313\n",
            "\n",
            "train cost = 0.5127051188295645 train accuracy = [0.65224019]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9314\n",
            "\n",
            "train cost = 0.5097796167483662 train accuracy = [0.65639287]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9315\n",
            "\n",
            "train cost = 0.5126922457946027 train accuracy = [0.65224997]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9316\n",
            "\n",
            "train cost = 0.5097688446005831 train accuracy = [0.65640187]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9317\n",
            "\n",
            "train cost = 0.5126793790980632 train accuracy = [0.65225975]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9318\n",
            "\n",
            "train cost = 0.5097580774230679 train accuracy = [0.65641086]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9319\n",
            "\n",
            "train cost = 0.5126665187361029 train accuracy = [0.65226952]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9320\n",
            "\n",
            "train cost = 0.5097473152127479 train accuracy = [0.65641985]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9321\n",
            "\n",
            "train cost = 0.5126536647048817 train accuracy = [0.65227929]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9322\n",
            "\n",
            "train cost = 0.5097365579665522 train accuracy = [0.65642883]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9323\n",
            "\n",
            "train cost = 0.5126408170005616 train accuracy = [0.65228906]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9324\n",
            "\n",
            "train cost = 0.509725805681412 train accuracy = [0.65643781]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9325\n",
            "\n",
            "train cost = 0.512627975619307 train accuracy = [0.65229883]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9326\n",
            "\n",
            "train cost = 0.509715058354261 train accuracy = [0.65644679]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9327\n",
            "\n",
            "train cost = 0.5126151405572846 train accuracy = [0.65230859]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9328\n",
            "\n",
            "train cost = 0.5097043159820346 train accuracy = [0.65645577]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9329\n",
            "\n",
            "train cost = 0.512602311810664 train accuracy = [0.65231835]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9330\n",
            "\n",
            "train cost = 0.5096935785616705 train accuracy = [0.65646474]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9331\n",
            "\n",
            "train cost = 0.5125894893756167 train accuracy = [0.6523281]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9332\n",
            "\n",
            "train cost = 0.5096828460901085 train accuracy = [0.65647371]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9333\n",
            "\n",
            "train cost = 0.5125766732483168 train accuracy = [0.65233785]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9334\n",
            "\n",
            "train cost = 0.5096721185642904 train accuracy = [0.65648268]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9335\n",
            "\n",
            "train cost = 0.512563863424941 train accuracy = [0.6523476]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9336\n",
            "\n",
            "train cost = 0.5096613959811602 train accuracy = [0.65649164]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9337\n",
            "\n",
            "train cost = 0.512551059901668 train accuracy = [0.65235734]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9338\n",
            "\n",
            "train cost = 0.509650678337664 train accuracy = [0.6565006]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9339\n",
            "\n",
            "train cost = 0.5125382626746795 train accuracy = [0.65236709]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9340\n",
            "\n",
            "train cost = 0.50963996563075 train accuracy = [0.65650956]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9341\n",
            "\n",
            "train cost = 0.512525471740159 train accuracy = [0.65237682]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9342\n",
            "\n",
            "train cost = 0.5096292578573685 train accuracy = [0.65651851]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9343\n",
            "\n",
            "train cost = 0.5125126870942929 train accuracy = [0.65238656]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9344\n",
            "\n",
            "train cost = 0.5096185550144718 train accuracy = [0.65652747]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9345\n",
            "\n",
            "train cost = 0.5124999087332696 train accuracy = [0.65239629]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9346\n",
            "\n",
            "train cost = 0.5096078570990146 train accuracy = [0.65653641]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9347\n",
            "\n",
            "train cost = 0.51248713665328 train accuracy = [0.65240602]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9348\n",
            "\n",
            "train cost = 0.5095971641079532 train accuracy = [0.65654536]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9349\n",
            "\n",
            "train cost = 0.5124743708505178 train accuracy = [0.65241574]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9350\n",
            "\n",
            "train cost = 0.5095864760382464 train accuracy = [0.6565543]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9351\n",
            "\n",
            "train cost = 0.5124616113211786 train accuracy = [0.65242547]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9352\n",
            "\n",
            "train cost = 0.5095757928868551 train accuracy = [0.65656324]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9353\n",
            "\n",
            "train cost = 0.5124488580614607 train accuracy = [0.65243518]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9354\n",
            "\n",
            "train cost = 0.5095651146507418 train accuracy = [0.65657218]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9355\n",
            "\n",
            "train cost = 0.5124361110675645 train accuracy = [0.6524449]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9356\n",
            "\n",
            "train cost = 0.5095544413268719 train accuracy = [0.65658111]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9357\n",
            "\n",
            "train cost = 0.5124233703356932 train accuracy = [0.65245461]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9358\n",
            "\n",
            "train cost = 0.5095437729122121 train accuracy = [0.65659004]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9359\n",
            "\n",
            "train cost = 0.5124106358620522 train accuracy = [0.65246432]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9360\n",
            "\n",
            "train cost = 0.5095331094037316 train accuracy = [0.65659897]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9361\n",
            "\n",
            "train cost = 0.512397907642849 train accuracy = [0.65247402]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9362\n",
            "\n",
            "train cost = 0.5095224507984017 train accuracy = [0.65660789]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9363\n",
            "\n",
            "train cost = 0.5123851856742941 train accuracy = [0.65248373]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9364\n",
            "\n",
            "train cost = 0.5095117970931958 train accuracy = [0.65661681]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9365\n",
            "\n",
            "train cost = 0.5123724699526 train accuracy = [0.65249342]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9366\n",
            "\n",
            "train cost = 0.5095011482850891 train accuracy = [0.65662573]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9367\n",
            "\n",
            "train cost = 0.5123597604739817 train accuracy = [0.65250312]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9368\n",
            "\n",
            "train cost = 0.509490504371059 train accuracy = [0.65663465]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9369\n",
            "\n",
            "train cost = 0.5123470572346563 train accuracy = [0.65251281]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9370\n",
            "\n",
            "train cost = 0.5094798653480852 train accuracy = [0.65664356]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9371\n",
            "\n",
            "train cost = 0.5123343602308439 train accuracy = [0.6525225]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9372\n",
            "\n",
            "train cost = 0.5094692312131492 train accuracy = [0.65665247]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9373\n",
            "\n",
            "train cost = 0.5123216694587664 train accuracy = [0.65253218]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9374\n",
            "\n",
            "train cost = 0.5094586019632349 train accuracy = [0.65666137]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9375\n",
            "\n",
            "train cost = 0.5123089849146484 train accuracy = [0.65254187]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9376\n",
            "\n",
            "train cost = 0.5094479775953279 train accuracy = [0.65667027]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9377\n",
            "\n",
            "train cost = 0.5122963065947168 train accuracy = [0.65255154]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9378\n",
            "\n",
            "train cost = 0.5094373581064161 train accuracy = [0.65667917]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9379\n",
            "\n",
            "train cost = 0.5122836344952009 train accuracy = [0.65256122]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9380\n",
            "\n",
            "train cost = 0.5094267434934894 train accuracy = [0.65668807]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9381\n",
            "\n",
            "train cost = 0.5122709686123322 train accuracy = [0.65257089]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9382\n",
            "\n",
            "train cost = 0.5094161337535398 train accuracy = [0.65669696]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9383\n",
            "\n",
            "train cost = 0.512258308942345 train accuracy = [0.65258056]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9384\n",
            "\n",
            "train cost = 0.5094055288835614 train accuracy = [0.65670586]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9385\n",
            "\n",
            "train cost = 0.5122456554814756 train accuracy = [0.65259023]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9386\n",
            "\n",
            "train cost = 0.5093949288805504 train accuracy = [0.65671474]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9387\n",
            "\n",
            "train cost = 0.5122330082259628 train accuracy = [0.65259989]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9388\n",
            "\n",
            "train cost = 0.5093843337415047 train accuracy = [0.65672363]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9389\n",
            "\n",
            "train cost = 0.5122203671720478 train accuracy = [0.65260955]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9390\n",
            "\n",
            "train cost = 0.509373743463425 train accuracy = [0.65673251]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9391\n",
            "\n",
            "train cost = 0.512207732315974 train accuracy = [0.6526192]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9392\n",
            "\n",
            "train cost = 0.5093631580433133 train accuracy = [0.65674139]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9393\n",
            "\n",
            "train cost = 0.5121951036539876 train accuracy = [0.65262886]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9394\n",
            "\n",
            "train cost = 0.5093525774781742 train accuracy = [0.65675026]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9395\n",
            "\n",
            "train cost = 0.5121824811823367 train accuracy = [0.6526385]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9396\n",
            "\n",
            "train cost = 0.509342001765014 train accuracy = [0.65675913]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9397\n",
            "\n",
            "train cost = 0.512169864897272 train accuracy = [0.65264815]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9398\n",
            "\n",
            "train cost = 0.5093314309008413 train accuracy = [0.656768]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9399\n",
            "\n",
            "train cost = 0.5121572547950466 train accuracy = [0.65265779]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9400\n",
            "\n",
            "train cost = 0.5093208648826667 train accuracy = [0.65677687]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9401\n",
            "\n",
            "train cost = 0.5121446508719159 train accuracy = [0.65266743]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9402\n",
            "\n",
            "train cost = 0.5093103037075027 train accuracy = [0.65678573]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9403\n",
            "\n",
            "train cost = 0.5121320531241375 train accuracy = [0.65267707]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9404\n",
            "\n",
            "train cost = 0.5092997473723643 train accuracy = [0.65679459]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9405\n",
            "\n",
            "train cost = 0.5121194615479716 train accuracy = [0.6526867]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9406\n",
            "\n",
            "train cost = 0.5092891958742678 train accuracy = [0.65680345]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9407\n",
            "\n",
            "train cost = 0.5121068761396809 train accuracy = [0.65269633]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9408\n",
            "\n",
            "train cost = 0.5092786492102324 train accuracy = [0.65681231]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9409\n",
            "\n",
            "train cost = 0.51209429689553 train accuracy = [0.65270596]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9410\n",
            "\n",
            "train cost = 0.5092681073772786 train accuracy = [0.65682116]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9411\n",
            "\n",
            "train cost = 0.5120817238117862 train accuracy = [0.65271558]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9412\n",
            "\n",
            "train cost = 0.5092575703724295 train accuracy = [0.65683001]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9413\n",
            "\n",
            "train cost = 0.5120691568847191 train accuracy = [0.6527252]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9414\n",
            "\n",
            "train cost = 0.5092470381927101 train accuracy = [0.65683885]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9415\n",
            "\n",
            "train cost = 0.5120565961106006 train accuracy = [0.65273481]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9416\n",
            "\n",
            "train cost = 0.5092365108351472 train accuracy = [0.65684769]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9417\n",
            "\n",
            "train cost = 0.512044041485705 train accuracy = [0.65274443]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9418\n",
            "\n",
            "train cost = 0.50922598829677 train accuracy = [0.65685653]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9419\n",
            "\n",
            "train cost = 0.512031493006309 train accuracy = [0.65275404]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9420\n",
            "\n",
            "train cost = 0.5092154705746096 train accuracy = [0.65686537]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9421\n",
            "\n",
            "train cost = 0.5120189506686916 train accuracy = [0.65276364]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9422\n",
            "\n",
            "train cost = 0.509204957665699 train accuracy = [0.6568742]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9423\n",
            "\n",
            "train cost = 0.512006414469134 train accuracy = [0.65277325]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9424\n",
            "\n",
            "train cost = 0.5091944495670734 train accuracy = [0.65688303]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9425\n",
            "\n",
            "train cost = 0.51199388440392 train accuracy = [0.65278285]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9426\n",
            "\n",
            "train cost = 0.50918394627577 train accuracy = [0.65689186]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9427\n",
            "\n",
            "train cost = 0.5119813604693356 train accuracy = [0.65279244]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9428\n",
            "\n",
            "train cost = 0.5091734477888282 train accuracy = [0.65690068]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9429\n",
            "\n",
            "train cost = 0.5119688426616693 train accuracy = [0.65280204]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9430\n",
            "\n",
            "train cost = 0.509162954103289 train accuracy = [0.6569095]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9431\n",
            "\n",
            "train cost = 0.5119563309772117 train accuracy = [0.65281163]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9432\n",
            "\n",
            "train cost = 0.5091524652161961 train accuracy = [0.65691832]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9433\n",
            "\n",
            "train cost = 0.511943825412256 train accuracy = [0.65282122]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9434\n",
            "\n",
            "train cost = 0.5091419811245945 train accuracy = [0.65692714]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9435\n",
            "\n",
            "train cost = 0.5119313259630976 train accuracy = [0.6528308]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9436\n",
            "\n",
            "train cost = 0.5091315018255318 train accuracy = [0.65693595]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9437\n",
            "\n",
            "train cost = 0.5119188326260343 train accuracy = [0.65284038]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9438\n",
            "\n",
            "train cost = 0.5091210273160574 train accuracy = [0.65694476]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9439\n",
            "\n",
            "train cost = 0.5119063453973661 train accuracy = [0.65284996]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9440\n",
            "\n",
            "train cost = 0.5091105575932225 train accuracy = [0.65695356]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9441\n",
            "\n",
            "train cost = 0.5118938642733954 train accuracy = [0.65285953]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9442\n",
            "\n",
            "train cost = 0.509100092654081 train accuracy = [0.65696237]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9443\n",
            "\n",
            "train cost = 0.5118813892504273 train accuracy = [0.6528691]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9444\n",
            "\n",
            "train cost = 0.5090896324956881 train accuracy = [0.65697117]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9445\n",
            "\n",
            "train cost = 0.5118689203247686 train accuracy = [0.65287867]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9446\n",
            "\n",
            "train cost = 0.5090791771151014 train accuracy = [0.65697997]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9447\n",
            "\n",
            "train cost = 0.511856457492729 train accuracy = [0.65288823]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9448\n",
            "\n",
            "train cost = 0.5090687265093805 train accuracy = [0.65698876]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9449\n",
            "\n",
            "train cost = 0.5118440007506201 train accuracy = [0.65289779]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9450\n",
            "\n",
            "train cost = 0.5090582806755869 train accuracy = [0.65699755]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9451\n",
            "\n",
            "train cost = 0.5118315500947561 train accuracy = [0.65290735]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9452\n",
            "\n",
            "train cost = 0.5090478396107844 train accuracy = [0.65700634]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9453\n",
            "\n",
            "train cost = 0.5118191055214535 train accuracy = [0.65291691]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9454\n",
            "\n",
            "train cost = 0.5090374033120383 train accuracy = [0.65701512]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9455\n",
            "\n",
            "train cost = 0.511806667027031 train accuracy = [0.65292646]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9456\n",
            "\n",
            "train cost = 0.5090269717764165 train accuracy = [0.65702391]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9457\n",
            "\n",
            "train cost = 0.5117942346078098 train accuracy = [0.65293601]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9458\n",
            "\n",
            "train cost = 0.5090165450009886 train accuracy = [0.65703268]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9459\n",
            "\n",
            "train cost = 0.5117818082601133 train accuracy = [0.65294555]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9460\n",
            "\n",
            "train cost = 0.5090061229828262 train accuracy = [0.65704146]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9461\n",
            "\n",
            "train cost = 0.5117693879802673 train accuracy = [0.65295509]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9462\n",
            "\n",
            "train cost = 0.5089957057190032 train accuracy = [0.65705023]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9463\n",
            "\n",
            "train cost = 0.5117569737645998 train accuracy = [0.65296463]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9464\n",
            "\n",
            "train cost = 0.508985293206595 train accuracy = [0.657059]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9465\n",
            "\n",
            "train cost = 0.5117445656094414 train accuracy = [0.65297417]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9466\n",
            "\n",
            "train cost = 0.5089748854426795 train accuracy = [0.65706777]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9467\n",
            "\n",
            "train cost = 0.5117321635111245 train accuracy = [0.6529837]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9468\n",
            "\n",
            "train cost = 0.5089644824243365 train accuracy = [0.65707654]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9469\n",
            "\n",
            "train cost = 0.5117197674659845 train accuracy = [0.65299323]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9470\n",
            "\n",
            "train cost = 0.5089540841486474 train accuracy = [0.6570853]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9471\n",
            "\n",
            "train cost = 0.5117073774703587 train accuracy = [0.65300275]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9472\n",
            "\n",
            "train cost = 0.5089436906126963 train accuracy = [0.65709406]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9473\n",
            "\n",
            "train cost = 0.5116949935205866 train accuracy = [0.65301227]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9474\n",
            "\n",
            "train cost = 0.5089333018135688 train accuracy = [0.65710281]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9475\n",
            "\n",
            "train cost = 0.5116826156130103 train accuracy = [0.65302179]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9476\n",
            "\n",
            "train cost = 0.5089229177483526 train accuracy = [0.65711156]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9477\n",
            "\n",
            "train cost = 0.5116702437439742 train accuracy = [0.65303131]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9478\n",
            "\n",
            "train cost = 0.5089125384141376 train accuracy = [0.65712031]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9479\n",
            "\n",
            "train cost = 0.5116578779098249 train accuracy = [0.65304082]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9480\n",
            "\n",
            "train cost = 0.5089021638080156 train accuracy = [0.65712906]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9481\n",
            "\n",
            "train cost = 0.5116455181069113 train accuracy = [0.65305033]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9482\n",
            "\n",
            "train cost = 0.5088917939270801 train accuracy = [0.6571378]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9483\n",
            "\n",
            "train cost = 0.5116331643315846 train accuracy = [0.65305984]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9484\n",
            "\n",
            "train cost = 0.5088814287684269 train accuracy = [0.65714654]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9485\n",
            "\n",
            "train cost = 0.5116208165801985 train accuracy = [0.65306934]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9486\n",
            "\n",
            "train cost = 0.508871068329154 train accuracy = [0.65715528]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9487\n",
            "\n",
            "train cost = 0.5116084748491088 train accuracy = [0.65307884]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9488\n",
            "\n",
            "train cost = 0.508860712606361 train accuracy = [0.65716401]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9489\n",
            "\n",
            "train cost = 0.5115961391346737 train accuracy = [0.65308833]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9490\n",
            "\n",
            "train cost = 0.5088503615971496 train accuracy = [0.65717275]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9491\n",
            "\n",
            "train cost = 0.5115838094332537 train accuracy = [0.65309783]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9492\n",
            "\n",
            "train cost = 0.5088400152986237 train accuracy = [0.65718147]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9493\n",
            "\n",
            "train cost = 0.5115714857412114 train accuracy = [0.65310732]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9494\n",
            "\n",
            "train cost = 0.5088296737078889 train accuracy = [0.6571902]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9495\n",
            "\n",
            "train cost = 0.5115591680549121 train accuracy = [0.6531168]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9496\n",
            "\n",
            "train cost = 0.5088193368220529 train accuracy = [0.65719892]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9497\n",
            "\n",
            "train cost = 0.5115468563707232 train accuracy = [0.65312629]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9498\n",
            "\n",
            "train cost = 0.5088090046382255 train accuracy = [0.65720764]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9499\n",
            "\n",
            "train cost = 0.5115345506850142 train accuracy = [0.65313577]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9500\n",
            "\n",
            "train cost = 0.5087986771535185 train accuracy = [0.65721636]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9501\n",
            "\n",
            "train cost = 0.5115222509941572 train accuracy = [0.65314525]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9502\n",
            "\n",
            "train cost = 0.5087883543650453 train accuracy = [0.65722507]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9503\n",
            "\n",
            "train cost = 0.5115099572945264 train accuracy = [0.65315472]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9504\n",
            "\n",
            "train cost = 0.5087780362699218 train accuracy = [0.65723378]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9505\n",
            "\n",
            "train cost = 0.5114976695824985 train accuracy = [0.65316419]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9506\n",
            "\n",
            "train cost = 0.5087677228652656 train accuracy = [0.65724249]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9507\n",
            "\n",
            "train cost = 0.5114853878544523 train accuracy = [0.65317366]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9508\n",
            "\n",
            "train cost = 0.5087574141481964 train accuracy = [0.6572512]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9509\n",
            "\n",
            "train cost = 0.511473112106769 train accuracy = [0.65318312]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9510\n",
            "\n",
            "train cost = 0.5087471101158357 train accuracy = [0.6572599]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9511\n",
            "\n",
            "train cost = 0.511460842335832 train accuracy = [0.65319258]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9512\n",
            "\n",
            "train cost = 0.5087368107653073 train accuracy = [0.6572686]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9513\n",
            "\n",
            "train cost = 0.511448578538027 train accuracy = [0.65320204]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9514\n",
            "\n",
            "train cost = 0.5087265160937366 train accuracy = [0.65727729]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9515\n",
            "\n",
            "train cost = 0.5114363207097421 train accuracy = [0.6532115]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9516\n",
            "\n",
            "train cost = 0.5087162260982512 train accuracy = [0.65728599]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9517\n",
            "\n",
            "train cost = 0.5114240688473676 train accuracy = [0.65322095]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9518\n",
            "\n",
            "train cost = 0.5087059407759806 train accuracy = [0.65729468]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9519\n",
            "\n",
            "train cost = 0.5114118229472961 train accuracy = [0.6532304]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9520\n",
            "\n",
            "train cost = 0.5086956601240565 train accuracy = [0.65730336]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9521\n",
            "\n",
            "train cost = 0.5113995830059225 train accuracy = [0.65323984]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9522\n",
            "\n",
            "train cost = 0.5086853841396122 train accuracy = [0.65731205]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9523\n",
            "\n",
            "train cost = 0.5113873490196439 train accuracy = [0.65324928]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9524\n",
            "\n",
            "train cost = 0.5086751128197831 train accuracy = [0.65732073]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9525\n",
            "\n",
            "train cost = 0.5113751209848598 train accuracy = [0.65325872]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9526\n",
            "\n",
            "train cost = 0.5086648461617068 train accuracy = [0.65732941]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9527\n",
            "\n",
            "train cost = 0.511362898897972 train accuracy = [0.65326816]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9528\n",
            "\n",
            "train cost = 0.5086545841625226 train accuracy = [0.65733808]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9529\n",
            "\n",
            "train cost = 0.5113506827553844 train accuracy = [0.65327759]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9530\n",
            "\n",
            "train cost = 0.5086443268193719 train accuracy = [0.65734675]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9531\n",
            "\n",
            "train cost = 0.5113384725535033 train accuracy = [0.65328702]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9532\n",
            "\n",
            "train cost = 0.508634074129398 train accuracy = [0.65735542]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9533\n",
            "\n",
            "train cost = 0.5113262682887373 train accuracy = [0.65329645]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9534\n",
            "\n",
            "train cost = 0.5086238260897461 train accuracy = [0.65736409]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9535\n",
            "\n",
            "train cost = 0.5113140699574973 train accuracy = [0.65330587]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9536\n",
            "\n",
            "train cost = 0.5086135826975636 train accuracy = [0.65737275]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9537\n",
            "\n",
            "train cost = 0.5113018775561962 train accuracy = [0.65331529]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9538\n",
            "\n",
            "train cost = 0.5086033439499997 train accuracy = [0.65738141]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9539\n",
            "\n",
            "train cost = 0.5112896910812494 train accuracy = [0.6533247]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9540\n",
            "\n",
            "train cost = 0.5085931098442055 train accuracy = [0.65739007]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9541\n",
            "\n",
            "train cost = 0.5112775105290749 train accuracy = [0.65333412]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9542\n",
            "\n",
            "train cost = 0.5085828803773342 train accuracy = [0.65739873]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9543\n",
            "\n",
            "train cost = 0.5112653358960922 train accuracy = [0.65334353]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9544\n",
            "\n",
            "train cost = 0.5085726555465409 train accuracy = [0.65740738]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9545\n",
            "\n",
            "train cost = 0.5112531671787237 train accuracy = [0.65335294]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9546\n",
            "\n",
            "train cost = 0.5085624353489825 train accuracy = [0.65741603]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9547\n",
            "\n",
            "train cost = 0.5112410043733939 train accuracy = [0.65336234]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9548\n",
            "\n",
            "train cost = 0.5085522197818183 train accuracy = [0.65742467]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9549\n",
            "\n",
            "train cost = 0.5112288474765293 train accuracy = [0.65337174]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9550\n",
            "\n",
            "train cost = 0.5085420088422089 train accuracy = [0.65743331]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9551\n",
            "\n",
            "train cost = 0.5112166964845591 train accuracy = [0.65338114]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9552\n",
            "\n",
            "train cost = 0.5085318025273176 train accuracy = [0.65744195]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9553\n",
            "\n",
            "train cost = 0.5112045513939144 train accuracy = [0.65339053]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9554\n",
            "\n",
            "train cost = 0.5085216008343089 train accuracy = [0.65745059]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9555\n",
            "\n",
            "train cost = 0.5111924122010288 train accuracy = [0.65339992]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9556\n",
            "\n",
            "train cost = 0.5085114037603499 train accuracy = [0.65745923]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9557\n",
            "\n",
            "train cost = 0.5111802789023381 train accuracy = [0.65340931]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9558\n",
            "\n",
            "train cost = 0.5085012113026091 train accuracy = [0.65746786]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9559\n",
            "\n",
            "train cost = 0.5111681514942801 train accuracy = [0.6534187]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9560\n",
            "\n",
            "train cost = 0.5084910234582574 train accuracy = [0.65747648]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9561\n",
            "\n",
            "train cost = 0.5111560299732953 train accuracy = [0.65342808]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9562\n",
            "\n",
            "train cost = 0.5084808402244674 train accuracy = [0.65748511]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9563\n",
            "\n",
            "train cost = 0.5111439143358262 train accuracy = [0.65343746]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9564\n",
            "\n",
            "train cost = 0.5084706615984137 train accuracy = [0.65749373]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9565\n",
            "\n",
            "train cost = 0.5111318045783175 train accuracy = [0.65344683]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9566\n",
            "\n",
            "train cost = 0.5084604875772729 train accuracy = [0.65750235]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9567\n",
            "\n",
            "train cost = 0.5111197006972164 train accuracy = [0.6534562]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9568\n",
            "\n",
            "train cost = 0.5084503181582233 train accuracy = [0.65751097]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9569\n",
            "\n",
            "train cost = 0.5111076026889719 train accuracy = [0.65346557]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9570\n",
            "\n",
            "train cost = 0.5084401533384455 train accuracy = [0.65751958]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9571\n",
            "\n",
            "train cost = 0.511095510550036 train accuracy = [0.65347494]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9572\n",
            "\n",
            "train cost = 0.5084299931151218 train accuracy = [0.65752819]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9573\n",
            "\n",
            "train cost = 0.511083424276862 train accuracy = [0.6534843]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9574\n",
            "\n",
            "train cost = 0.5084198374854365 train accuracy = [0.6575368]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9575\n",
            "\n",
            "train cost = 0.5110713438659064 train accuracy = [0.65349366]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9576\n",
            "\n",
            "train cost = 0.5084096864465759 train accuracy = [0.6575454]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9577\n",
            "\n",
            "train cost = 0.511059269313627 train accuracy = [0.65350302]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9578\n",
            "\n",
            "train cost = 0.5083995399957281 train accuracy = [0.65755401]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9579\n",
            "\n",
            "train cost = 0.5110472006164848 train accuracy = [0.65351237]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9580\n",
            "\n",
            "train cost = 0.5083893981300831 train accuracy = [0.6575626]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9581\n",
            "\n",
            "train cost = 0.5110351377709422 train accuracy = [0.65352172]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9582\n",
            "\n",
            "train cost = 0.508379260846833 train accuracy = [0.6575712]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9583\n",
            "\n",
            "train cost = 0.5110230807734645 train accuracy = [0.65353107]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9584\n",
            "\n",
            "train cost = 0.5083691281431719 train accuracy = [0.65757979]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9585\n",
            "\n",
            "train cost = 0.5110110296205188 train accuracy = [0.65354041]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9586\n",
            "\n",
            "train cost = 0.5083590000162956 train accuracy = [0.65758838]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9587\n",
            "\n",
            "train cost = 0.5109989843085747 train accuracy = [0.65354975]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9588\n",
            "\n",
            "train cost = 0.508348876463402 train accuracy = [0.65759697]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9589\n",
            "\n",
            "train cost = 0.5109869448341038 train accuracy = [0.65355909]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9590\n",
            "\n",
            "train cost = 0.5083387574816907 train accuracy = [0.65760556]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9591\n",
            "\n",
            "train cost = 0.5109749111935802 train accuracy = [0.65356842]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9592\n",
            "\n",
            "train cost = 0.5083286430683635 train accuracy = [0.65761414]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9593\n",
            "\n",
            "train cost = 0.51096288338348 train accuracy = [0.65357775]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9594\n",
            "\n",
            "train cost = 0.5083185332206238 train accuracy = [0.65762272]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9595\n",
            "\n",
            "train cost = 0.5109508614002817 train accuracy = [0.65358708]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9596\n",
            "\n",
            "train cost = 0.5083084279356774 train accuracy = [0.65763129]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9597\n",
            "\n",
            "train cost = 0.510938845240466 train accuracy = [0.65359641]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9598\n",
            "\n",
            "train cost = 0.5082983272107316 train accuracy = [0.65763986]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9599\n",
            "\n",
            "train cost = 0.5109268349005157 train accuracy = [0.65360573]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9600\n",
            "\n",
            "train cost = 0.5082882310429958 train accuracy = [0.65764843]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9601\n",
            "\n",
            "train cost = 0.5109148303769161 train accuracy = [0.65361505]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9602\n",
            "\n",
            "train cost = 0.5082781394296813 train accuracy = [0.657657]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9603\n",
            "\n",
            "train cost = 0.5109028316661545 train accuracy = [0.65362436]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9604\n",
            "\n",
            "train cost = 0.5082680523680011 train accuracy = [0.65766556]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9605\n",
            "\n",
            "train cost = 0.5108908387647203 train accuracy = [0.65363367]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9606\n",
            "\n",
            "train cost = 0.5082579698551706 train accuracy = [0.65767413]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9607\n",
            "\n",
            "train cost = 0.5108788516691056 train accuracy = [0.65364298]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9608\n",
            "\n",
            "train cost = 0.5082478918884066 train accuracy = [0.65768268]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9609\n",
            "\n",
            "train cost = 0.5108668703758042 train accuracy = [0.65365229]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9610\n",
            "\n",
            "train cost = 0.5082378184649282 train accuracy = [0.65769124]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9611\n",
            "\n",
            "train cost = 0.5108548948813126 train accuracy = [0.65366159]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9612\n",
            "\n",
            "train cost = 0.5082277495819562 train accuracy = [0.65769979]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9613\n",
            "\n",
            "train cost = 0.5108429251821293 train accuracy = [0.65367089]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9614\n",
            "\n",
            "train cost = 0.5082176852367133 train accuracy = [0.65770834]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9615\n",
            "\n",
            "train cost = 0.5108309612747547 train accuracy = [0.65368019]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9616\n",
            "\n",
            "train cost = 0.5082076254264243 train accuracy = [0.65771689]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9617\n",
            "\n",
            "train cost = 0.5108190031556918 train accuracy = [0.65368948]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9618\n",
            "\n",
            "train cost = 0.5081975701483157 train accuracy = [0.65772543]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9619\n",
            "\n",
            "train cost = 0.5108070508214461 train accuracy = [0.65369877]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9620\n",
            "\n",
            "train cost = 0.508187519399616 train accuracy = [0.65773397]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9621\n",
            "\n",
            "train cost = 0.5107951042685246 train accuracy = [0.65370806]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9622\n",
            "\n",
            "train cost = 0.5081774731775556 train accuracy = [0.65774251]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9623\n",
            "\n",
            "train cost = 0.510783163493437 train accuracy = [0.65371734]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9624\n",
            "\n",
            "train cost = 0.5081674314793668 train accuracy = [0.65775104]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9625\n",
            "\n",
            "train cost = 0.5107712284926951 train accuracy = [0.65372663]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9626\n",
            "\n",
            "train cost = 0.5081573943022838 train accuracy = [0.65775958]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9627\n",
            "\n",
            "train cost = 0.5107592992628128 train accuracy = [0.6537359]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9628\n",
            "\n",
            "train cost = 0.5081473616435427 train accuracy = [0.6577681]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9629\n",
            "\n",
            "train cost = 0.5107473758003065 train accuracy = [0.65374518]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9630\n",
            "\n",
            "train cost = 0.5081373335003815 train accuracy = [0.65777663]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9631\n",
            "\n",
            "train cost = 0.5107354581016944 train accuracy = [0.65375445]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9632\n",
            "\n",
            "train cost = 0.5081273098700401 train accuracy = [0.65778515]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9633\n",
            "\n",
            "train cost = 0.5107235461634974 train accuracy = [0.65376372]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9634\n",
            "\n",
            "train cost = 0.5081172907497602 train accuracy = [0.65779367]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9635\n",
            "\n",
            "train cost = 0.5107116399822381 train accuracy = [0.65377298]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9636\n",
            "\n",
            "train cost = 0.5081072761367856 train accuracy = [0.65780219]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9637\n",
            "\n",
            "train cost = 0.5106997395544417 train accuracy = [0.65378225]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9638\n",
            "\n",
            "train cost = 0.5080972660283619 train accuracy = [0.65781071]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9639\n",
            "\n",
            "train cost = 0.5106878448766353 train accuracy = [0.65379151]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9640\n",
            "\n",
            "train cost = 0.5080872604217366 train accuracy = [0.65781922]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9641\n",
            "\n",
            "train cost = 0.5106759559453485 train accuracy = [0.65380076]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9642\n",
            "\n",
            "train cost = 0.5080772593141589 train accuracy = [0.65782773]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9643\n",
            "\n",
            "train cost = 0.510664072757113 train accuracy = [0.65381001]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9644\n",
            "\n",
            "train cost = 0.5080672627028804 train accuracy = [0.65783623]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9645\n",
            "\n",
            "train cost = 0.5106521953084625 train accuracy = [0.65381926]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9646\n",
            "\n",
            "train cost = 0.5080572705851538 train accuracy = [0.65784474]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9647\n",
            "\n",
            "train cost = 0.5106403235959331 train accuracy = [0.65382851]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9648\n",
            "\n",
            "train cost = 0.5080472829582345 train accuracy = [0.65785324]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9649\n",
            "\n",
            "train cost = 0.5106284576160631 train accuracy = [0.65383775]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9650\n",
            "\n",
            "train cost = 0.5080372998193793 train accuracy = [0.65786173]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9651\n",
            "\n",
            "train cost = 0.510616597365393 train accuracy = [0.653847]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9652\n",
            "\n",
            "train cost = 0.508027321165847 train accuracy = [0.65787023]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9653\n",
            "\n",
            "train cost = 0.5106047428404652 train accuracy = [0.65385623]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9654\n",
            "\n",
            "train cost = 0.5080173469948983 train accuracy = [0.65787872]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9655\n",
            "\n",
            "train cost = 0.5105928940378248 train accuracy = [0.65386547]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9656\n",
            "\n",
            "train cost = 0.5080073773037957 train accuracy = [0.65788721]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9657\n",
            "\n",
            "train cost = 0.5105810509540188 train accuracy = [0.6538747]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9658\n",
            "\n",
            "train cost = 0.507997412089804 train accuracy = [0.65789569]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9659\n",
            "\n",
            "train cost = 0.5105692135855964 train accuracy = [0.65388393]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9660\n",
            "\n",
            "train cost = 0.5079874513501891 train accuracy = [0.65790418]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9661\n",
            "\n",
            "train cost = 0.5105573819291089 train accuracy = [0.65389315]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9662\n",
            "\n",
            "train cost = 0.5079774950822193 train accuracy = [0.65791266]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9663\n",
            "\n",
            "train cost = 0.51054555598111 train accuracy = [0.65390237]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9664\n",
            "\n",
            "train cost = 0.5079675432831651 train accuracy = [0.65792113]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9665\n",
            "\n",
            "train cost = 0.5105337357381555 train accuracy = [0.65391159]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9666\n",
            "\n",
            "train cost = 0.5079575959502979 train accuracy = [0.65792961]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9667\n",
            "\n",
            "train cost = 0.5105219211968034 train accuracy = [0.65392081]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9668\n",
            "\n",
            "train cost = 0.507947653080892 train accuracy = [0.65793808]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9669\n",
            "\n",
            "train cost = 0.5105101123536138 train accuracy = [0.65393002]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9670\n",
            "\n",
            "train cost = 0.5079377146722228 train accuracy = [0.65794655]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9671\n",
            "\n",
            "train cost = 0.510498309205149 train accuracy = [0.65393923]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9672\n",
            "\n",
            "train cost = 0.5079277807215681 train accuracy = [0.65795501]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9673\n",
            "\n",
            "train cost = 0.5104865117479738 train accuracy = [0.65394844]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9674\n",
            "\n",
            "train cost = 0.5079178512262073 train accuracy = [0.65796348]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9675\n",
            "\n",
            "train cost = 0.5104747199786545 train accuracy = [0.65395764]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9676\n",
            "\n",
            "train cost = 0.5079079261834217 train accuracy = [0.65797194]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9677\n",
            "\n",
            "train cost = 0.5104629338937603 train accuracy = [0.65396684]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9678\n",
            "\n",
            "train cost = 0.5078980055904946 train accuracy = [0.65798039]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9679\n",
            "\n",
            "train cost = 0.5104511534898621 train accuracy = [0.65397604]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9680\n",
            "\n",
            "train cost = 0.5078880894447109 train accuracy = [0.65798885]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9681\n",
            "\n",
            "train cost = 0.5104393787635333 train accuracy = [0.65398523]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9682\n",
            "\n",
            "train cost = 0.5078781777433576 train accuracy = [0.6579973]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9683\n",
            "\n",
            "train cost = 0.5104276097113492 train accuracy = [0.65399442]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9684\n",
            "\n",
            "train cost = 0.5078682704837236 train accuracy = [0.65800575]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9685\n",
            "\n",
            "train cost = 0.5104158463298875 train accuracy = [0.65400361]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9686\n",
            "\n",
            "train cost = 0.5078583676630994 train accuracy = [0.65801419]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9687\n",
            "\n",
            "train cost = 0.5104040886157277 train accuracy = [0.6540128]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9688\n",
            "\n",
            "train cost = 0.5078484692787777 train accuracy = [0.65802264]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9689\n",
            "\n",
            "train cost = 0.510392336565452 train accuracy = [0.65402198]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9690\n",
            "\n",
            "train cost = 0.5078385753280528 train accuracy = [0.65803108]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9691\n",
            "\n",
            "train cost = 0.5103805901756445 train accuracy = [0.65403116]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9692\n",
            "\n",
            "train cost = 0.5078286858082208 train accuracy = [0.65803951]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9693\n",
            "\n",
            "train cost = 0.5103688494428915 train accuracy = [0.65404033]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9694\n",
            "\n",
            "train cost = 0.50781880071658 train accuracy = [0.65804795]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9695\n",
            "\n",
            "train cost = 0.5103571143637813 train accuracy = [0.65404951]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9696\n",
            "\n",
            "train cost = 0.5078089200504303 train accuracy = [0.65805638]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9697\n",
            "\n",
            "train cost = 0.5103453849349044 train accuracy = [0.65405868]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9698\n",
            "\n",
            "train cost = 0.5077990438070735 train accuracy = [0.65806481]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9699\n",
            "\n",
            "train cost = 0.510333661152854 train accuracy = [0.65406784]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9700\n",
            "\n",
            "train cost = 0.5077891719838132 train accuracy = [0.65807323]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9701\n",
            "\n",
            "train cost = 0.5103219430142247 train accuracy = [0.65407701]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9702\n",
            "\n",
            "train cost = 0.5077793045779551 train accuracy = [0.65808166]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9703\n",
            "\n",
            "train cost = 0.5103102305156137 train accuracy = [0.65408617]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9704\n",
            "\n",
            "train cost = 0.5077694415868064 train accuracy = [0.65809008]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9705\n",
            "\n",
            "train cost = 0.5102985236536203 train accuracy = [0.65409532]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9706\n",
            "\n",
            "train cost = 0.5077595830076763 train accuracy = [0.65809849]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9707\n",
            "\n",
            "train cost = 0.5102868224248459 train accuracy = [0.65410448]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9708\n",
            "\n",
            "train cost = 0.507749728837876 train accuracy = [0.65810691]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9709\n",
            "\n",
            "train cost = 0.510275126825894 train accuracy = [0.65411363]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9710\n",
            "\n",
            "train cost = 0.5077398790747182 train accuracy = [0.65811532]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9711\n",
            "\n",
            "train cost = 0.5102634368533705 train accuracy = [0.65412278]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9712\n",
            "\n",
            "train cost = 0.507730033715518 train accuracy = [0.65812373]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9713\n",
            "\n",
            "train cost = 0.5102517525038832 train accuracy = [0.65413192]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9714\n",
            "\n",
            "train cost = 0.5077201927575916 train accuracy = [0.65813213]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9715\n",
            "\n",
            "train cost = 0.5102400737740421 train accuracy = [0.65414106]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9716\n",
            "\n",
            "train cost = 0.5077103561982578 train accuracy = [0.65814054]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9717\n",
            "\n",
            "train cost = 0.5102284006604596 train accuracy = [0.6541502]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9718\n",
            "\n",
            "train cost = 0.5077005240348367 train accuracy = [0.65814894]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9719\n",
            "\n",
            "train cost = 0.5102167331597499 train accuracy = [0.65415934]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9720\n",
            "\n",
            "train cost = 0.5076906962646504 train accuracy = [0.65815733]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9721\n",
            "\n",
            "train cost = 0.5102050712685294 train accuracy = [0.65416847]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9722\n",
            "\n",
            "train cost = 0.5076808728850231 train accuracy = [0.65816573]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9723\n",
            "\n",
            "train cost = 0.5101934149834171 train accuracy = [0.6541776]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9724\n",
            "\n",
            "train cost = 0.5076710538932803 train accuracy = [0.65817412]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9725\n",
            "\n",
            "train cost = 0.5101817643010336 train accuracy = [0.65418673]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9726\n",
            "\n",
            "train cost = 0.5076612392867499 train accuracy = [0.65818251]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9727\n",
            "\n",
            "train cost = 0.5101701192180017 train accuracy = [0.65419585]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9728\n",
            "\n",
            "train cost = 0.5076514290627612 train accuracy = [0.65819089]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9729\n",
            "\n",
            "train cost = 0.5101584797309467 train accuracy = [0.65420497]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9730\n",
            "\n",
            "train cost = 0.5076416232186457 train accuracy = [0.65819927]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9731\n",
            "\n",
            "train cost = 0.5101468458364959 train accuracy = [0.65421409]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9732\n",
            "\n",
            "train cost = 0.5076318217517364 train accuracy = [0.65820766]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9733\n",
            "\n",
            "train cost = 0.5101352175312787 train accuracy = [0.6542232]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9734\n",
            "\n",
            "train cost = 0.5076220246593683 train accuracy = [0.65821603]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9735\n",
            "\n",
            "train cost = 0.5101235948119263 train accuracy = [0.65423232]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9736\n",
            "\n",
            "train cost = 0.5076122319388783 train accuracy = [0.65822441]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9737\n",
            "\n",
            "train cost = 0.5101119776750727 train accuracy = [0.65424142]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9738\n",
            "\n",
            "train cost = 0.5076024435876049 train accuracy = [0.65823278]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9739\n",
            "\n",
            "train cost = 0.5101003661173535 train accuracy = [0.65425053]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9740\n",
            "\n",
            "train cost = 0.5075926596028887 train accuracy = [0.65824115]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9741\n",
            "\n",
            "train cost = 0.5100887601354068 train accuracy = [0.65425963]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9742\n",
            "\n",
            "train cost = 0.507582879982072 train accuracy = [0.65824951]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9743\n",
            "\n",
            "train cost = 0.5100771597258728 train accuracy = [0.65426873]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9744\n",
            "\n",
            "train cost = 0.5075731047224988 train accuracy = [0.65825788]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9745\n",
            "\n",
            "train cost = 0.5100655648853933 train accuracy = [0.65427783]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9746\n",
            "\n",
            "train cost = 0.5075633338215152 train accuracy = [0.65826624]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9747\n",
            "\n",
            "train cost = 0.510053975610613 train accuracy = [0.65428692]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9748\n",
            "\n",
            "train cost = 0.5075535672764688 train accuracy = [0.65827459]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9749\n",
            "\n",
            "train cost = 0.5100423918981782 train accuracy = [0.65429601]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9750\n",
            "\n",
            "train cost = 0.5075438050847093 train accuracy = [0.65828295]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9751\n",
            "\n",
            "train cost = 0.5100308137447378 train accuracy = [0.6543051]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9752\n",
            "\n",
            "train cost = 0.5075340472435881 train accuracy = [0.6582913]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9753\n",
            "\n",
            "train cost = 0.5100192411469422 train accuracy = [0.65431418]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9754\n",
            "\n",
            "train cost = 0.5075242937504584 train accuracy = [0.65829965]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9755\n",
            "\n",
            "train cost = 0.5100076741014444 train accuracy = [0.65432326]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9756\n",
            "\n",
            "train cost = 0.5075145446026752 train accuracy = [0.65830799]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9757\n",
            "\n",
            "train cost = 0.5099961126048995 train accuracy = [0.65433234]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9758\n",
            "\n",
            "train cost = 0.5075047997975954 train accuracy = [0.65831634]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9759\n",
            "\n",
            "train cost = 0.5099845566539645 train accuracy = [0.65434142]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9760\n",
            "\n",
            "train cost = 0.5074950593325779 train accuracy = [0.65832468]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9761\n",
            "\n",
            "train cost = 0.5099730062452987 train accuracy = [0.65435049]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9762\n",
            "\n",
            "train cost = 0.5074853232049829 train accuracy = [0.65833302]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9763\n",
            "\n",
            "train cost = 0.5099614613755635 train accuracy = [0.65435956]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9764\n",
            "\n",
            "train cost = 0.5074755914121727 train accuracy = [0.65834135]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9765\n",
            "\n",
            "train cost = 0.5099499220414224 train accuracy = [0.65436862]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9766\n",
            "\n",
            "train cost = 0.5074658639515116 train accuracy = [0.65834968]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9767\n",
            "\n",
            "train cost = 0.5099383882395411 train accuracy = [0.65437769]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9768\n",
            "\n",
            "train cost = 0.5074561408203654 train accuracy = [0.65835801]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9769\n",
            "\n",
            "train cost = 0.5099268599665872 train accuracy = [0.65438675]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9770\n",
            "\n",
            "train cost = 0.5074464220161019 train accuracy = [0.65836634]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9771\n",
            "\n",
            "train cost = 0.5099153372192304 train accuracy = [0.6543958]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9772\n",
            "\n",
            "train cost = 0.5074367075360907 train accuracy = [0.65837466]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9773\n",
            "\n",
            "train cost = 0.5099038199941431 train accuracy = [0.65440486]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9774\n",
            "\n",
            "train cost = 0.507426997377703 train accuracy = [0.65838298]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9775\n",
            "\n",
            "train cost = 0.5098923082879991 train accuracy = [0.65441391]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9776\n",
            "\n",
            "train cost = 0.507417291538312 train accuracy = [0.6583913]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9777\n",
            "\n",
            "train cost = 0.5098808020974748 train accuracy = [0.65442295]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9778\n",
            "\n",
            "train cost = 0.5074075900152927 train accuracy = [0.65839961]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9779\n",
            "\n",
            "train cost = 0.5098693014192482 train accuracy = [0.654432]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9780\n",
            "\n",
            "train cost = 0.5073978928060219 train accuracy = [0.65840793]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9781\n",
            "\n",
            "train cost = 0.5098578062500001 train accuracy = [0.65444104]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9782\n",
            "\n",
            "train cost = 0.5073881999078781 train accuracy = [0.65841624]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9783\n",
            "\n",
            "train cost = 0.5098463165864129 train accuracy = [0.65445008]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9784\n",
            "\n",
            "train cost = 0.5073785113182416 train accuracy = [0.65842454]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9785\n",
            "\n",
            "train cost = 0.5098348324251711 train accuracy = [0.65445912]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9786\n",
            "\n",
            "train cost = 0.5073688270344946 train accuracy = [0.65843285]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9787\n",
            "\n",
            "train cost = 0.5098233537629617 train accuracy = [0.65446815]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9788\n",
            "\n",
            "train cost = 0.5073591470540212 train accuracy = [0.65844115]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9789\n",
            "\n",
            "train cost = 0.5098118805964735 train accuracy = [0.65447718]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9790\n",
            "\n",
            "train cost = 0.507349471374207 train accuracy = [0.65844945]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9791\n",
            "\n",
            "train cost = 0.5098004129223974 train accuracy = [0.6544862]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9792\n",
            "\n",
            "train cost = 0.5073397999924396 train accuracy = [0.65845774]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9793\n",
            "\n",
            "train cost = 0.5097889507374266 train accuracy = [0.65449523]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9794\n",
            "\n",
            "train cost = 0.5073301329061083 train accuracy = [0.65846603]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9795\n",
            "\n",
            "train cost = 0.5097774940382561 train accuracy = [0.65450425]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9796\n",
            "\n",
            "train cost = 0.5073204701126043 train accuracy = [0.65847432]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9797\n",
            "\n",
            "train cost = 0.5097660428215833 train accuracy = [0.65451327]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9798\n",
            "\n",
            "train cost = 0.5073108116093205 train accuracy = [0.65848261]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9799\n",
            "\n",
            "train cost = 0.5097545970841076 train accuracy = [0.65452228]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9800\n",
            "\n",
            "train cost = 0.5073011573936516 train accuracy = [0.65849089]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9801\n",
            "\n",
            "train cost = 0.5097431568225305 train accuracy = [0.65453129]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9802\n",
            "\n",
            "train cost = 0.5072915074629941 train accuracy = [0.65849918]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9803\n",
            "\n",
            "train cost = 0.5097317220335554 train accuracy = [0.6545403]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9804\n",
            "\n",
            "train cost = 0.5072818618147463 train accuracy = [0.65850745]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9805\n",
            "\n",
            "train cost = 0.5097202927138883 train accuracy = [0.65454931]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9806\n",
            "\n",
            "train cost = 0.5072722204463084 train accuracy = [0.65851573]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9807\n",
            "\n",
            "train cost = 0.5097088688602367 train accuracy = [0.65455831]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9808\n",
            "\n",
            "train cost = 0.507262583355082 train accuracy = [0.658524]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9809\n",
            "\n",
            "train cost = 0.5096974504693106 train accuracy = [0.65456731]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9810\n",
            "\n",
            "train cost = 0.507252950538471 train accuracy = [0.65853227]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9811\n",
            "\n",
            "train cost = 0.5096860375378219 train accuracy = [0.65457631]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9812\n",
            "\n",
            "train cost = 0.5072433219938807 train accuracy = [0.65854054]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9813\n",
            "\n",
            "train cost = 0.5096746300624848 train accuracy = [0.6545853]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9814\n",
            "\n",
            "train cost = 0.5072336977187184 train accuracy = [0.65854881]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9815\n",
            "\n",
            "train cost = 0.5096632280400153 train accuracy = [0.65459429]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9816\n",
            "\n",
            "train cost = 0.5072240777103929 train accuracy = [0.65855707]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9817\n",
            "\n",
            "train cost = 0.5096518314671316 train accuracy = [0.65460328]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9818\n",
            "\n",
            "train cost = 0.5072144619663151 train accuracy = [0.65856533]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9819\n",
            "\n",
            "train cost = 0.5096404403405541 train accuracy = [0.65461226]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9820\n",
            "\n",
            "train cost = 0.5072048504838977 train accuracy = [0.65857358]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9821\n",
            "\n",
            "train cost = 0.5096290546570051 train accuracy = [0.65462124]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9822\n",
            "\n",
            "train cost = 0.5071952432605548 train accuracy = [0.65858184]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9823\n",
            "\n",
            "train cost = 0.5096176744132094 train accuracy = [0.65463022]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9824\n",
            "\n",
            "train cost = 0.5071856402937025 train accuracy = [0.65859009]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9825\n",
            "\n",
            "train cost = 0.5096062996058932 train accuracy = [0.6546392]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9826\n",
            "\n",
            "train cost = 0.5071760415807588 train accuracy = [0.65859833]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9827\n",
            "\n",
            "train cost = 0.5095949302317855 train accuracy = [0.65464817]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9828\n",
            "\n",
            "train cost = 0.5071664471191432 train accuracy = [0.65860658]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9829\n",
            "\n",
            "train cost = 0.5095835662876167 train accuracy = [0.65465714]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9830\n",
            "\n",
            "train cost = 0.5071568569062772 train accuracy = [0.65861482]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9831\n",
            "\n",
            "train cost = 0.5095722077701199 train accuracy = [0.65466611]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9832\n",
            "\n",
            "train cost = 0.5071472709395841 train accuracy = [0.65862306]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9833\n",
            "\n",
            "train cost = 0.5095608546760299 train accuracy = [0.65467507]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9834\n",
            "\n",
            "train cost = 0.5071376892164886 train accuracy = [0.6586313]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9835\n",
            "\n",
            "train cost = 0.5095495070020836 train accuracy = [0.65468403]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9836\n",
            "\n",
            "train cost = 0.5071281117344176 train accuracy = [0.65863953]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9837\n",
            "\n",
            "train cost = 0.5095381647450201 train accuracy = [0.65469299]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9838\n",
            "\n",
            "train cost = 0.5071185384907995 train accuracy = [0.65864776]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9839\n",
            "\n",
            "train cost = 0.5095268279015807 train accuracy = [0.65470194]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9840\n",
            "\n",
            "train cost = 0.5071089694830646 train accuracy = [0.65865599]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9841\n",
            "\n",
            "train cost = 0.5095154964685084 train accuracy = [0.6547109]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9842\n",
            "\n",
            "train cost = 0.5070994047086449 train accuracy = [0.65866422]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9843\n",
            "\n",
            "train cost = 0.5095041704425486 train accuracy = [0.65471984]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9844\n",
            "\n",
            "train cost = 0.5070898441649742 train accuracy = [0.65867244]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9845\n",
            "\n",
            "train cost = 0.5094928498204485 train accuracy = [0.65472879]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9846\n",
            "\n",
            "train cost = 0.507080287849488 train accuracy = [0.65868066]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9847\n",
            "\n",
            "train cost = 0.5094815345989577 train accuracy = [0.65473773]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9848\n",
            "\n",
            "train cost = 0.5070707357596236 train accuracy = [0.65868888]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9849\n",
            "\n",
            "train cost = 0.5094702247748276 train accuracy = [0.65474667]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9850\n",
            "\n",
            "train cost = 0.5070611878928201 train accuracy = [0.65869709]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9851\n",
            "\n",
            "train cost = 0.5094589203448118 train accuracy = [0.65475561]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9852\n",
            "\n",
            "train cost = 0.5070516442465182 train accuracy = [0.6587053]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9853\n",
            "\n",
            "train cost = 0.5094476213056659 train accuracy = [0.65476454]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9854\n",
            "\n",
            "train cost = 0.5070421048181606 train accuracy = [0.65871351]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9855\n",
            "\n",
            "train cost = 0.5094363276541475 train accuracy = [0.65477347]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9856\n",
            "\n",
            "train cost = 0.5070325696051916 train accuracy = [0.65872172]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9857\n",
            "\n",
            "train cost = 0.5094250393870164 train accuracy = [0.6547824]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9858\n",
            "\n",
            "train cost = 0.5070230386050573 train accuracy = [0.65872992]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9859\n",
            "\n",
            "train cost = 0.5094137565010346 train accuracy = [0.65479133]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9860\n",
            "\n",
            "train cost = 0.5070135118152054 train accuracy = [0.65873812]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9861\n",
            "\n",
            "train cost = 0.5094024789929658 train accuracy = [0.65480025]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9862\n",
            "\n",
            "train cost = 0.5070039892330857 train accuracy = [0.65874632]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9863\n",
            "\n",
            "train cost = 0.5093912068595758 train accuracy = [0.65480917]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9864\n",
            "\n",
            "train cost = 0.5069944708561492 train accuracy = [0.65875451]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9865\n",
            "\n",
            "train cost = 0.509379940097633 train accuracy = [0.65481808]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9866\n",
            "\n",
            "train cost = 0.5069849566818493 train accuracy = [0.65876271]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9867\n",
            "\n",
            "train cost = 0.509368678703907 train accuracy = [0.654827]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9868\n",
            "\n",
            "train cost = 0.5069754467076407 train accuracy = [0.6587709]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9869\n",
            "\n",
            "train cost = 0.5093574226751703 train accuracy = [0.65483591]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9870\n",
            "\n",
            "train cost = 0.50696594093098 train accuracy = [0.65877908]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9871\n",
            "\n",
            "train cost = 0.5093461720081968 train accuracy = [0.65484482]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9872\n",
            "\n",
            "train cost = 0.5069564393493255 train accuracy = [0.65878727]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9873\n",
            "\n",
            "train cost = 0.5093349266997628 train accuracy = [0.65485372]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9874\n",
            "\n",
            "train cost = 0.5069469419601373 train accuracy = [0.65879545]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9875\n",
            "\n",
            "train cost = 0.5093236867466466 train accuracy = [0.65486262]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9876\n",
            "\n",
            "train cost = 0.5069374487608772 train accuracy = [0.65880363]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9877\n",
            "\n",
            "train cost = 0.5093124521456286 train accuracy = [0.65487152]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9878\n",
            "\n",
            "train cost = 0.5069279597490087 train accuracy = [0.6588118]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9879\n",
            "\n",
            "train cost = 0.5093012228934909 train accuracy = [0.65488041]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9880\n",
            "\n",
            "train cost = 0.5069184749219972 train accuracy = [0.65881998]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9881\n",
            "\n",
            "train cost = 0.5092899989870182 train accuracy = [0.65488931]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9882\n",
            "\n",
            "train cost = 0.5069089942773095 train accuracy = [0.65882815]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9883\n",
            "\n",
            "train cost = 0.5092787804229969 train accuracy = [0.6548982]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9884\n",
            "\n",
            "train cost = 0.5068995178124146 train accuracy = [0.65883631]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9885\n",
            "\n",
            "train cost = 0.5092675671982155 train accuracy = [0.65490708]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9886\n",
            "\n",
            "train cost = 0.506890045524783 train accuracy = [0.65884448]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9887\n",
            "\n",
            "train cost = 0.5092563593094644 train accuracy = [0.65491597]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9888\n",
            "\n",
            "train cost = 0.5068805774118867 train accuracy = [0.65885264]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9889\n",
            "\n",
            "train cost = 0.5092451567535365 train accuracy = [0.65492485]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9890\n",
            "\n",
            "train cost = 0.5068711134712 train accuracy = [0.6588608]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9891\n",
            "\n",
            "train cost = 0.5092339595272264 train accuracy = [0.65493373]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9892\n",
            "\n",
            "train cost = 0.5068616537001983 train accuracy = [0.65886895]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9893\n",
            "\n",
            "train cost = 0.5092227676273305 train accuracy = [0.6549426]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9894\n",
            "\n",
            "train cost = 0.5068521980963592 train accuracy = [0.65887711]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9895\n",
            "\n",
            "train cost = 0.5092115810506479 train accuracy = [0.65495147]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9896\n",
            "\n",
            "train cost = 0.5068427466571618 train accuracy = [0.65888526]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9897\n",
            "\n",
            "train cost = 0.5092003997939791 train accuracy = [0.65496034]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9898\n",
            "\n",
            "train cost = 0.506833299380087 train accuracy = [0.65889341]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9899\n",
            "\n",
            "train cost = 0.509189223854127 train accuracy = [0.65496921]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9900\n",
            "\n",
            "train cost = 0.5068238562626176 train accuracy = [0.65890155]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9901\n",
            "\n",
            "train cost = 0.5091780532278964 train accuracy = [0.65497807]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9902\n",
            "\n",
            "train cost = 0.5068144173022378 train accuracy = [0.6589097]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9903\n",
            "\n",
            "train cost = 0.5091668879120944 train accuracy = [0.65498693]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9904\n",
            "\n",
            "train cost = 0.5068049824964336 train accuracy = [0.65891784]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9905\n",
            "\n",
            "train cost = 0.5091557279035296 train accuracy = [0.65499579]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9906\n",
            "\n",
            "train cost = 0.5067955518426929 train accuracy = [0.65892597]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9907\n",
            "\n",
            "train cost = 0.5091445731990132 train accuracy = [0.65500464]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9908\n",
            "\n",
            "train cost = 0.5067861253385051 train accuracy = [0.65893411]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9909\n",
            "\n",
            "train cost = 0.509133423795358 train accuracy = [0.65501349]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9910\n",
            "\n",
            "train cost = 0.5067767029813617 train accuracy = [0.65894224]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9911\n",
            "\n",
            "train cost = 0.5091222796893791 train accuracy = [0.65502234]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9912\n",
            "\n",
            "train cost = 0.5067672847687554 train accuracy = [0.65895037]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9913\n",
            "\n",
            "train cost = 0.5091111408778934 train accuracy = [0.65503119]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9914\n",
            "\n",
            "train cost = 0.5067578706981811 train accuracy = [0.65895849]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9915\n",
            "\n",
            "train cost = 0.5091000073577202 train accuracy = [0.65504003]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9916\n",
            "\n",
            "train cost = 0.5067484607671351 train accuracy = [0.65896662]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9917\n",
            "\n",
            "train cost = 0.5090888791256803 train accuracy = [0.65504887]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9918\n",
            "\n",
            "train cost = 0.5067390549731154 train accuracy = [0.65897474]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9919\n",
            "\n",
            "train cost = 0.5090777561785972 train accuracy = [0.6550577]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9920\n",
            "\n",
            "train cost = 0.5067296533136222 train accuracy = [0.65898286]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9921\n",
            "\n",
            "train cost = 0.5090666385132957 train accuracy = [0.65506654]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9922\n",
            "\n",
            "train cost = 0.5067202557861566 train accuracy = [0.65899097]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9923\n",
            "\n",
            "train cost = 0.5090555261266031 train accuracy = [0.65507537]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9924\n",
            "\n",
            "train cost = 0.5067108623882222 train accuracy = [0.65899908]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9925\n",
            "\n",
            "train cost = 0.5090444190153486 train accuracy = [0.6550842]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9926\n",
            "\n",
            "train cost = 0.5067014731173238 train accuracy = [0.65900719]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9927\n",
            "\n",
            "train cost = 0.5090333171763634 train accuracy = [0.65509302]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9928\n",
            "\n",
            "train cost = 0.5066920879709682 train accuracy = [0.6590153]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9929\n",
            "\n",
            "train cost = 0.5090222206064806 train accuracy = [0.65510184]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9930\n",
            "\n",
            "train cost = 0.5066827069466637 train accuracy = [0.6590234]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9931\n",
            "\n",
            "train cost = 0.5090111293025357 train accuracy = [0.65511066]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9932\n",
            "\n",
            "train cost = 0.5066733300419205 train accuracy = [0.65903151]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9933\n",
            "\n",
            "train cost = 0.5090000432613656 train accuracy = [0.65511948]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9934\n",
            "\n",
            "train cost = 0.5066639572542503 train accuracy = [0.6590396]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9935\n",
            "\n",
            "train cost = 0.50898896247981 train accuracy = [0.65512829]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9936\n",
            "\n",
            "train cost = 0.5066545885811667 train accuracy = [0.6590477]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9937\n",
            "\n",
            "train cost = 0.5089778869547098 train accuracy = [0.6551371]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9938\n",
            "\n",
            "train cost = 0.506645224020185 train accuracy = [0.65905579]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9939\n",
            "\n",
            "train cost = 0.5089668166829084 train accuracy = [0.65514591]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9940\n",
            "\n",
            "train cost = 0.506635863568822 train accuracy = [0.65906388]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9941\n",
            "\n",
            "train cost = 0.5089557516612514 train accuracy = [0.65515471]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9942\n",
            "\n",
            "train cost = 0.5066265072245963 train accuracy = [0.65907197]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9943\n",
            "\n",
            "train cost = 0.5089446918865858 train accuracy = [0.65516352]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9944\n",
            "\n",
            "train cost = 0.5066171549850284 train accuracy = [0.65908006]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9945\n",
            "\n",
            "train cost = 0.508933637355761 train accuracy = [0.65517231]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9946\n",
            "\n",
            "train cost = 0.5066078068476402 train accuracy = [0.65908814]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9947\n",
            "\n",
            "train cost = 0.5089225880656284 train accuracy = [0.65518111]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9948\n",
            "\n",
            "train cost = 0.5065984628099555 train accuracy = [0.65909622]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9949\n",
            "\n",
            "train cost = 0.5089115440130414 train accuracy = [0.6551899]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9950\n",
            "\n",
            "train cost = 0.5065891228694996 train accuracy = [0.6591043]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9951\n",
            "\n",
            "train cost = 0.5089005051948552 train accuracy = [0.65519869]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9952\n",
            "\n",
            "train cost = 0.5065797870237999 train accuracy = [0.65911237]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9953\n",
            "\n",
            "train cost = 0.5088894716079273 train accuracy = [0.65520748]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9954\n",
            "\n",
            "train cost = 0.5065704552703849 train accuracy = [0.65912044]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9955\n",
            "\n",
            "train cost = 0.5088784432491169 train accuracy = [0.65521626]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9956\n",
            "\n",
            "train cost = 0.5065611276067854 train accuracy = [0.65912851]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9957\n",
            "\n",
            "train cost = 0.5088674201152856 train accuracy = [0.65522505]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9958\n",
            "\n",
            "train cost = 0.5065518040305333 train accuracy = [0.65913658]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9959\n",
            "\n",
            "train cost = 0.5088564022032965 train accuracy = [0.65523382]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9960\n",
            "\n",
            "train cost = 0.5065424845391628 train accuracy = [0.65914464]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9961\n",
            "\n",
            "train cost = 0.5088453895100152 train accuracy = [0.6552426]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9962\n",
            "\n",
            "train cost = 0.5065331691302094 train accuracy = [0.6591527]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9963\n",
            "\n",
            "train cost = 0.5088343820323089 train accuracy = [0.65525137]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9964\n",
            "\n",
            "train cost = 0.5065238578012103 train accuracy = [0.65916076]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9965\n",
            "\n",
            "train cost = 0.5088233797670471 train accuracy = [0.65526014]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9966\n",
            "\n",
            "train cost = 0.5065145505497046 train accuracy = [0.65916881]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9967\n",
            "\n",
            "train cost = 0.508812382711101 train accuracy = [0.65526891]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9968\n",
            "\n",
            "train cost = 0.5065052473732329 train accuracy = [0.65917687]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9969\n",
            "\n",
            "train cost = 0.508801390861344 train accuracy = [0.65527767]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9970\n",
            "\n",
            "train cost = 0.5064959482693374 train accuracy = [0.65918492]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9971\n",
            "\n",
            "train cost = 0.5087904042146515 train accuracy = [0.65528643]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9972\n",
            "\n",
            "train cost = 0.5064866532355623 train accuracy = [0.65919296]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9973\n",
            "\n",
            "train cost = 0.5087794227679008 train accuracy = [0.65529519]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9974\n",
            "\n",
            "train cost = 0.5064773622694533 train accuracy = [0.65920101]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9975\n",
            "\n",
            "train cost = 0.5087684465179713 train accuracy = [0.65530395]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9976\n",
            "\n",
            "train cost = 0.5064680753685578 train accuracy = [0.65920905]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9977\n",
            "\n",
            "train cost = 0.5087574754617441 train accuracy = [0.6553127]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9978\n",
            "\n",
            "train cost = 0.5064587925304247 train accuracy = [0.65921709]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9979\n",
            "\n",
            "train cost = 0.5087465095961027 train accuracy = [0.65532145]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9980\n",
            "\n",
            "train cost = 0.5064495137526049 train accuracy = [0.65922512]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9981\n",
            "\n",
            "train cost = 0.5087355489179324 train accuracy = [0.65533019]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9982\n",
            "\n",
            "train cost = 0.5064402390326509 train accuracy = [0.65923316]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9983\n",
            "\n",
            "train cost = 0.5087245934241202 train accuracy = [0.65533894]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9984\n",
            "\n",
            "train cost = 0.5064309683681166 train accuracy = [0.65924119]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9985\n",
            "\n",
            "train cost = 0.5087136431115558 train accuracy = [0.65534768]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9986\n",
            "\n",
            "train cost = 0.506421701756558 train accuracy = [0.65924922]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9987\n",
            "\n",
            "train cost = 0.5087026979771302 train accuracy = [0.65535642]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9988\n",
            "\n",
            "train cost = 0.5064124391955325 train accuracy = [0.65925724]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9989\n",
            "\n",
            "train cost = 0.5086917580177366 train accuracy = [0.65536515]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9990\n",
            "\n",
            "train cost = 0.5064031806825992 train accuracy = [0.65926527]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9991\n",
            "\n",
            "train cost = 0.5086808232302702 train accuracy = [0.65537388]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9992\n",
            "\n",
            "train cost = 0.5063939262153189 train accuracy = [0.65927329]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9993\n",
            "\n",
            "train cost = 0.5086698936116283 train accuracy = [0.65538261]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9994\n",
            "\n",
            "train cost = 0.5063846757912541 train accuracy = [0.6592813]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9995\n",
            "\n",
            "train cost = 0.5086589691587101 train accuracy = [0.65539134]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9996\n",
            "\n",
            "train cost = 0.506375429407969 train accuracy = [0.65928932]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9997\n",
            "\n",
            "train cost = 0.5086480498684166 train accuracy = [0.65540006]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9998\n",
            "\n",
            "train cost = 0.5063661870630294 train accuracy = [0.65929733]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :9999\n",
            "\n",
            "train cost = 0.508637135737651 train accuracy = [0.65540878]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.78892734]\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "Iteration Number :10000\n",
            "\n",
            "train cost = 0.5063569487540026 train accuracy = [0.65930534]\n",
            "\n",
            "val_test cost = nan  accuracy = [0.79238754]\n",
            "\n",
            "──────────────────────────────────────────────────\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(cost_array)\n",
        "\n",
        "# Set labels for the axes\n",
        "plt.xlabel(' Number of iterations')\n",
        "plt.ylabel('Cost')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "-9fzyZt3DpcG",
        "outputId": "38fafb1f-5e2a-422f-cf4d-b51135831f12"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cost_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLd7N8zjOySk",
        "outputId": "0d17e7d5-c52c-4892-fa3e-891cdf14d60d"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nan, nan, nan, nan, nan, inf, 9.186870422356739883, 7.0782835163001102236, 5.641262064065738837, 4.5856021282947354045, 3.69199413615533498, 2.9525292742907686865, 2.3684381155388121343, 1.928865332200623147, 1.6029760333768689748, 1.4177541644686531847, 1.3441727492761726126, 1.3153198683663186932, 1.3032437989043845484, 1.2971127135810231673, 1.29351472275198533, 1.2910550231713963919, 1.2891268876824458431, 1.2874542180665092397, 1.2859086603838895137, 1.2844279474686821957, 1.2829810975925867359, 1.2815523824576946436, 1.2801337321872587844, 1.2787209776875809375, 1.2773119570395792667, 1.2759055469076227896, 1.2745011633260036226, 1.2730985030447927468, 1.2716974090966627416, 1.2702978007901360064, 1.2688996372118075048, 1.2675028981885209349, 1.2661075743557055881, 1.2647136619774253043, 1.2633211602459949703, 1.2619300698752411616, 1.2605403923683465942, 1.2591521296372322059, 1.2577652838049684758, 1.2563798571033806395, 1.2549958518200989938, 1.2536132702712507282, 1.2522321147874225354, 1.2508523877064741847, 1.2494740913698772663, 1.2480972281208598895, 1.2467218003034711834, 1.2453478102621102908, 1.2439752603412871913, 1.2426041528854972558, 1.2412344902391499567, 1.2398662747465223047, 1.2384995087517225781, 1.2371341945986576342, 1.2357703346310006529, 1.2344079311921582203, 1.2330469866252362172, 1.2316875032730045953, 1.2303294834778610549, 1.2289729295817938381, 1.2276178439263437586, 1.2262642288525655409, 1.2249120867009886633, 1.2235614198115776291, 1.2222122305236918913, 1.2208645211760453203, 1.2195182941066653335, 1.2181735516528516875, 1.2168302961511349168, 1.2154885299372344947, 1.2141482553460166401, 1.2128094747114518576, 1.2114721903665721643, 1.2101364046434280152, 1.208802119873044933, 1.2074693383853798691, 1.2061380625092772333, 1.2048082945724246667, 1.2034800369013085065, 1.2021532918211689606, 1.200828061655954993, 1.1995043487282789254, 1.198182155359370733, 1.1968614838690320843, 1.1955423365755900397, 1.1942247157958505105, 1.19290862384505139, 1.1915940630368154308, 1.1902810356831028026, 1.18896954409416337, 1.1876595905784887003, 1.186351177442763752, 1.1850443069918183114, 1.1837389815285780929, 1.1824352033540156206, 1.18113297476710074, 1.1798322980647509284, 1.1785331755417812505, 1.1772356094908540727, 1.1759396022024284749, 1.1746451559647093643, 1.1733522730635963526, 1.1720609557826322891, 1.1707712064029515663, 1.1694830272032280978, 1.1681964204596230574, 1.1669113884457323076, 1.1656279334325335608, 1.164346057688333263, 1.1630657634787131983, 1.1617870530664768183, 1.1605099287115952912, 1.1592343926711532769, 1.1579604471992944401, 1.1566880945471666762, 1.1554173369628670712, 1.154148176691386596, 1.1528806159745545258, 1.1516146570509825868, 1.1503503021560088553, 1.1490875535216413657, 1.1478264133765014808, 1.1465668839457669707, 1.1453089674511148634, 1.1440526661106640036, 1.1427979821389173793, 1.1415449177467041688, 1.1402934751411215497, 1.1390436565254762531, 1.1377954640992258506, 1.1365489000579198039, 1.1353039665931402701, 1.1340606658924426441, 1.1328190001392958685, 1.1315789715130224958, 1.1303405821887385069, 1.1291038343372929033, 1.1278687301252070344, 1.1266352717146137193, 1.1254034612631961201, 1.1241733009241263806, 1.1229447928460040436, 1.1217179391727942374, 1.1204927420437656432, 1.1192692035934282235, 1.1180473259514707532, 1.116827111242698114, 1.1156085615869683798, 1.1143916790991296886, 1.1131764658889569075, 1.1119629240610880831, 1.1107510557149606868, 1.1095408629447476595, 1.1083323478392932554, 1.1071255124820486871, 1.1059203589510075744, 1.1047168893186412041, 1.1035151056518335925, 1.1023150100118163722, 1.1011166044541034853, 1.0999198910284257002, 1.0987248717786649445, 1.0975315487427884795, 1.096339923952782876, 1.0951499994345878442, 1.0939617772080298873, 1.0927752592867557937, 1.0915904476781659701, 1.0904073443833476183, 1.0892259513970077613, 1.0880462707074061141, 1.0868683042962878143, 1.0856920541388160064, 1.0845175222035042882, 1.0833447104521490217, 1.0821736208397615068, 1.0810042553145000357, 1.0798366158176018143, 1.0786707042833147617, 1.0775065226388291996, 1.0763440728042094182, 1.0751833566923251418, 1.0740243762087828761, 1.0728671332518571641, 1.0717116297124217363, 1.0705578674738805711, 1.0694058484120988583, 1.0682555743953338793, 1.0671070472841658089, 1.0659602689314284286, 1.0648152411821397745, 1.0636719658734327093, 1.0625304448344854308, 1.0613906798864519188, 1.0602526728423923184, 1.0591164255072032831, 1.0579819396775482526, 1.0568492171417876973, 1.0557182596799093196, 1.0545890690634582183, 1.0534616470554670224, 1.0523359954103859975, 1.0512121158740131309, 1.0500900101834241997, 1.0489696800669028244, 1.0478511272438705125, 1.0467343534248167023, 1.0456193603112288045, 1.0445061495955222535, 1.0433947229609705624, 1.0422850820816353981, 1.0411772286222966773, 1.0400711642383826844, 1.0389668905759002191, 1.0378644092713647849, 1.0367637219517308051, 1.0356648302343218951, 1.034567735726761182, 1.0334724400269016704, 1.032378944722756679, 1.0312872513924303354, 1.0301973616040481416, 1.0291092769156876173, 1.0280229988753090172, 1.0269385290206861413, 1.0258558688793372291, 1.0247750199684559516, 1.0236959837948425065, 1.0226187618548348135, 1.0215433556342398282, 1.0204697666082649672, 1.0193979962414496552, 1.018328045987597004, 1.0172599172897056174, 1.0161936115799015387, 1.0151291302793703394, 1.0140664747982893589, 1.013005646535760091, 1.011946646879740741, 1.010889477206978936, 1.009834138882944614, 1.0087806332617630831, 1.0077289616861482664, 1.0066791254873361276, 1.0056311259850182931, 1.004584964487275872, 1.0035406422905134731, 1.0024981606793934389, 1.0014575209267702851, 1.0004187242936253647, 0.99938177202900175617, 0.99834666536993938, 0.99731340554141035306, 0.99628199375625458347, 0.9952524312151156119, 0.9942247191063767072, 0.99319885860609721757, 0.99217485087794918724, 0.9911526970731542393, 0.99013239833042073564, 0.98911395577588121696, 0.98809737052303012487, 0.98708264367266181896, 0.9860697763128088872, 0.9850587695186807602, 0.98404962435260263134, 0.9830423418639546915, 0.9820369230891116791, 0.9810333690513827582, 0.9800316807609517213, 0.9790318592148175292, 0.97803390539673519034, 0.97703782027715698627, 0.9760436048131740457, 0.9750512599484582765, 0.9740607866132046596, 0.97307218572407390726, 0.97208545818413549604, 0.97110060488281107593, 0.9701176266958182623, 0.9691365244851148162, 0.96815729909884321703, 0.9671799513712756342, 0.9662044821227593024, 0.96523089215966230685, 0.9642591822743197789, 0.96328935324498051455, 0.96232140583575401536, 0.96135534079655795837, 0.9603911588630661018, 0.9594288607566566262, 0.9584684471843609262, 0.95750991883881284516, 0.9565532763981983675, 0.95559852052620576933, 0.9546456518719762323, 0.95369467107005492736, 0.9527455787403425705, 0.9517983754880474582, 0.9508530619036379847, 0.94990963856279564875, 0.94896810602636854894, 0.94802846484032538184, 0.9470907155357099343, 0.94615485862859608895, 0.94522089462004333575, 0.94428882399605279887, 0.94335864722752378505, 0.9424303647702108547, 0.9415039770646814186, 0.9405794845362738724, 0.9396568875950562617, 0.93873618663578549225, 0.937817382037867083, 0.9369004741653154681, 0.9359854633667148537, 0.93507234997518062986, 0.9341611343083213436, 0.9332518166682012383, 0.93234439734130336125, 0.93143887659849324223, 0.93053525469498315066, 0.9296335318702969312, 0.9287337083482354233, 0.9278357843368424692, 0.92693976002837150975, 0.92604563559925277827, 0.9251534112100610881, 0.92426308700548422516, 0.9233746631142919406, 0.92248813964930555557, 0.9216035167073681722, 0.92072079436931550167, 0.9198399726999473084, 0.9189610517479994736, 0.91808403154611668254, 0.91720891211082573673, 0.91633569344250949575, 0.9154643755253814504, 0.9145949583274609296, 0.9137274418005489433, 0.9128618258802046672, 0.9119981104857225668, 0.911136295520110167, 0.9102763808700664688, 0.9094183664059610143, 0.9085622519818136034, 0.9077080374352746639, 0.9068557225876062779, 0.9060053072436638649, 0.90515679119187852523, 0.90431017420424004586, 0.9034654560362805681, 0.90262263642705892197, 0.9017817150991456276, 0.9009426917586085644, 0.9001055660949993122, 0.89927033778134016125, 0.89843700647411180063, 0.89760557181324167477, 0.8967760334220930224, 0.8959483909074545882, 0.8951226438595310155, 0.8942987918519339169, 0.89347683444167362685, 0.8926567711691516353, 0.89183860155815370286, 0.8910223251158436608, 0.89020794133275789523, 0.8893954496828005137, 0.88858484962323919964, 0.88777614059470175286, 0.88696932202117331366, 0.8861643933099942769, 0.88536135385185889176, 0.88456020302081454884, 0.8837609401742617558, 0.88296356465295479924, 0.8821680757810030967, 0.8813744728658732337, 0.88058275519839169105, 0.8797929220527482582, 0.8790049726865001341, 0.87821890634057671635, 0.8774347222392850749, 0.8766524195903161145, 0.87587199758475142063, 0.8750934553970707924, 0.87431679218516045925, 0.87354200709032198136, 0.8727690992372818337, 0.8719980677342016717, 0.8712289116726892793, 0.87046163012781019606, 0.86969622215810002424, 0.86893268680557741475, 0.868171023095757729, 0.86741123003766737703, 0.86665330662385883055, 0.86589725183042630846, 0.86514306461702213417, 0.8643907439268737637, 0.86364028868680148136, 0.8628916978072367635, 0.86214497018224130697, 0.8614001046895267209, 0.8606571001904748804, 0.8599159555301589408, 0.8591766695373650084, 0.858439241024614468, 0.8577036687881869642, 0.85696995160814403456, 0.85623808824835339124, 0.85550807745651385127, 0.8547799179641809104, 0.85405360848679296115, 0.85332914772369814935, 0.85260653435818187007, 0.8518857670574948969, 0.8511668444728821455, 0.850449765239612066, 0.8497345279770066632, 0.84902113128847214155, 0.8483095737615301704, 0.84759985396784977003, 0.84689197046327981263, 0.84618592178788213596, 0.8454817064659652679, 0.8447793230061187574, 0.84407876990124810904, 0.8433800456286103192, 0.84268314864985000815, 0.84198807741103614877, 0.8412948303426993829, 0.84060340585986992963, 0.83991380236211607245, 0.8392260182335832335, 0.83854005184303361805, 0.8378559015438864409, 0.8371735656742587133, 0.83649304255700660915, 0.8358143304997673775, 0.8351374277950018367, 0.8344623327200373951, 0.8337890435371116606, 0.8331175584934165524, 0.83244787582114301443, 0.8317799937375261971, 0.8311139104448912662, 0.8304496241306996307, 0.829787132967595843, 0.829126435113454841, 0.82846752871142994386, 0.827810411890001068, 0.8271550827630238321, 0.8265015394297786923, 0.82584977997502119, 0.82519980246903192517, 0.82455160496766801353, 0.8239051855124137733, 0.8232605421304335142, 0.82261767283462274263, 0.82197657562366249436, 0.82133724848207074543, 0.82069968938025865385, 0.82006389627458166716, 0.81942986710739829304, 0.8187975998071200661, 0.81816709228827389904, 0.8175383424515495209, 0.81691134818386719, 0.81628610735842030264, 0.8156626178347514895, 0.81504087745878658514, 0.81442088406292427276, 0.81380263546605516333, 0.8131861294736739363, 0.8125713638778734938, 0.81195833645749550467, 0.8113470449780843528, 0.8107374871921057163, 0.81012966083884003013, 0.80952356364472876016, 0.8089191933231918955, 0.8083165475752362095, 0.8077156240892352307, 0.807116420542144017, 0.80651893459953130015, 0.80592316391839910266, 0.80532910614874551734, 0.80473675894116134894, 0.8041461199548755721, 0.80355718688084262676, 0.8029699574745770044, 0.8023844296323622199, 0.80180060151529630963, 0.80121847181454279336, 0.80063804020522614017, 0.8000593083040496981, 0.799482281300595029, 0.79890697152530234924, 0.79833340419683291147, 0.79776163119851913156, 0.79719175056174007305, 0.79662396227115063186, 0.79605862843208634837, 0.7954965150286046428, 0.7949389304033263067, 0.79438886980073991634, 0.7938509173398694129, 0.79333725486180348076, 0.7928628062503825158, 0.79248016589594734975, 0.7922282564563652841, 0.79235268089281714, 0.7928669965958329811, 0.79500196303267009257, 0.7977754349256080071, 0.8070741533926414023, 0.81274951005883077666, 0.83587453438926902216, 0.8322925044598847628, 0.8611380275049211444, 0.83766847109895971676, 0.8635217960818226149, 0.8336462176900199515, 0.85666529174977472513, 0.8295961209906392146, 0.8517741414977239792, 0.82755262176881752985, 0.85009638600107804903, 0.82678897241388906646, 0.84991366776958333484, 0.8263292524477948085, 0.8498414702674938631, 0.82571821472165295294, 0.8494407601085285468, 0.82493747023620235965, 0.84881374292510267175, 0.8240975989695833806, 0.8481399356198378111, 0.82326910172422100684, 0.8474994797557941155, 0.8224649890194395471, 0.84689292165470942927, 0.821674587461245622, 0.8462995634250867126, 0.8208881881435310552, 0.84570701288254730157, 0.8201028625697962003, 0.84511339092836481175, 0.8193194204044092833, 0.8445208246360597205, 0.8185390854543088447, 0.8439310256198479841, 0.81776234754108487017, 0.8433444141689769169, 0.8169891643635681537, 0.8427607721832920602, 0.81621937842451374297, 0.8421798515419799113, 0.81545290446263164855, 0.8416015535248543915, 0.81468972818111505844, 0.8410258723880489283, 0.8139298567685111047, 0.8404528163061007287, 0.81317329120129700425, 0.83988237728276202095, 0.8124200234697964669, 0.8393145347878989975, 0.8116700421929760595, 0.83874926562364131614, 0.8109233364618152452, 0.83818654847934021267, 0.8101798965176836238, 0.8376263638969685739, 0.8094397131338138865, 0.8370686930712624527, 0.80870277710008450057, 0.8365135171997021429, 0.807969079099573054, 0.8359608174369244881, 0.80723860977230064875, 0.8354105750366587874, 0.80651135977969852335, 0.83486277143901902966, 0.805787319821360543, 0.83431738827979048235, 0.80506648062604505467, 0.83377440737191131447, 0.80434883294121663986, 0.833233810692203897, 0.8036343675284557782, 0.8326955803780486633, 0.8029230751623857748, 0.8321596987279571104, 0.8022149466299916807, 0.8316261482016369343, 0.8015099727292897195, 0.83109491141878925874, 0.80080814426760195624, 0.8305659711573757068, 0.8001094520598340924, 0.83003931035193574857, 0.79941388692689614666, 0.82951491209206637117, 0.7987214396942354493, 0.82899275962097380434, 0.7980321011904297392, 0.8284728363340233203, 0.79734586224582064526, 0.8279551257772732995, 0.7966627136911899217, 0.8274396116460062355, 0.7959826463564837354, 0.8269262777832672934, 0.79530565106958616854, 0.8264151081784131543, 0.79463171865514030336, 0.82590608696567037776, 0.7939608399334149329, 0.82539919842270279503, 0.7932930057192155174, 0.82489442696918833706, 0.7926282068208383863, 0.82439175716540615376, 0.79196643403906725344, 0.8238911737108346704, 0.791307678166211071, 0.82339266144276116485, 0.7906519299851822286, 0.8228962053349032413, 0.7899991802686141317, 0.8224017904960426083, 0.78934941977801722753, 0.8219094021686715262, 0.788702639262972585, 0.82141902572765224187, 0.78805882946036215526, 0.8209306466788897057, 0.7874179810936348713, 0.82044425065801783264, 0.78678008487210776426, 0.8199598234290995119, 0.78614513149030130254, 0.81947735088334055874, 0.78551311162730818585, 0.81899681903781778686, 0.7848840159461948474, 0.81851821403422130106, 0.7842578350934349399, 0.81804152213761115094, 0.78363455969837410753, 0.81756672973518841, 0.7830141803727253647, 0.81709382333508074036, 0.78239668771009442274, 0.8166227895651425045, 0.78178207228553432825, 0.8161536151717694122, 0.78117032465512879525, 0.8156862870187277333, 0.78056143535560363077, 0.8152207920859980397, 0.77995539490396567406, 0.81475711746863346726, 0.7793521937971686834, 0.81429525037563241886, 0.77875182251180562323, 0.8138351781288256788, 0.7781542715038268205, 0.81337688816177785076, 0.77755953120828347224, 0.8129203680187030209, 0.7769675920390960026, 0.8124656053533945782, 0.77637844438884677674, 0.81201258792816904334, 0.7757920786285966997, 0.8115613036128238268, 0.7752084851077252337, 0.81111174038360875555, 0.7746276541537933816, 0.8106638863222112588, 0.77404957607242919434, 0.8102177296147550425, 0.7734742411472353731, 0.8097732585508121241, 0.7729016396397185426, 0.8093304615224280509, 0.7723317617892397841, 0.8088893270231601563, 0.7717645978129860246, 0.808449843647128663, 0.7712001379059618876, 0.8080120000880804756, 0.77063837224100161535, 0.8075757851384654774, 0.7700792909688006821, 0.8071411876885251413, 0.7695228842179667282, 0.8067081967253932846, 0.7689691420950894399, 0.8062768013322087527, 0.76841805468482901885, 0.8058469906872398693, 0.76786961205002288085, 0.80541875406302042235, 0.76732380423181023376, 0.80499208082549702717, 0.7667806212497741847, 0.80456696043318763135, 0.7662400531021010362, 0.8041433824363509835, 0.76570208976575642926, 0.8037213364761668586, 0.7651667211966780002, 0.8033008122839268316, 0.764633937329984214, 0.80288179968023540647, 0.7641037280801990516, 0.80246428857422128276, 0.7635760833414922171, 0.8020482689627585771, 0.76305099298793454404, 0.80163373092969777625, 0.7625284468737682801, 0.801220664645106228, 0.762008434833691925, 0.8008090603645179734, 0.76149094668315930764, 0.8003989084281927084, 0.7609759722186925839, 0.7999901992603836807, 0.7604635012182088404, 0.7995829233686143277, 0.75995352344135999015, 0.7991770713429634503, 0.7594460286298856456, 0.79877263385535872734, 0.75894100650797866135, 0.79836960165887838845, 0.7584384467826630332, 0.7979679655870608414, 0.75793833914418384134, 0.79756771655322206507, 0.7574406732664089355, 0.79716884554978059035, 0.75694543880724204655, 0.79677134364758987604, 0.75645262540904702057, 0.79637520199527789857, 0.7559622226990828691, 0.79598041181859377594, 0.75547422028994932596, 0.7955869644197612512, 0.7549886077800426068, 0.7951948511768388555, 0.75450537475402106713, 0.79480406354308658675, 0.7540245107832804503, 0.79441459304633893187, 0.75354600542643842485, 0.7940264312883840576, 0.75306984822982810443, 0.79363956994434902684, 0.7525960287280002504, 0.79325400076209086776, 0.7521245364442338502, 0.7928697155615933419, 0.7516553608910547731, 0.79248670623436925773, 0.75118849157076219983, 0.79210496474286818286, 0.75072391797596252664, 0.79172448311988941614, 0.75026162959011044205, 0.7913452534680000548, 0.7498016158880568794, 0.79096726795895804055, 0.749343866336603546, 0.79059051883314004666, 0.7488883703950637295, 0.7902149983989740496, 0.74843511751582908904, 0.78984069903237649605, 0.74798409714494213236, 0.7894676131761938989, 0.7475352987226740881, 0.78909573333964877934, 0.7470887116841078761, 0.7887250520977898009, 0.7466443254597258891, 0.78835556209094601764, 0.74620212947600229333, 0.78798725602418508976, 0.745762113155999557, 0.78762012666677538153, 0.745324265919968926, 0.78725416685165184894, 0.7448885771859545556, 0.7868893694748855687, 0.74445503637040101835, 0.7865257274951568777, 0.7440236328887639042, 0.78616323393323197177, 0.74359435615612323625, 0.7858018818714429121, 0.74316719558779942143, 0.7854416644531709317, 0.74274214059997146424, 0.7850825748823329736, 0.7423191806102971677, 0.78472460642287137077, 0.74189830503853505204, 0.7843677523982465911, 0.7414795033071677264, 0.7840120061909329896, 0.7410627648420264399, 0.78365736124191747965, 0.74064807907291656014, 0.7833038110502010664, 0.74023543543424371016, 0.7829513491723031824, 0.73982482336564031155, 0.7825999692217687532, 0.7394162323125922784, 0.78224966486867795546, 0.7390096517270656117, 0.7819004298391585907, 0.738605071068132645, 0.7815522579149010401, 0.7382024798025976989, 0.78120514293267575597, 0.7378018674056219002, 0.7808590787838532187, 0.73740322336134693217, 0.7805140594139263581, 0.73700653716351747667, 0.7801700788220353554, 0.7366117983161021215, 0.7798271310604948243, 0.7362189963339125043, 0.7794852102343233235, 0.7358281207432204682, 0.77914431050077515803, 0.7354391610823730131, 0.7788044260688744628, 0.73505210690240482466, 0.7784655511989515297, 0.7346669477676481689, 0.77812768020218135056, 0.7342836732563399467, 0.7777908074401243571, 0.7339022729612257022, 0.77745492732426935226, 0.73352273649016038815, 0.7771200343155785932, 0.73314505346670569047, 0.77678612292403501565, 0.7327692135307237231, 0.77645318770819161556, 0.73239520633896690305, 0.7761212232747229241, 0.7320230215656638282, 0.77579022427797861625, 0.73165264890310097406, 0.7754601854195392214, 0.7312840780622000394, 0.7751311014477739284, 0.7309172987730907687, 0.77480296715740049377, 0.73055230078567909096, 0.7744757773890472419, 0.7301890738702104077, 0.7741495270288171636, 0.72982760781782788045, 0.7738242110078540993, 0.7294678924411255603, 0.77349982430191103073, 0.7291099175746962206, 0.7731763619309204687, 0.7287536730756737403, 0.77285381895856694536, 0.7283991488242699086, 0.7725321904918616247, 0.72804633472430551244, 0.7722114716807190243, 0.7276952207037355805, 0.7718916577175358768, 0.7273457967151686589, 0.77157274383677211575, 0.726998052736379998, 0.7712547253145340274, 0.72665197877081853654, 0.77093759746815954353, 0.7263075648481075709, 0.77062135565580572495, 0.7259648010245390064, 0.7703059952760384193, 0.7256236773835610846, 0.7699915117674241255, 0.72528418403625949265, 0.7696779006081240672, 0.7249463111218317624, 0.7693651573154905033, 0.7246100488080548687, 0.76905327744566527907, 0.7242753872917459444, 0.768742256593180642, 0.723942316799216033, 0.7684320903905623378, 0.7236108275867168039, 0.76812277450793500094, 0.72328090994088015716, 0.76781430465262985835, 0.72295255417915065585, 0.76750667656879476887, 0.7226257506502107182, 0.76719988603700661197, 0.72230048973439851736, 0.76689392887388604415, 0.72197676184411852944, 0.76658880093171464116, 0.7216545574242446834, 0.76628449809805445145, 0.72133386695251606633, 0.765981016295369966, 0.72101468093992514285, 0.7656783514806525417, 0.72069698993109844806, 0.7653764996450472767, 0.72038078450466972444, 0.76507545681348237325, 0.7200660552736454701, 0.76477521904430100083, 0.7197527928857628725, 0.7644757824288956697, 0.71944098802384010433, 0.7641771430913451497, 0.71913063140611896537, 0.76387929718805393545, 0.71882171378659985263, 0.76358224090739428724, 0.7185142259553690477, 0.76328597046935086723, 0.7182081587389183139, 0.7629904821251679666, 0.717903503000456797, 0.76269577215699937495, 0.71760024964021523053, 0.76240183687756087424, 0.7172983895957424454, 0.7621086726297854032, 0.7169979138421941886, 0.7618162757864808805, 0.71669881339261426294, 0.76152464274999072846, 0.71640107929820799366, 0.76123376995185708236, 0.71610470264860804214, 0.7609436538524867388, 0.71580967457213257813, 0.7606542909408198084, 0.7155159862360358373, 0.76036567773400113706, 0.7152236288467510802, 0.7600778107770544616, 0.71493259365012598464, 0.7597906866425593536, 0.71464287193165049525, 0.75950430193033092866, 0.7143544550166771644, 0.75921865326710235406, 0.71406733427063401635, 0.7589337373062101549, 0.71378150109922997084, 0.7586495507272823342, 0.71349694694865286557, 0.75836609023592930996, 0.71321366330576011644, 0.7580833525634376843, 0.7129316416982620618, 0.7578013344664668525, 0.71265087369489803123, 0.7575200327267484414, 0.71237135090560519, 0.75723944415078861704, 0.71209306498168020683, 0.75695956556957323875, 0.7118160076159337964, 0.75668039383827587246, 0.7115401705428381894, 0.75640192583596867464, 0.71126554553866758667, 0.7561241584653361443, 0.71099212442163165133, 0.7558470886523917498, 0.7107198990520020997, 0.7555707133461974262, 0.7104488613322324507, 0.7552950295185859525, 0.71017900320707099337, 0.7550200341638862065, 0.70991031666366703913, 0.75474572429865129905, 0.70964279373167051874, 0.75447209696138957977, 0.7093764264833249945, 0.75419914921229853325, 0.7091112070335541505, 0.75392687813300152865, 0.70884712754004183225, 0.7536552808262874673, 0.7085841802033057054, 0.75338435441585327847, 0.70832235726676459945, 0.7531140960460492948, 0.70806165101679961646, 0.75284450288162747984, 0.7078020537828090691, 0.7525755721074925179, 0.70754355793725732545, 0.75230730092845575187, 0.7072861558957176368, 0.7520396865689919681, 0.70702984011690901976, 0.751772726272999012, 0.7067746031027272713, 0.75150641730356024033, 0.7065204373982701914, 0.7512407569427097921, 0.70626733559185709497, 0.75097574249120066997, 0.7060152903150426839, 0.7507113712682756269, 0.7057642942426253656, 0.75044764061144084784, 0.70551434009265009, 0.750184547876242409, 0.70526542062640579017, 0.74992209043604550776, 0.70501752864841750256, 0.74966026568181646383, 0.70477065700643324843, 0.7493990710219074526, 0.7045247985914057573, 0.7491385038818439773, 0.7042799463374691111, 0.74887856170411506903, 0.70403609322191039395, 0.74861924194796618893, 0.70379323226513642437, 0.74836054208919482005, 0.70355135653063565344, 0.74810245961994874595, 0.7033104591249353089, 0.7478449920485269798, 0.7030705331975538694, 0.7475881368991833599, 0.70283157194094894474, 0.74733189171193275033, 0.7025935685904606505, 0.74707625404235987975, 0.70235651642425055103, 0.74682122146143076446, 0.7021204087632362587, 0.7465667915553067177, 0.7018852389710217635, 0.74631296192516091194, 0.7016510004538235806, 0.746059730186997498, 0.7014176866603927905, 0.7458070939714732374, 0.70118529108193305716, 0.7455550509237216527, 0.7009538072520146988, 0.74530359870317965175, 0.70072322874648489756, 0.74505273498341663456, 0.7004935491833741203, 0.7448024574519660355, 0.7002647622227988362, 0.74455276381015929684, 0.70003686156686060446, 0.74430365177296225755, 0.69980984095954161436, 0.7440551190688139131, 0.699583694186596754, 0.74380716343946755715, 0.6993584150754422828, 0.743559782639834255, 0.699133997495041189, 0.74331297443782864666, 0.6989104353557853053, 0.74306673661421704965, 0.6986877226093742597, 0.74282106696246784, 0.69846585324869133593, 0.7425759632886040888, 0.69824482130767632, 0.74233142341105844315, 0.6980246208611954052, 0.74208744516053020513, 0.6978052460249082293, 0.7418440263798446156, 0.69758669095513211906, 0.74160116492381429246, 0.69736894984870360963, 0.7413588586591028239, 0.69715201694283731665, 0.7411171054640904775, 0.6969358865149822265, 0.7408759032287420072, 0.69672055288267547885, 0.740635249854476543, 0.69650601040339370825, 0.740395143254039522, 0.69629225347440201646, 0.74015558135137665864, 0.69607927653260064, 0.73991656208150991203, 0.6958670740543693856, 0.73967808339041544145, 0.69565564055540989464, 0.73944014323490351936, 0.69544497059058580756, 0.7392027395825003754, 0.6952350587537608884, 0.7389658704113319581, 0.69502589967763517915, 0.73872953371000957886, 0.69481748803357924385, 0.7384937274775174265, 0.69460981853146656765, 0.73825844972310191653, 0.69440288591950417125, 0.73802369846616286425, 0.6941966849840615026, 0.7377894717361464461, 0.6939912105494976676, 0.737555767572439936, 0.6937864574779870586, 0.737322584024268187, 0.69358242066934343924, 0.7370899191505918371, 0.6933790950608425452, 0.73685777102000721425, 0.69317647562704325553, 0.7366261377106479226, 0.69297455737960739674, 0.73639501731008807694, 0.6927733353671182278, 0.73616440791524717695, 0.6925728046748976671, 0.7359343076322965766, 0.6923729604248223128, 0.73570471457656755175, 0.69217379777513831106, 0.7354756268724609265, 0.69197531192027512395, 0.7352470426533582312, 0.69177749809065824764, 0.73501896006153438756, 0.69158035155252093516, 0.73479137724807188774, 0.6913838676077149705, 0.73456429237277643665, 0.6911880415935205452, 0.7343377036040940554, 0.6909928688824552849, 0.7341116091190296071, 0.6907983448820824742, 0.73388600710306672853, 0.69060446503481852707, 0.73366089575008914967, 0.6904112248177397476, 0.7334362732623033723, 0.6902186197423884279, 0.73321213785016269384, 0.69002664535457832644, 0.7329884877322925463, 0.689835297234199571, 0.7327653211354171386, 0.6896445709950230311, 0.7325426362942873777, 0.68945446228450419713, 0.7323204314516100394, 0.6892649667835866125, 0.73209870485797818153, 0.68907608020650489955, 0.73187745477180277706, 0.6888877983005874128, 0.7316566794592455328, 0.6887001168460585672, 0.73143637719415289085, 0.6885130316558408741, 0.73121654625799118853, 0.68832653857535672215, 0.73099718493978294227, 0.6881406334823299461, 0.7307782915360442634, 0.6879553122865872115, 0.7305598643507233615, 0.6877705709298592571, 0.7303419016951401272, 0.68758640538558202633, 0.7301244018879267773, 0.6874028116586977228, 0.7299073632549695403, 0.6872197857854558231, 0.72969078412935135905, 0.6870373238332140792, 0.72947466285129560263, 0.6868554219002395426, 0.72925899776811076234, 0.6866740761155096418, 0.72904378723413611733, 0.6864932826385133418, 0.72882902961068834054, 0.6863130376590524179, 0.7286147232660090588, 0.68613333739704287167, 0.7284008665752132991, 0.68595417810231651634, 0.7281874579202388592, 0.6857755560544227624, 0.7279744956897965475, 0.6855974675624306286, 0.7277619782793212876, 0.68541990896473100383, 0.7275499040909240719, 0.6852428766288391897, 0.72733827153334475765, 0.6850663669511977446, 0.72712707902190565747, 0.6848903763569796561, 0.72691632497846595597, 0.6847149012998918674, 0.72670600783137689215, 0.6845399382619791779, 0.7264961260154377208, 0.68436548375342854186, 0.7262866779718524253, 0.68419153431237379027, 0.72607766214818717074, 0.6840180865047007928, 0.72586907699832848006, 0.6838451369238530859, 0.72566092098244212866, 0.6836726821906379831, 0.7254531925669327201, 0.6835007189530331913, 0.7252458902244039595, 0.68332924388599395, 0.7250390124336195845, 0.68315825369126071374, 0.72483255767946495624, 0.68298774509716739624, 0.72462652445290928814, 0.68281771485845019275, 0.7244209112509685047, 0.68264815975605699916, 0.72421571657666871465, 0.6824790765969574461, 0.72401093893901028543, 0.6823104622139535597, 0.72380657685293250497, 0.68214231346549107106, 0.7236026288392788258, 0.68197462723547138584, 0.7233990934247626694, 0.6818074004330642285, 0.7231959691419337831, 0.68164062999252097904, 0.7229932545291451406, 0.6814743128729887128, 0.72279094813052036827, 0.68130844605832495934, 0.7225890484959216925, 0.6811430265569131917, 0.7223875541809183928, 0.68097805140147905884, 0.72218646374675574373, 0.6808135176489073761, 0.7219857757603244552, 0.68064942238005988166, 0.72178548879413057063, 0.68048576269959377266, 0.7215856014262658443, 0.68032253573578103185, 0.721386112240378552, 0.6801597386403285548, 0.72118701982564477186, 0.6799973685881990885, 0.7209883227767400683, 0.6798354227774329896, 0.7207900196938116222, 0.6796738984289708137, 0.7205921091824507626, 0.679512792786476744, 0.72039458985366590463, 0.67935210311616286776, 0.7201974603238558895, 0.67919182670661430804, 0.72000071921478370044, 0.6790319608686152211, 0.71980436515355055815, 0.6788725029349756642, 0.7196083967725703925, 0.67871345026035934404, 0.71941281270954466275, 0.6785548002211122495, 0.719217611607437537, 0.6783965502150921781, 0.71902279211445140967, 0.6782386976614991614, 0.71882835288400276033, 0.67808124000070679554, 0.7186342925746983323, 0.6779241746940944807, 0.7184406098503116356, 0.67776749922388057954, 0.7182473033797597614, 0.67761121109295649386, 0.7180543718370804947, 0.67745530782472166835, 0.7178618139014097358, 0.67729978696291952544, 0.7176696282569592031, 0.67714464607147433346, 0.71747781359299441395, 0.6769898827343290137, 0.7172863686038129581, 0.6768354945552838906, 0.7170952919887230243, 0.6766814791578363865, 0.7169045824520221968, 0.6765278341850216662, 0.7167142387029765076, 0.67637455729925423224, 0.71652425945579973753, 0.6762216461821704764, 0.71633464342963296405, 0.6760690985344721864, 0.71614538934852434014, 0.67591691207577101223, 0.71595649594140911435, 0.67576508454443389486, 0.71576796194208986785, 0.6756136136974294566, 0.7155797860892169799, 0.6754624973101753562, 0.7153919671262693008, 0.6753117331763866098, 0.7152045038015350397, 0.675161319107924879, 0.71501739486809285393, 0.6750112529346487276, 0.7148306390837931396, 0.67486153250426484507, 0.7146442352112395119, 0.67471215568218024085, 0.7144581820177704809, 0.67456312035135540766, 0.71427247827544130393, 0.67441442441215845437, 0.7140871227610060241, 0.6742660657822202089, 0.71390211425589967994, 0.6741180423962902902, 0.7137174515462206866, 0.6739703522060941495, 0.71353313342271338076, 0.67382299318019108067, 0.7133491586807507333, 0.6736759633038331985, 0.7131655261203172115, 0.6735292605788253827, 0.71298223454599179955, 0.67338288302338619045, 0.7127992827669311687, 0.6732368286720097316, 0.7126166695968529903, 0.6730910955753285095, 0.7124343938540193911, 0.67294568179997722257, 0.7122524543612205485, 0.67280058542845752785, 0.71207084994575841784, 0.67265580455900376284, 0.71188957943943059287, 0.67251133730544962496, 0.7117086416785142908, 0.6723671817970958052, 0.71152803550375046175, 0.67222333617857857774, 0.7113477597603280229, 0.67207979860973933635, 0.71116781329786820563, 0.67193656726549508355, 0.7109881949704090175, 0.6717936403357098637, 0.7108089036363898198, 0.67165101602506714054, 0.7106299381586360071, 0.6715086925529431148, 0.71045129740434379765, 0.6713666681532809832, 0.71027298024506512587, 0.6712249410744661289, 0.7100949855566926316, 0.6710835095792022473, 0.7099173122194447498, 0.6709423719443884005, 0.7097399591178508935, 0.67080152646099699916, 0.7095629251407367317, 0.67066097143395270603, 0.7093862091812095537, 0.6705207051820122598, 0.70920981013664372044, 0.6703807260376452172, 0.70903372690866620515, 0.6702410323469156067, 0.7088579584031422167, 0.67010162246936449184, 0.7086825035301608948, 0.6699624947778934412, 0.7085073612040210975, 0.66982364765864890066, 0.70833253034321725316, 0.6696850795109074637, 0.70815800987042529496, 0.6695467887469620377, 0.7079837987124886626, 0.6694087737920088996, 0.70780989580040437853, 0.66927103308403564037, 0.70763630006930918924, 0.6691335650737099913, 0.70746301045846577863, 0.66899636822426952895, 0.70729002591124903927, 0.668859441011412257, 0.7071173453751324152, 0.66872278192318805666, 0.70694496780167430145, 0.66858638945989100523, 0.70677289214650450565, 0.6684502621339525558, 0.70660111736931076516, 0.66831439846983557544, 0.7064296424338253289, 0.6681787970039292387, 0.70625846630781158775, 0.66804345628444476685, 0.70608758796305076373, 0.66790837487131201564, 0.70591700637532864743, 0.66777355133607690154, 0.7057467205244223928, 0.6676389842617996647, 0.7055767293940873604, 0.6675046722429539628, 0.7054070319720440074, 0.6673706138853267926, 0.7052376272499648288, 0.6672368078059192333, 0.70506851422346134245, 0.66710325263284800753, 0.70489969189207112547, 0.66696994700524785555, 0.70473115925924488653, 0.66683688957317471834, 0.7045629153323335894, 0.6667040789975097234, 0.704394959122575615, 0.66657151394986397045, 0.70422728964508396883, 0.66643919311248411163, 0.7040599059188335224, 0.66630711517815871905, 0.703892806966648301, 0.6661752788501254407, 0.7037259918151888073, 0.666043682841978933, 0.7035594594949393836, 0.6659123258775795701, 0.7033932090401956097, 0.66578120669096292405, 0.70322723948905174084, 0.6656503240262500087, 0.7030615498833881759, 0.66551967663755828565, 0.70289613926885896394, 0.665389263288913426, 0.7027310066948793472, 0.66525908275416182285, 0.70256615121461333173, 0.6651291338168838485, 0.7024015718849612918, 0.6649994152703078554, 0.70223726776654761485, 0.66486992591722491263, 0.70207323792370836366, 0.6647406645699042722, 0.7019094814244789854, 0.6646116300500095632, 0.7017459973405820332, 0.6644828211885157091, 0.7015827847474149383, 0.66435423682562655856, 0.7014198427240377915, 0.66422587581069323065, 0.7012571703531611669, 0.6640977370021331669, 0.70109476672113396975, 0.6639698192673498839, 0.7009326309179313098, 0.66384212148265342484, 0.70077076203714240517, 0.6637146425331815027, 0.70060915917595851317, 0.66358738131282133066, 0.7004478214351608865, 0.66346033672413213606, 0.70028674791910875704, 0.6633335076782683504, 0.70012593773572734114, 0.663206893094903474, 0.6999653899964958783, 0.6630804919021546083, 0.699805103816435689, 0.6629543030365076503, 0.6996450783140982585, 0.6628283254427431459, 0.69948531261155334434, 0.6627025580738627978, 0.69932580583437711357, 0.66257699989101662033, 0.6991665571116402972, 0.66245164986343073907, 0.69900756557589637023, 0.6623265069683358305, 0.6988488303631697603, 0.6622015701908961935, 0.69869035061294406893, 0.66207683852413945307, 0.69853212546815032893, 0.6619523109688868866, 0.69837415407515527396, 0.6618279865336843724, 0.69821643558374963566, 0.6617038642347339513, 0.6980589691471364627, 0.6615799430958260006, 0.6979017539219194633, 0.6614562221482720122, 0.6977447890680913631, 0.66133270043083797494, 0.6975880737490222986, 0.6612093769896783494, 0.6974316071314482159, 0.6610862508782706376, 0.6972753883854593042, 0.6609633211573505393, 0.69711941668448844476, 0.6608405868948476905, 0.69696369120529968224, 0.6607180471658219802, 0.6968082111279767187, 0.66059570105240044097, 0.6966529756359114267, 0.66047354764371470775, 0.69649798391579238694, 0.66035158603583904186, 0.6963432351575934411, 0.66022981533172891375, 0.6961887285545622741, 0.660108234641160143, 0.69603446330320900874, 0.65998684308066858667, 0.695880438603294828, 0.65986563977349037654, 0.695726653657820614, 0.65974462384950269657, 0.6955731076730156112, 0.6596237944451650989, 0.69541979985832610343, 0.65950315070346135233, 0.6952667294264041235, 0.6593826917738418205, 0.69511389559309617214, 0.6592624168121663634, 0.6949612975774319627, 0.65914232498064776, 0.6948089346016131871, 0.6590224154477956459, 0.69465680589100230115, 0.65890268738836096343, 0.69450491067411133136, 0.65878313998328091807, 0.6943532481825907002, 0.65866377241962443775, 0.6942018176512180745, 0.6585445838905381321, 0.69405061831788723376, 0.6584255735951927461, 0.6938996494235969623, 0.6583067407387301029, 0.6937489102124399529, 0.65818808453221053396, 0.69359839993159174245, 0.65806960419256079105, 0.69344811783129966013, 0.6579512989425224374, 0.69329806316487180317, 0.65783316801060071077, 0.69314823518866602544, 0.65771521063101385797, 0.6929986331620789546, 0.6575974260436429354, 0.69284925634753502505, 0.6574798134939820706, 0.6927001040104755361, 0.65736237223308918265, 0.69255117541934772574, 0.6572451015175371554, 0.69240246984559386913, 0.6571280006093654622, 0.69225398656364040006, 0.657011068776032236, 0.69210572485088704795, 0.65689430529036678223, 0.69195768398769600164, 0.6567777094305225288, 0.6918098632573810901, 0.65666128047993041277, 0.6916622619461969877, 0.65654501772725269606, 0.6915148793433284383, 0.6564289204663372091, 0.6913677147408795018, 0.6563129879961720175, 0.6912207674338628254, 0.65619721962084050936, 0.6910740367201889309, 0.6560816146494768958, 0.69092752190065552623, 0.6559661723962221261, 0.6907812222789368383, 0.65585089218018021107, 0.6906351371615729706, 0.65573577332537495027, 0.69048926585795927796, 0.65562081516070706166, 0.6903436076803357665, 0.65550601701991170766, 0.69019816194377651264, 0.6553913782415164164, 0.6900529279661791073, 0.6552768981687993947, 0.68990790506825412214, 0.6551625761497482253, 0.68976309257351459505, 0.65504841153701895054, 0.68961848980826554004, 0.6549344036878955346, 0.6894740961015934805, 0.654820551964249703, 0.689329910785356001, 0.6547068557325011566, 0.68918593319417132716, 0.65459331436357815365, 0.6890421626654079245, 0.65447992723287846007, 0.68889859853917411765, 0.65436669372023066317, 0.68875524015830774075, 0.6542536132098558436, 0.68861208686836580047, 0.6541406850903296068, 0.68846913801761416927, 0.6540279087545444653, 0.68832639295701729854, 0.65391528359967257403, 0.6881838510402279549, 0.65380280902712881055, 0.688041511623576983, 0.6536904844425341997, 0.68789937406606308163, 0.6535783092556796805, 0.6877574377293426196, 0.65346628288049020893, 0.68761570197771945873, 0.6533544047349891966, 0.68747416617813481037, 0.65324267424126327954, 0.68733282970015710967, 0.6531310908254274164, 0.6871916919159719189, 0.65301965391759031254, 0.6870507522003718501, 0.6529083629518201656, 0.68691000993074651285, 0.65279721736611073037, 0.6867694644870724858, 0.65268621660234770094, 0.6866291152519033148, 0.6525753601062754059, 0.68648896161035952737, 0.6524646473274638143, 0.6863490029501186815, 0.65235407771927584925, 0.68620923866140542995, 0.65224365073883500653, 0.686069668136981614, 0.6521333658469932753, 0.6859302907721363792, 0.65202322250829935785, 0.6857911059646763158, 0.6519132201909671857, 0.68565211311491562276, 0.6518033583668447293, 0.6855133116256662983, 0.65169363651138310017, 0.68537470090222835464, 0.6515840541036059381, 0.68523628035238005234, 0.6514746106260790865, 0.68509804938636816913, 0.6513653055648805505, 0.6849600074168982842, 0.65125613840957073333, 0.68482215385912509115, 0.65114710865316295153, 0.6846844881306427362, 0.6510382157920942248, 0.68454700965147517993, 0.6509294593261963393, 0.6844097178440665864, 0.65082083875866717827, 0.6842726121332717333, 0.6507123535960423218, 0.6841356919463464506, 0.65060400334816691085, 0.68399895671293808354, 0.65049578752816777303, 0.68386240586507597913, 0.65038770565242580863, 0.6837260388371620015, 0.65027975724054863334, 0.6835898550659610674, 0.6501719418153434764, 0.6834538539905917129, 0.65006425890279033035, 0.6833180350525166811, 0.6499567080320153519, 0.6831823976955335368, 0.6498492887352645089, 0.6830469413657653069, 0.64974200054787747407, 0.6829116655116511469, 0.6496348430082617609, 0.6827765695839370313, 0.64952781565786709973, 0.68264165303566647135, 0.6494209180411600533, 0.68250691532217125644, 0.6493141497055988679, 0.68237235590106222645, 0.64920751020160855695, 0.68223797423222006046, 0.64910099908255621903, 0.6821037697777861019, 0.6489946159047265824, 0.68196974200215320223, 0.6488883602272977781, 0.68183589037195659277, 0.64878223161231733814, 0.6817022143560647839, 0.64867622962467841606, 0.68156871342557048897, 0.648570353832096228, 0.6814353870537815728, 0.64846460380508471325, 0.68130223471621202853, 0.64835897911693341015, 0.68116925589057298137, 0.64825347934368454753, 0.6810364500567637146, 0.64814810406411034786, 0.6809038166968627261, 0.6480428528596905403, 0.68077135529511880667, 0.6479377253145900828, 0.68063906533794214934, 0.6478327210156370907, 0.6805069463138954808, 0.64772783955230096864, 0.6803749977136852234, 0.64762308051667074574, 0.68024321903015267835, 0.64751844350343361074, 0.6801116097582652384, 0.6474139281098536453, 0.6799801693951076276, 0.6473095339357507545, 0.6798488974398731659, 0.6472052605834797917, 0.6797177933938550595, 0.6471011076579098765, 0.67958685676043772, 0.64699707476640390256, 0.6794560870450881073, 0.64689316151879823583, 0.6793254837553471023, 0.6467893675273825995, 0.67919504640082090124, 0.64668569240688014363, 0.6790647744931724425, 0.64658213577442770037, 0.678934667546112855, 0.6464786972495562188, 0.6788047250753929359, 0.646375376454171381, 0.6786749465987946522, 0.64627217301253439677, 0.6785453316361226743, 0.6461690865512429748, 0.67841587970919592804, 0.64606611669921246795, 0.6782865903418391821, 0.6459632630876571933, 0.67815746305987465413, 0.6458605253500719223, 0.67802849739111364924, 0.64575790312221354096, 0.6778996928653482218, 0.64565539604208288044, 0.6777710490143428668, 0.64555300374990671125, 0.6776425653718262337, 0.64545072588811990385, 0.6775142414734828697, 0.64534856210134775316, 0.6773860768569449905, 0.6452465120363884644, 0.6772580710617842757, 0.64514457534219579846, 0.6771302236295036901, 0.64504275166986187786, 0.67700253410352933587, 0.6449410406726001479, 0.6768750020292023275, 0.6448394420057284939, 0.67674762695377069113, 0.64473795532665251375, 0.67662040842638129865, 0.64463658029484894224, 0.67649334599807182174, 0.6445353165718492258, 0.67636643922176271205, 0.6444341638212232483, 0.676239687652249214, 0.6443331217085632045, 0.6761130908461933986, 0.64423218990146762007, 0.6759866483621162257, 0.64413136806952551707, 0.67586035976038963245, 0.6440306558843007238, 0.67573422460322865064, 0.6439300530193163265, 0.6756082424546835468, 0.643829559150039263, 0.6754824128806319926, 0.64372917395386505574, 0.67535673544877125845, 0.6436288971101026826, 0.6752312097286104349, 0.6435287282999595866, 0.67510583529146268313, 0.64342866720652681906, 0.6749806117104375073, 0.64332871351476431955, 0.6748555385604330583, 0.6432288669114863267, 0.674730615418128458, 0.64312912708534692303, 0.6746058418619761567, 0.64302949372682570824, 0.67448121747219431195, 0.64292996652821360364, 0.67435674183075919606, 0.64283054518359878353, 0.67423241452139763, 0.64273122938885273364, 0.67410823512957944236, 0.6426320188416164349, 0.67398420324250995513, 0.642532913241286672, 0.67386031844912249806, 0.64243391228900246473, 0.67373658034007094514, 0.64233501568763162195, 0.67361298850772228255, 0.642236223141757415, 0.673489542546149198, 0.6421375343576653719, 0.6733662420511226971, 0.6420389490433301888, 0.6732430866201047504, 0.6419404669084027605, 0.6731200758522409625, 0.6418420876641973251, 0.6729972093483532657, 0.64174381102367872493, 0.67287448671093264555, 0.64164563670144978064, 0.6727519075441318875, 0.64154756441373877914, 0.6726294714537583526, 0.64144959387838707107, 0.6725071780472667753, 0.6413517248148367819, 0.67238502693375209465, 0.64125395694411862986, 0.6722630177239423019, 0.64115628998883985397, 0.67214115003019132345, 0.641058723673172248, 0.6720194234664719196, 0.64096125772284030277, 0.6718978376483686222, 0.6408638918651094523, 0.67177639219307068484, 0.64076662582877442467, 0.67165508671936506645, 0.6406694593441476961, 0.6715339208476294404, 0.6405723921430480473, 0.67141289419982522646, 0.64047542395878922204, 0.67129200639949065054, 0.6403785545261686843, 0.6711712570717338289, 0.6402817835814564761, 0.67105064584322587955, 0.6401851108623841739, 0.670930172342194059, 0.6400885361081339415, 0.6708098361984149207, 0.6399920590593276795, 0.6706896370432075058, 0.6398956794580162713, 0.67056957450942655325, 0.6397993970476689224, 0.67044964823145573846, 0.63970321157316259534, 0.67032985784520093836, 0.6396071227807715351, 0.67021020298808351714, 0.6395111304181568884, 0.6700906832990336437, 0.63941523423435641255, 0.6699712984184836271, 0.6393194339797742759, 0.6698520479883612848, 0.63922372940617094595, 0.66973293165208333023, 0.6391281202666531678, 0.66961394905454878825, 0.63903260631566402794, 0.66949509984213243446, 0.63893718730897310694, 0.66937638366267826126, 0.6388418630036667167, 0.66925780016549296777, 0.6387466331581382227, 0.66913934900133947327, 0.6386514975320784517, 0.66902102982243045776, 0.63855645588646618114, 0.6689028422824219285, 0.63846150798355871293, 0.66878478603640680587, 0.63836665358688252753, 0.66866686074090854024, 0.6382718924612240201, 0.66854906605387475046, 0.63817722437262031673, 0.6684314016346708866, 0.63808264908835017, 0.6683138671440739196, 0.6379881663769249329, 0.6681964622442660526, 0.637893776008079611, 0.6680791865988284577, 0.63779947775276399266, 0.6679620398727350399, 0.6377052713831338548, 0.6678450217323462222, 0.6376111566725422445, 0.6677281318454027555, 0.63751713339553083514, 0.66761136988101955295, 0.6374232013278213585, 0.6674947355096795514, 0.63732936024630710864, 0.66737822840322759134, 0.6372356099290445203, 0.66726184823486432723, 0.63714195015524481715, 0.667145594679140157, 0.6370483807052657346, 0.6670294674119491784, 0.6369549013606033109, 0.66691346611052316904, 0.63686151190388374946, 0.66679759045342558795, 0.6367682121188553505, 0.6666818401205456045, 0.63667500179038051165, 0.666566214793092149, 0.6365818807044277961, 0.66645071415358798765, 0.63648884864806406944, 0.66633533788586382066, 0.6363959054094467015, 0.6662200856750524053, 0.6363030507778158363, 0.6661049572075827012, 0.6362102845434867269, 0.6659899521711740393, 0.63611760649784213404, 0.6658750702548303159, 0.63602501643332479147, 0.6657603111488342075, 0.6359325141434299326, 0.66564567454474141123, 0.6358400994226978818, 0.66553116013537490757, 0.6357477720667067083, 0.6654167676148192473, 0.6356555318720649406, 0.6653024966784148601, 0.63556337863640434367, 0.6651883470227523862, 0.63547131215837275624, 0.66507431834566703373, 0.63537933223762698825, 0.66496041034623295667, 0.63528743867482577836, 0.6648466227247576549, 0.63519563127162281043, 0.664732955182776403, 0.63510390983065978716, 0.66461940742304669076, 0.6350122741555595642, 0.6645059791495427004, 0.63492072405091933945, 0.66439267006744979425, 0.63482925932230389995, 0.6642794798831590322, 0.63473787977623892536, 0.6641664083042617098, 0.63464658522020434695, 0.6640534550395439185, 0.6345553754626277618, 0.66394061979898112886, 0.6344642503128779017, 0.66382790229373279527, 0.63437320958125815675, 0.6637153022361369863, 0.634282253079000152, 0.66360281933970502997, 0.6341913806182573775, 0.6634904533191161919, 0.6341005920120988713, 0.66337820389021236574, 0.6340098870745029545, 0.66326607076999279046, 0.6339192656203510176, 0.66315405367660878994, 0.6338287274654213593, 0.6630421523293585346, 0.63373827242638307473, 0.66293036644868182046, 0.633647900320789994, 0.6628186957561548766, 0.633557610967074672, 0.66270713997448519125, 0.63346740418454242603, 0.662595698827506357, 0.63337727979336542236, 0.6624843720401729436, 0.63328723761457681314, 0.6623731593385553883, 0.633197277470064918, 0.66226206044983490685, 0.63310739918256745654, 0.6621510751022984293, 0.63301760257566582493, 0.66204020302533355606, 0.6329278874737794222, 0.66192944394942353334, 0.6328382537021600198, 0.66181879760614225244, 0.63274870108688617895, 0.6617082637281492694, 0.63265922945485771213, 0.66159784204918484506, 0.6325698386337901904, 0.6614875323040650059, 0.6324805284522094949, 0.66137733422867662965, 0.63239129873944641217, 0.6612672475599725462, 0.63230214932563127316, 0.66115727203596666325, 0.63221308004168863656, 0.66104740739572911273, 0.6321240907193320132, 0.66093765337938141486, 0.63203518119105863457, 0.6608280097280916667, 0.6319463512901442626, 0.66071847618406974977, 0.63185760085063804215, 0.6606090524905625581, 0.63176892970735739247, 0.6604997383918492462, 0.6316803376958829422, 0.66039053363323649786, 0.6315918246525535041, 0.66028143796105381776, 0.6315033904144610892, 0.66017245112264883963, 0.6314150348194459617, 0.66006357286638265636, 0.63132675770609173247, 0.65995480294162517046, 0.6312385589137204932, 0.65984614109875046523, 0.63115043828238798706, 0.65973758708913219365, 0.6310623956528788197, 0.65962914066513899107, 0.6309744308667017071, 0.6595208015801299029, 0.63088654376608476214, 0.65941256958844983725, 0.63079873419397081667, 0.65930444444542503276, 0.63071100199401278295, 0.659196425907358551, 0.6306233470105690502, 0.6590885137315257836, 0.6305357690886989177, 0.65898070767616998233, 0.63044826807415806395, 0.65887300750049780727, 0.6303608438133940521, 0.6587654129646748954, 0.6302734961535418697, 0.6586579238298214479, 0.63018622494241950427, 0.6585505398580078364, 0.630099030028523553, 0.6584432608122502294, 0.63001191126102486716, 0.6583360864565062374, 0.62992486848976423123, 0.65822901655567057927, 0.6298379015652480741, 0.65812205087557076145, 0.62975101033864421506, 0.65801518918296278437, 0.62966419466177764335, 0.6579084312455268636, 0.6295774543871263286, 0.65780177683186316667, 0.62949078936781706627, 0.6576952257114875771, 0.62940419945762135344, 0.657588777654827469, 0.6293176845109512975, 0.6574824324332175053, 0.6292312443828555568, 0.65737618981889545203, 0.6291448789290153111, 0.65727004958499801187, 0.62905858800574026565, 0.65716401150555667806, 0.62897237146996468385, 0.6570580753554936038, 0.6288862291792434522, 0.6569522409106174927, 0.6288001609917481736, 0.65684650794761950335, 0.628714166766263293, 0.65674087624406917856, 0.62862824636218225056, 0.65663534557841038506, 0.62854239963950366644, 0.65652991572995728114, 0.62845662645882755285, 0.65642458647889028963, 0.62837092668135155623, 0.65631935760625210205, 0.6282853001688672283, 0.6562142288939436905, 0.6281997467837563244, 0.6561092001247203424, 0.6281142663889871314, 0.65600427108218771315, 0.62802885884811082234, 0.6558994415507978933, 0.6279435240252578395, 0.6557947113158454965, 0.62785826178513430557, 0.6556900801634637647, 0.6277730719930184598, 0.65558554788062068736, 0.6276879545147571237, 0.65548111425511514474, 0.62760290921676219216, 0.65537677907557306155, 0.6275179359660071508, 0.6552725421314435815, 0.62743303463002362007, 0.6551684032129952603, 0.62734820507689792576, 0.6550643621113122731, 0.6272634471752676949, 0.65496041861829064, 0.62717876079431847695, 0.6548565725266344703, 0.62709414580378039045, 0.6547528236298522197, 0.62700960207392479476, 0.6546491717222529692, 0.6269251294755609885, 0.65454561659894271885, 0.6268407278800329292, 0.65444215805582069823, 0.62675639715921598033, 0.65433879588957569065, 0.62667213718551368127, 0.6542355298976823814, 0.626587947831854542, 0.65413235987839771636, 0.6265038289716888608, 0.65402928563075727854, 0.6264197804789855664, 0.6539263069545716826, 0.6263358022282290833, 0.6538234236504229848, 0.62625189409441621983, 0.65372063551966110945, 0.62616805595305307974, 0.6536179423644002909, 0.6260842876801519972, 0.6535153439875155353, 0.626000589152228493, 0.6534128401926390919, 0.62591696024629825396, 0.653310430784156947, 0.62583340083987413463, 0.6532081155672053307, 0.6257499108109631813, 0.65310589434766724105, 0.62566649003806367683, 0.6530037669321689818, 0.6255831384001622085, 0.65290173312807671976, 0.6254998557767307564, 0.65279979274349305405, 0.6254166420477238026, 0.65269794558725360384, 0.6253334970935754638, 0.652596191468923611, 0.6252504207951966416, 0.65249453019879455833, 0.6251674130339721967, 0.6523929615878808024, 0.62508447369175814057, 0.65229148544791622363, 0.62500160265087885094, 0.6521901015913508914, 0.62491879979412430447, 0.65208880983134774313, 0.62483606500474733096, 0.651987609981779281, 0.62475339816646088744, 0.6518865018572242807, 0.6246707991634353514, 0.65178548527296451947, 0.6245882678802958344, 0.6516845600449815163, 0.62450580420211951365, 0.65158372598995328764, 0.6244234080144329846, 0.65148298292525111923, 0.62434107920320963133, 0.6513823306689363534, 0.62425881765486701574, 0.65128176903975718655, 0.6241766232562642864, 0.6511812978571454889, 0.6240944958946996054, 0.65108091694121363307, 0.6240124354579075934, 0.6509806261127513405, 0.62393044183405679343, 0.6508804251932225408, 0.6238485149117471525, 0.65078031400476224765, 0.62376665458000752127, 0.6506802923701734467, 0.6236848607282931712, 0.6505803601129239995, 0.62360313324648333023, 0.65048051705714356353, 0.62352147202487873454, 0.65038076302762052157, 0.6234398769541991991, 0.65028109784979893233, 0.62335834792558120486, 0.6501815213497754882, 0.62327688483057550235, 0.65008203335429649237, 0.6231954875611447335, 0.6499826336907548489, 0.62311415600966106907, 0.64988332218718706455, 0.6230328900689038636, 0.6497840986722702677, 0.6229516896320573261, 0.6496849629753192394, 0.62287055459270820786, 0.64958591492628345873, 0.62278948484484350566, 0.6494869543557441628, 0.6227084802828481822, 0.6493880810949114187, 0.62262754080150290115, 0.64928929497562121203, 0.62254666629598177913, 0.64919059583033254535, 0.62246585666185015286, 0.64909198349212455506, 0.6223851117950623622, 0.64899345779469363614, 0.6223044315919595479, 0.64889501857235058535, 0.62222381594926746577, 0.64879666566001775556, 0.62214326476409431525, 0.6486983988932262238, 0.62206277793392858314, 0.64860021810811297176, 0.62198235535663690295, 0.6485021231414180829, 0.6219019969304619277, 0.64840411383048194714, 0.62182170255402021894, 0.64830619001324248557, 0.62174147212630014937, 0.64820835152823238134, 0.62166130554665982003, 0.6481105982145763295, 0.6215812027148249926, 0.6480129299119882951, 0.6215011635308870344, 0.6479153464607687893, 0.62142118789530087985, 0.64781784770180215295, 0.6213412757088830028, 0.6477204334765538564, 0.6212614268728094058, 0.64762310362706781216, 0.6211816412886136211, 0.6475258579959636985, 0.62110191885818472627, 0.64742869642643429697, 0.6210222594837653733, 0.64733161876224284216, 0.62094266306794983055, 0.6472346248477203847, 0.62086312951368203866, 0.64713771452776316473, 0.6207836587242536787, 0.6470408876478300009, 0.6207042506033022556, 0.64694414405393968893, 0.62062490505480919243, 0.64684748359266841587, 0.6205456219830979381, 0.6467509061111471814, 0.6204664012928320883, 0.6466544114570592371, 0.62038724288901351867, 0.64655799947863753434, 0.620308146676980532, 0.6464616700246621861, 0.62022911256240601495, 0.64636542294445793863, 0.62015014045129561053, 0.64626925808789165774, 0.6200712302499859003, 0.6461731753053698264, 0.6199923818651426005, 0.64607717444783605286, 0.61991359520375876956, 0.64598125536676859285, 0.61983487017315302725, 0.64588541791417788067, 0.61975620668096778615, 0.64578966194260407576, 0.6196776046351674953, 0.6456939873051146169, 0.619599063944036895, 0.6455983938553017929, 0.61952058451617928375, 0.6455028814472803202, 0.6194421662605147957, 0.64540744993568493424, 0.6193638090862786914, 0.6453120991756679937, 0.61928551290301965803, 0.6452168290228970939, 0.6192072776205981218, 0.6451216393335526916, 0.61912910314918457135, 0.64502652996432574214, 0.6190509893992578925, 0.6449315007724153499, 0.6189729362816037141, 0.644836551615526426, 0.61889494370731276413, 0.6447416823518673601, 0.6188170115877792366, 0.64464689284014770027, 0.6187391398346991708, 0.64455218293957585, 0.61866132836006883884, 0.64445755250985676865, 0.618583577076183146, 0.64436300141118968947, 0.61850588589563404007, 0.6442685295042658451, 0.6184282547313089315, 0.6441741366502662028, 0.61835068349638912423, 0.6440798227108592151, 0.6182731721043482565, 0.64398558754819857725, 0.61819572046895075246, 0.6438914310249209984, 0.6181183285042502821, 0.64379735300414397974, 0.6180409961245882336, 0.64370335334946360707, 0.61796372324459219387, 0.643609431924952352, 0.61788650977917444, 0.64351558859515688556, 0.6178093556435304394, 0.6434218232250958987, 0.6177322607531373612, 0.64332813568025793777, 0.61765522502375259565, 0.6432345258265992471, 0.6175782483714122842, 0.6431409935305416241, 0.61750133071242985894, 0.64304753865897028287, 0.61742447196339459093, 0.6429541610792317297, 0.61734767204117014833, 0.64286086065913164797, 0.61727093086289316426, 0.6427676372669327948, 0.61719424834597181234, 0.64267449077135290354, 0.61711762440808439286, 0.6425814210415626024, 0.61704105896717792787, 0.6424884279471833387, 0.6169645519414667648, 0.6423955113582853162, 0.61688810324943118847, 0.6423026711453854378, 0.6168117128098160438, 0.6422099071794452644, 0.61673538054162936503, 0.6421172193318689786, 0.6166591063641410154, 0.64202460747450136087, 0.61658289019688133535, 0.6419320714796257745, 0.6165067319596397983, 0.6418396112199621616, 0.6164306315724636756, 0.64174722656866504646, 0.61635458895565671, 0.6416549173993215494, 0.6162786040297777975, 0.6415626835859494134, 0.61620267671563967797, 0.641470525002995036, 0.61612680693430763273, 0.6413784415253315121, 0.6160509946070981921, 0.64128643302825668704, 0.61597523965557784964, 0.6411944993874912201, 0.61589954200156178524, 0.64110264047917665266, 0.61582390156711259613, 0.6410108561798734926, 0.6157483182745390362, 0.640919146366559303, 0.6156727920463947627, 0.64082751091662680146, 0.61559732280547709054, 0.6407359497078819687, 0.6155219104748257553, 0.6406444626185421669, 0.6154465549777216841, 0.6405530495272342668, 0.61537125623768577215, 0.64046171031299278364, 0.6152960141784776703, 0.6403704448552580229, 0.61522082872409457674, 0.6402792530338742343, 0.6151456997987700386, 0.6401881347290877738, 0.61507062732697275956, 0.64009708982154527783, 0.61499561123340541595, 0.64000611819229184275, 0.61492065144300348006, 0.6399152197227692169, 0.61484574788093404986, 0.63982439429481399707, 0.6147709004725946866, 0.63973364179065583644, 0.6146961091436122601, 0.6396429620929156628, 0.6146213738198417999, 0.63955235508460390087, 0.6145466944273653553, 0.6394618206491187083, 0.6144720708924908604, 0.6393713586702442157, 0.6143975031417510077, 0.63928096903214877964, 0.61432299110190212785, 0.6391906516193832403, 0.61424853469992307713, 0.6391004063168791907, 0.61417413386301413007, 0.6390102330099472515, 0.6140997885185958811, 0.6389201315842753574, 0.61402549859430815134, 0.6388301019259270484, 0.613951264018008902, 0.63874014392133977305, 0.61387708471777315634, 0.63865025745732319733, 0.6138029606218919255, 0.638560442421057522, 0.61372889165887114295, 0.6384706987000918101, 0.6136548777574306047, 0.6383810261823423205, 0.61358091884650291616, 0.6382914247560908513, 0.61350701485523244464, 0.6382018943099830884, 0.6134331657129742796, 0.6381124347330269668, 0.6133593713492931979, 0.6380230459145910361, 0.61328563169396263646, 0.63793372774440283457, 0.6132119466769636701, 0.6378444801125472737, 0.6131383162284839962, 0.63775530290946502645, 0.61306474027891692526, 0.6376661960259509276, 0.6129912187588603779, 0.6375771593531523784, 0.61291775159911588756, 0.63748819278256776337, 0.6128443387306876094, 0.63739929620604486875, 0.61277098008478133467, 0.6373104695157793143, 0.6126976755928035119, 0.63722171260431299, 0.6126244251863602736, 0.637133025364532502, 0.61255122879725646816, 0.6370444076896676224, 0.61247808635749469847, 0.6369558594732897524, 0.612404997799274366, 0.63686738060931038813, 0.61233196305499072037, 0.63677897099197959747, 0.61225898205723391476, 0.63669063051588450095, 0.61218605473878806724, 0.63660235907594776237, 0.6121131810326303272, 0.6365141565674260881, 0.61204036087192994743, 0.63642602288590872873, 0.61196759419004736244, 0.6363379579273159937, 0.6118948809205332711, 0.6362499615878977706, 0.6118222209971277256, 0.6361620337642320509, 0.61174961435375922595, 0.63607417435322346485, 0.6116770609245438186, 0.635986383252101822, 0.61160456064378420184, 0.63589866035842065986, 0.6115321134459688361, 0.63581100557005580025, 0.61145971926577105855, 0.6357234187852039096, 0.6113873780380482047, 0.6356358999023810713, 0.6113150896978407335, 0.63554844882042136077, 0.6112428541803713584, 0.63546106543847542884, 0.61117067142104418334, 0.6353737496560090926, 0.6110985413554438447, 0.6352865013728019344, 0.61102646391933465613, 0.6351993204889459041, 0.6109544390486597609, 0.63511220690484393073, 0.61088246667954028776, 0.63502516052120854277, 0.610810546748274512, 0.63493818123906048943, 0.61073867919133702117, 0.6348512689597273749, 0.610666863945377887, 0.6347644235848422962, 0.6105951009472218399, 0.6346776450163424872, 0.61052339013386745076, 0.63459093315646797134, 0.6104517314424863153, 0.6345042879077602196, 0.6103801248104222449, 0.634417709173060815, 0.61030857017519046147, 0.63433119685551012535, 0.6102370674744767961, 0.6342447508585459797, 0.61016561664613689405, 0.63415837108590235327, 0.61009421762819542333, 0.63407205744160805884, 0.61002287035884528805, 0.6339858098299854439, 0.60995157477644684643, 0.6338996281556490951, 0.60988033081952713275, 0.6338135123235045464, 0.60980913842677908504, 0.63372746223874699744, 0.60973799753706077616, 0.6336414778068600369, 0.60966690808939464973, 0.63355555893361436946, 0.6095958700229667602, 0.6334697055250665523, 0.6095248832771260177, 0.63338391748755773757, 0.6094539477913834366, 0.6332981947277124175, 0.6093830635054113897, 0.63321253715243718137, 0.609312230359042865, 0.6331269446689194739, 0.6092414482922707279, 0.6330414171846263614, 0.6091707172452469872, 0.63295595460730330443, 0.6091000371582820653, 0.6328705568449729357, 0.6090294079718440732, 0.63278522380593384497, 0.6089588296265580884, 0.63269995539875936827, 0.60888830206320543776, 0.63261475153229638493, 0.60881782522272298477, 0.6325296121156641191, 0.60874739904620242027, 0.63244453705825294824, 0.6086770234748895572, 0.632359526269723217, 0.6086066984501836295, 0.6322745796600040562, 0.60853642391363659567, 0.63218969713929220943, 0.60846619980695244487, 0.6321048786180508635, 0.60839602607198650793, 0.632020124007008485, 0.60832590265074477264, 0.63193543321715766453, 0.60825582948538320187, 0.6318508061597539652, 0.608185806518207056, 0.63176624274631477355, 0.6081158336916702194, 0.6316817428886181636, 0.608045910948374531, 0.63159730649870175973, 0.6079760382310691172, 0.6315129334888616084, 0.60790621548264973077, 0.6314286237716510543, 0.6078364426461580907, 0.6313443772598796218, 0.60776671966478122857, 0.63126019386661190365, 0.6076970464818508368, 0.63117607350516645323, 0.60762742304084262166, 0.6310920160891146842, 0.6075578492853756583, 0.6310080215322797714, 0.6074883251592117514, 0.6309240897487355643, 0.607418850606254798, 0.63084022065280549783, 0.6073494255705501546, 0.630756414159061514, 0.60728004999628400716, 0.63067267018232298743, 0.6072107238277827452, 0.6305889886376556553, 0.6071414470095123399, 0.6305053694403705536, 0.6070722194860777236, 0.63042181250602295707, 0.60700304120222217544, 0.6303383177504113273, 0.60693391210282670814, 0.6302548850895762622, 0.6068648321329094602, 0.63017151443979945417, 0.6067958012376250895, 0.6300882057176026511, 0.6067268193622641718, 0.63000495883974662277, 0.60665788645225260156, 0.62992177372323013333, 0.60658900245315099716, 0.6298386502852889183, 0.6065201673106541083, 0.6297555884433946667, 0.60645138097059022735, 0.6296725881152540066, 0.6063826433789206038, 0.62958964921880749925, 0.6063139544817388618, 0.62950677167222863465, 0.60624531422527042134, 0.6294239553939228338, 0.60617672255587192175, 0.6293412003025264542, 0.60610817942003064986, 0.62925850631690580434, 0.60603968476436396946, 0.6291758733561561572, 0.60597123853561875556, 0.62909330133960077375, 0.60590284068067083094, 0.62901079018678992725, 0.6058344911465244052, 0.62892833981749993524, 0.60576618988031151865, 0.6288459501517321956, 0.60569793682929148773, 0.6287636211097122261, 0.6056297319408503539, 0.62868135261188870977, 0.6055615751625003354, 0.6285991445789325445, 0.605493466441879283, 0.62851699693173589855, 0.6054254057267501368, 0.62843490959141126755, 0.60535739296500038804, 0.62835288247929054113, 0.605289428104641543, 0.6282709155169240691, 0.60522151109380858876, 0.6281890086260797344, 0.60515364188075946363, 0.6281071617287420317, 0.6050858204138745301, 0.62802537474711114915, 0.6050180466416560497, 0.62794364760360205427, 0.6049503205127276607, 0.6278619802208435844, 0.60488264197583386074, 0.6277803725216775438, 0.60481501097983948967, 0.6276988244291578031, 0.6047474274737292164, 0.62761733586654940194, 0.6046798914066070287, 0.6275359067573276591, 0.60461240272769572535, 0.6274545370251772863, 0.60454496138633641154, 0.6273732265939915037, 0.6044775673319879959, 0.62729197538787116295, 0.6044102205142266916, 0.62721078333112387205, 0.60434292088274551934, 0.627129650348263128, 0.6042756683873538133, 0.6270485763640074481, 0.60420846297797672913, 0.6269675613032795113, 0.6041413046046547559, 0.62688660509120530044, 0.6040741932175432296, 0.6268057076531132491, 0.60400712876691184904, 0.6267248689145333939, 0.6039401112031441961, 0.62664408880119652914, 0.6038731404767372558, 0.6265633672390333674, 0.6038062165383009419, 0.6264827041541737023, 0.6037393393385576221, 0.6264020994729455773, 0.60367250882834164893, 0.6263215531218744574, 0.60360572495859889043, 0.62624106502768240543, 0.6035389876803862644, 0.6261606351172872614, 0.60347229694487127596, 0.62608026331780182715, 0.60340565270333155595, 0.62599994955653305497, 0.60333905490715440326, 0.6259196937609812386, 0.60327250350783632825, 0.62583949585883920895, 0.6032059984569826001, 0.625759355777991536, 0.60313953970630679535, 0.6256792734465137305, 0.6030731272076303495, 0.6255992487926714532, 0.60300676091288211034, 0.6255192817449197256, 0.6029404407740978947, 0.62543937223190214554, 0.60287416674342004646, 0.6253595201824501069, 0.6028079387730969975, 0.62527972552558202195, 0.602741756815482831, 0.62519998819050254946, 0.60267562082303684684, 0.625120308106601824, 0.6026095307483231294, 0.6250406852034546919, 0.6025434865440101175, 0.6249611194108199479, 0.6024774881628701768, 0.6248816106586395776, 0.60241153555777917435, 0.62480215887703800353, 0.60234562868171605553, 0.624722763996321334, 0.602279767487762423, 0.6246434259469766159, 0.60221395192910211815, 0.62456414465967109245, 0.6021481819590208042, 0.6244849200652514605, 0.60208245753090555225, 0.624405752094743138, 0.6020167785982444291, 0.6243266406793495297, 0.60195114511462608723, 0.6242475857504512969, 0.6018855570337393565, 0.62416858723960563353, 0.60182001430937283917, 0.6240896450785455446, 0.601754516895414506, 0.62401075919917912683, 0.60168906474585129464, 0.623931929533588853, 0.60162365781476871067, 0.62385315601403086257, 0.60155829605635043007, 0.62377443857293425193, 0.60149297942487790373, 0.62369577714290037044, 0.60142770787472996525, 0.6236171716567021204, 0.6013624813603824386, 0.6235386220472832563, 0.60129729983640775007, 0.62346012824775769466, 0.60123216325747454115, 0.62338169019140881986, 0.60116707157834728284, 0.62330330781168879757, 0.6011020247538858933, 0.6232249810422178913, 0.6010370227390453569, 0.62314670981678378067, 0.60097206548887534456, 0.6230684940693408836, 0.60090715295851983725, 0.6229903337340096825, 0.60084228510321675067, 0.6229122287450760534, 0.6007774618782975624, 0.6228341790369905981, 0.6007126832391869398, 0.62275618454436797855, 0.60064794914140237197, 0.6226782452019862578, 0.6005832595405538012, 0.62260036094478623907, 0.6005186143923432581, 0.6225225317078708129, 0.60045401365256449787, 0.62244475742650430555, 0.60038945727710263845, 0.62236703803611182894, 0.6003249452219338009, 0.62228937347227863627, 0.6002604774431247511, 0.62221176367074947956, 0.6001960538968325435, 0.62213420856742797, 0.6001316745393041671, 0.6220567080983759421, 0.6000673393268761928, 0.6219792621998128217, 0.60000304821597442274, 0.6219018708081149944, 0.5999388011631135411, 0.62182453385981517916, 0.59987459812489676744, 0.62174725129160180533, 0.59981043905801551116, 0.6216700230403183915, 0.5997463239192490277, 0.6215928490429629256, 0.59968225266546407675, 0.62151572923668725303, 0.59961822525361458276, 0.6214386635587964635, 0.5995542416407412961, 0.62136165194674828125, 0.5994903017839714562, 0.6212846943381524604, 0.59942640564051845775, 0.62120779067077018157, 0.599362553167681516, 0.62113094088251345114, 0.59929874432284533677, 0.62105414491144450475, 0.5992349790634797857, 0.6209774026957752116, 0.5991712573471395604, 0.6209007141738664852, 0.5991075791314638642, 0.62082407928422769273, 0.5990439443741760811, 0.6207474979655160694, 0.59898035303308345284, 0.62067097015653613677, 0.5989168050660767573, 0.62059449579623912174, 0.59885330043112998887, 0.6205180748237223789, 0.5987898390863000398, 0.62044170717822881594, 0.5987264209897263838, 0.62036539279914632223, 0.59866304609963076123, 0.62028913162600720005, 0.59859971437431686524, 0.6202129235984875975, 0.5985364257721700303, 0.6201367686564069443, 0.5984731802516569215, 0.62006066673972739254, 0.59840997777132522617, 0.61998461778855325753, 0.5983468182898033472, 0.6199086217431304621, 0.59828370176580009663, 0.6198326785438459848, 0.5982206281581043913, 0.6197567881312273067, 0.5981575974255849517, 0.6196809504459418685, 0.59809460952718999975, 0.61960516542879652145, 0.5980316644219469601, 0.61952943302073698763, 0.5979687620689621611, 0.6194537531628473178, 0.59790590242742053956, 0.61937812579634935666, 0.59784308545658534483, 0.6193025508626022076, 0.59778031111579784605, 0.6192270283031016999, 0.5977175793644770386, 0.61915155805947985867, 0.59765489016211935565, 0.6190761400735043806, 0.5975922434682983771, 0.61900077428707810724, 0.5975296392426645434, 0.6189254606422385044, 0.597467077444944868, 0.61885019908115714097, 0.59740455803494265307, 0.61877498954613917396, 0.5973420809725372063, 0.6186998319796228336, 0.59727964621768355873, 0.61862472632417891006, 0.5972172537304121839, 0.6185496725225102459, 0.59715490347082871914, 0.6184746705174512273, 0.59709259539911368727, 0.6183997202519672807, 0.59703032947552222036, 0.6183248216691543688, 0.5969681056603837849, 0.618249974712238493, 0.5969059239141019078, 0.6181751793245751935, 0.59684378419715390396, 0.6181004354496490559, 0.5967816864700906058, 0.61802574303107321796, 0.59671963069353609326, 0.61795110201258887926, 0.5966576168281874255, 0.61787651233806481206, 0.5965956448348143736, 0.61780197395149687693, 0.5965337146742591559, 0.61772748679700753896, 0.5964718263074361727, 0.61765305081884538637, 0.5964099796953317433, 0.6175786659613846512, 0.5963481747990038449, 0.617504332169124734, 0.5962864115795818513, 0.617430049386689728, 0.59622468999826627457, 0.6173558175588279485, 0.59616301001632850646, 0.6172816366304114614, 0.59610137159511056206, 0.61720750654643561777, 0.5960397746960248245, 0.6171334272520185861, 0.59597821928055379084, 0.61705939869240089107, 0.5959167053102498189, 0.61698542081294495106, 0.59585523274673487604, 0.6169114935591346208, 0.5957938015517002885, 0.616837616876574733, 0.59573241168690649247, 0.61676379071099064614, 0.5956710631141827857, 0.6166900150082277901, 0.5956097557954270813, 0.61661628971425121696, 0.59554848969260566207, 0.6165426147751451531, 0.5954872647677529358, 0.61646899013711255227, 0.59542608098297119255, 0.6163954157464746537, 0.59536493830043036267, 0.61632189154967053794, 0.5953038366823677758, 0.6162484174932566897, 0.5952427760910879217, 0.61617499352390655795, 0.5951817564889622111, 0.61610161958841012247, 0.59512077783842873925, 0.6160282956336734587, 0.595059840101992049, 0.6159550216067183074, 0.59499894324222289677, 0.6158817974546816457, 0.594938087221758018, 0.6158086231248152586, 0.59487727200329989456, 0.6157354985644853137, 0.5948164975496165235, 0.61566242372117193916, 0.59475576382354118627, 0.6155893985424688009, 0.59469507078797221943, 0.61551642297608268354, 0.59463441840587278676, 0.61544349696983307474, 0.59457380664027065193, 0.615370620471651747, 0.59451323545425795245, 0.61529779342958234635, 0.59445270481099097437, 0.6152250157917799793, 0.59439221467368992964, 0.6151522875065108048, 0.5943317650056387318, 0.6150796085221516258, 0.59427135577018477564, 0.6150069787871894824, 0.59421098693073871573, 0.61493439825022124987, 0.5941506584507742468, 0.6148618668599532353, 0.5940903702938278861, 0.61478938456520077865, 0.59403012242349875527, 0.61471695131488785326, 0.5939699148034483641, 0.6146445670580466706, 0.5939097473974003951, 0.61457223174381728583, 0.5938496201691404892, 0.61449994532144720295, 0.59378953308251603227, 0.6144277077402909868, 0.59372948610143594324, 0.6143555189498098728, 0.5936694791898704629, 0.61428337889957137987, 0.59360951231185094267, 0.614211287539248925, 0.5935495854314696367, 0.6141392448186214396, 0.59348969851287949217, 0.6140672506875729875, 0.5934298515202939435, 0.613995305096092386, 0.5933700444179867051, 0.61392340799427282604, 0.59331027717029156623, 0.6138515593323114971, 0.5932505497416021871, 0.61377975906050921207, 0.59319086209637189507, 0.6137080071292700333, 0.5931312141991134824, 0.6136363034891009029, 0.5930716060143990048, 0.61356464809061127144, 0.59301203750685958104, 0.61349304088451273216, 0.59295250864118519406, 0.6134214818216186536, 0.5928930193821244911, 0.61334997085284381497, 0.5928335696944845869, 0.613278507929204044, 0.59277415954313086724, 0.61320709300181585564, 0.59271478889298679266, 0.613135726021896093, 0.59265545770903370397, 0.6130644069407615699, 0.5925961659563106281, 0.6129931357098287131, 0.5925369135999140851, 0.6129219122806132101, 0.5924777006049978966, 0.6128507366047296552, 0.59241852693677299373, 0.61277960863389119804, 0.5923593925605072278, 0.6127085283199091955, 0.59230029744152518025, 0.61263749561469286206, 0.59224124154520797455, 0.6125665104702489249, 0.5921822248369930883, 0.6124955728386812787, 0.5921232472823741667, 0.6124246826721906415, 0.59206430884690083676, 0.6123538399230742152, 0.59200540949617852217, 0.61228304454372534387, 0.5919465491958682594, 0.6122122964866331763, 0.59188772791168651414, 0.61214159570438232824, 0.5918289456094049994, 0.61207094214965254903, 0.5917702022548504937, 0.61200033577521838543, 0.59171149781390466025, 0.61192977653394885033, 0.59165283225250386737, 0.6118592643788070931, 0.5915942055366390095, 0.61178879926285006853, 0.59153561763235532875, 0.6117183811392282113, 0.59147706850575223813, 0.61164800996118510773, 0.5914185581229831445, 0.6115776856820571726, 0.59136008645025527324, 0.61150740825527332464, 0.59130165345382949295, 0.6114371776343546663, 0.59124325910002014197, 0.6113669937729141627, 0.59118490335519485507, 0.6112968566246563227, 0.5911265861857743905, 0.6112267661433768816, 0.59106830755823245904, 0.61115672228296248674, 0.5910100674390955524, 0.61108672499739037957, 0.5909518657949427739, 0.61101677424072808675, 0.590893702592405669, 0.6109468699671331063, 0.59083557779816805647, 0.61087701213085259736, 0.59077749137896586127, 0.6108072006862230723, 0.590719443301586947, 0.6107374355876700893, 0.5906614335328709503, 0.6106677167897079452, 0.59060346203970911523, 0.61059804424693937293, 0.59054552878904412866, 0.61052841791405523694, 0.59048763374786995654, 0.61045883774583423176, 0.59042977688323168036, 0.6103893036971425811, 0.59037195816222533544, 0.6103198157229337403, 0.59031417755199774904, 0.61025037377824809713, 0.59025643501974637894, 0.6101809778182126755, 0.59019873053271915435, 0.61011162779804084126, 0.5901410640582143159, 0.6100423236730320082, 0.5900834355635802567, 0.6099730653985713458, 0.5900258450162153652, 0.6099038529301294869, 0.58996829238356786746, 0.609834686223262241, 0.58991077763313567057, 0.60976556523361030254, 0.58985330073246620714, 0.60969648991689896544, 0.5897958616491562802, 0.60962746022893783747, 0.58973846035085190887, 0.60955847612562055483, 0.5896810968052481741, 0.6094895375629244999, 0.58962377098008906704, 0.6094206444969105186, 0.5895664828431673353, 0.6093517968837226396, 0.5895092323623243324, 0.60928299467958779516, 0.5894520195054498665, 0.60921423784081554353, 0.5893948442404820502, 0.60914552632379779086, 0.58933770653540715103, 0.6090768600850085158, 0.5892806063582594428, 0.6090082390810034957, 0.58922354367712105773, 0.60893966326842003355, 0.5891665184601218379, 0.6088711326039766844, 0.58910953067543918944, 0.6088026470444729862, 0.58905258029129793597, 0.60873420654678918876, 0.5889956672759701732, 0.6086658110678859873, 0.5889387915977751241, 0.6085974605648042539, 0.58888195322507899497, 0.60852915499466477163, 0.58882515212629483114, 0.6084608943146679701, 0.5887683882698823752, 0.6083926784820936617, 0.5887116616243479237, 0.6083245074543007792, 0.5886549721582441862, 0.6082563811887271143, 0.5885983198401701441, 0.6081882996428890577, 0.5885417046387709102, 0.60812026277438134043, 0.58848512652273758937, 0.60805227054087677535, 0.5884285854608071395, 0.60798432290012600167, 0.5883720814217622323, 0.6079164198099572282, 0.5883156143744311163, 0.6078485612282759792, 0.5882591842876874791, 0.6077807471130648415, 0.588202791130450311, 0.6077129774223832124, 0.5881464348716837695, 0.6076452521143670491, 0.5880901154803970432, 0.60757757114722861773, 0.58803383292564421694, 0.6075099344792562443, 0.5879775871765241384, 0.6074423420688140693, 0.58792137820218028386, 0.60737479387434179775, 0.58786520597180062553, 0.60730728985435445744, 0.5878090704546174994, 0.6072398299674421517, 0.58775297161990747266, 0.6071724141722698164, 0.58769690943699121315, 0.60710504242757697845, 0.5876408838752333587, 0.6070377146921775152, 0.58758489490404238706, 0.6069704309249594124, 0.58752894249287048584, 0.6069031910848845265, 0.5874730266112134244, 0.606835995130988346, 0.58741714722861042525, 0.60676884302237975516, 0.5873613043146440361, 0.60670173471824079714, 0.58730549783894000294, 0.6066346701778264396, 0.5872497277711671433, 0.6065676493604643408, 0.58719399408103721994, 0.6065006722255546165, 0.5871382967383048158, 0.60643373873256960807, 0.58708263571276720884, 0.60636684884105365246, 0.5870270109742642474, 0.60630000251062285123, 0.58697142249267822656, 0.60623319970096484194, 0.58691587023793376487, 0.6061664403718385714, 0.5868603541799976814, 0.60609972448307406737, 0.58680487428887887353, 0.6060330519945722136, 0.5867494305346281954, 0.6059664228663045248, 0.58669402288733833674, 0.605899837058312923, 0.5866386513171437019, 0.60583329453070951416, 0.58658331579422029006, 0.6057667952436763658, 0.58652801628878557555, 0.605700339157465288, 0.58647275277109838887, 0.6056339262323976108, 0.5864175252114587979, 0.6055675564288639668, 0.5863623335802079901, 0.6055012297073240727, 0.58630717784772815515, 0.60543494602830651244, 0.5862520579844423676, 0.6053687053524085203, 0.5861969739608144704, 0.6053025076402957653, 0.5861419257473489591, 0.60523635285270213825, 0.5860869133145908664, 0.6051702409504295373, 0.58603193663312564695, 0.6051041718943476555, 0.5859769956735790633, 0.60503814564539376927, 0.5859220904066170711, 0.60497216216457252796, 0.58586722080294570663, 0.6049062214129557442, 0.58581238683331097306, 0.6048403233516821837, 0.5857575884684987282, 0.6047744679419573598, 0.5857028256793345727, 0.6047086551450533237, 0.5856480984366837378, 0.60464288492230845945, 0.5855934067114509752, 0.604577157235127279, 0.58553875047458044584, 0.60451147204498021775, 0.5854841296970556102, 0.6044458293134034304, 0.585429544349899118, 0.60438022900199858786, 0.58537499440417269987, 0.60431467107243267654, 0.5853204798309770583, 0.60424915548643779746, 0.5852660006014517594, 0.60418368220581096533, 0.58521155668677512514, 0.60411825119241390923, 0.58515714805816412586, 0.60405286240817287493, 0.5851027746868742741, 0.6039875158150784273, 0.58504843654419951736, 0.6039222113751852528, 0.5849941336014721331, 0.6038569490506119646, 0.5849398658300626221, 0.6037917288035409069, 0.5848856332013796046, 0.60372655059621796117, 0.5848314356868697152, 0.60366141439095235215, 0.5847772732580174987, 0.6035963201501164562, 0.58472314588634530664, 0.603531267836145609, 0.58466905354341319355, 0.6034662574115379141, 0.58461499620081881497, 0.60340128883885405366, 0.58456097383019732435, 0.6033363620807170993, 0.5845069864032212715, 0.6032714770998123229, 0.584453033891600501, 0.60320663385888700854, 0.5843991162670820512, 0.60314183232075026714, 0.58434523350145005355, 0.6030770724482728487, 0.58429138556652563205, 0.6030123542043869575, 0.584237572434166804, 0.60294767755208606764, 0.58418379407626838044, 0.60288304245442474015, 0.58413005046476186734, 0.602818448874518438, 0.58407634157161536655, 0.6027538967755433457, 0.5840226673688334782, 0.60268938612073618576, 0.58396902782845720254, 0.60262491687339404036, 0.58391542292256384355, 0.6025604889968741699, 0.5838618526232669108, 0.602496102454593834, 0.58380831690271602416, 0.6024317572100301138, 0.583754815733096817, 0.6023674532267197325, 0.58370134908663084044, 0.60230319046825888014, 0.5836479169355754689, 0.60223896889830303694, 0.5835945192522238043, 0.6021747884805667971, 0.5835411560089045821, 0.6021106491788236946, 0.58348782717798207707, 0.6020465509569060291, 0.5834345327318560097, 0.60198249377870469276, 0.583381272642961453, 0.6019184776081689973, 0.5833280468837687393, 0.60185450240930650256, 0.5832748554267833679, 0.6017905681461828447, 0.583221698244545913, 0.6017266747829215665, 0.58316857530963193187, 0.60166282228370394675, 0.5831154865946518734, 0.60159901061276883206, 0.5830624320722509874, 0.60153523973441246824, 0.58300941171510923356, 0.6014715096129883326, 0.58295642549594119195, 0.6014078202129069664, 0.582903473387495972, 0.60134417149863580966, 0.5828505553625571242, 0.6012805634346990349, 0.58279767139394255017, 0.60121699598567738274, 0.58274482145450441444, 0.60115346911620799687, 0.5826920055171290557, 0.60108998279098426105, 0.5826392235547368991, 0.60102653697475563705, 0.5825864755402823681, 0.6009631316323275004, 0.5825337614467537976, 0.60089976672856098053, 0.5824810812471733469, 0.6008364422283727995, 0.5824284349145969131, 0.6007731580967351114, 0.58237582242211404496, 0.60070991429867534426, 0.58232324374284785706, 0.60064671079927603876, 0.5822706988499549437, 0.60058354756367469127, 0.58221818771662529456, 0.60052042455706359695, 0.58216571031608220965, 0.6004573417446896916, 0.58211326662158221427, 0.60039429909185439534, 0.58206085660641497553, 0.60033129656391345845, 0.5820084802439032186, 0.6002683341262768042, 0.5819561375074026426, 0.60020541174440837596, 0.58190382837030183805, 0.6001425293838259827, 0.58185155280602220406, 0.60007968701010114656, 0.5817993107880178658, 0.60001688458885894936, 0.5817471022897755925, 0.59995412208577788096, 0.5816949272848147153, 0.59989139946658968835, 0.58164278574668704623, 0.5998287166970792246, 0.581590677648976797, 0.59976607374308429905, 0.58153860296530049856, 0.59970347057049552743, 0.58148656166930691967, 0.5996409071452561834, 0.581434553734676988, 0.5995783834333620495, 0.5813825791351237095, 0.5995158994008612713, 0.5813306378443920893, 0.59945345501385420773, 0.5812787298362590522, 0.5993910502384932865, 0.5812268550845333644, 0.5993286850409828576, 0.5811750135630555545, 0.59926635938757904837, 0.5811232052456978355, 0.59920407324458961737, 0.5810714301063640266, 0.5991418265783738125, 0.5810196881189894766, 0.5990796193553422256, 0.580967979257540985, 0.5990174515419566498, 0.5809163034960167263, 0.5989553231047299371, 0.58086466080844617333, 0.5988932340102258579, 0.5808130511688900202, 0.5988311842250589576, 0.58076147455144010704, 0.598769173715894417, 0.5807099309302193435, 0.59870720244944791234, 0.58065842027938163386, 0.5986452703924854747, 0.58060694257311180167, 0.5985833775118233517, 0.580555497785625515, 0.59852152377432786887, 0.5805040858911692118, 0.598459709146915292, 0.58045270686402002596, 0.59839793359655168927, 0.58040136067848571304, 0.5983361970902527942, 0.58035004730890457663, 0.5982744995950838701, 0.5802987667296453952, 0.59821284107815957386, 0.5802475189151073487, 0.59815122150664381986, 0.58019630383971994633, 0.59808964084774964747, 0.5801451214779429534, 0.59802809906873908446, 0.5800939718042663195, 0.597966596136923015, 0.5800428547932101067, 0.5979051320196610458, 0.57999177041932441733, 0.5978437066843613737, 0.5799407186571893234, 0.5977823200984806533, 0.57988969948141479485, 0.5977209722295238666, 0.57983871286664062915, 0.59765966304504418926, 0.57978775878753638064, 0.5975983925126428641, 0.57973683721880129037, 0.59753716059996906746, 0.57968594813516421575, 0.5974759672747197813, 0.5796350915113835615, 0.5974148125046396643, 0.5795842673222472097, 0.59735369625752092265, 0.579533475542572451, 0.5972926185012031834, 0.5794827161472059155, 0.59723157920357336536, 0.57943198911102350397, 0.59717057833256555216, 0.57938129440893032, 0.59710961585616086804, 0.57933063201586060156, 0.5970486917423873488, 0.5792800019067776531, 0.5969878059593198186, 0.57922940405667377875, 0.5969269584750797638, 0.5791788384405702137, 0.5968661492578352084, 0.57912830503351705845, 0.59680537827580059057, 0.5790778038105932112, 0.59674464549723663857, 0.5790273347469063018, 0.5966839508904502479, 0.5789768978175926253, 0.59662329442379435825, 0.5789264929978170759, 0.59656267606566783235, 0.5788761202627730814, 0.5965020957845153331, 0.5788257795876825376, 0.59644155354882720364, 0.5787754709477957431, 0.5963810493271393449, 0.57872519431839133404, 0.59632058308803309707, 0.57867494967477621986, 0.59626015480013511865, 0.57862473699228551843, 0.59619976443211726764, 0.57857455624628249195, 0.59613941195269648244, 0.57852440741215848275, 0.59607909733063466323, 0.5784742904653328498, 0.5960188205347385544, 0.5784242053812529053, 0.5959585815338596269, 0.5783741521353938509, 0.5958983802968939611, 0.57832413070325871537, 0.5958382167927821299, 0.57827414106037829095, 0.5957780909905090829, 0.5782241831823110717, 0.59571800285910403043, 0.57817425704464319065, 0.59565795236764032837, 0.5781243626229883578, 0.5955979394852353634, 0.5780744998929877984, 0.59553796418105043795, 0.57802466883031019136, 0.59547802642429065764, 0.57797486941065160786, 0.5954181261842048159, 0.5779251016097354499, 0.59535826343008528116, 0.5778753654033123897, 0.59529843813126788606, 0.57782566076716030946, 0.595238650257131813, 0.57777598767708423995, 0.59517889977709948325, 0.57772634610891630103, 0.59511918666063644485, 0.57767673603851564133, 0.59505951087725126295, 0.5776271574417683786, 0.59499987239649540765, 0.57757761029458754026, 0.5949402711879631449, 0.57752809457291300396, 0.5948807072212914261, 0.57747861025271143826, 0.59482118046615977937, 0.5774291573099762443, 0.5947616908922902, 0.57737973572072749656, 0.59470223846944704184, 0.577330345461011885, 0.59464282316743691064, 0.5772809865069026564, 0.59458344495610855384, 0.57723165883449955655, 0.594524103805352756, 0.57718236241992877254, 0.5944647996851022295, 0.5771330972393428749, 0.5944055325653315096, 0.57708386326892076063, 0.59434630241605684723, 0.5770346604848675957, 0.59428710920733610447, 0.57698548886341475867, 0.59422795290926864873, 0.5769363483808197833, 0.59416883349199524686, 0.57688723901336630244, 0.5941097509256979619, 0.5768381607373639915, 0.5940507051806000484, 0.5767891135291485127, 0.59399169622696584836, 0.57674009736508145905, 0.5939327240351006892, 0.57669111222155029875, 0.59387378857535077863, 0.5766421580749683191, 0.59381488981810310365, 0.5765932349017745723, 0.5937560277337853278, 0.5765443426784338195, 0.5936972022928656892, 0.5764954813814364764, 0.59363841346585289916, 0.57644665098729855844, 0.5935796612232960409, 0.57639785147256162657, 0.59352094553578447, 0.5763490828137927326, 0.5934622663739477116, 0.5763003449875843652, 0.59340362370845536243, 0.57625163797055439655, 0.5933450175100169909, 0.5762029617393460281, 0.5932864477493820371, 0.5761543162706277373, 0.5932279143973397143, 0.5761057015410932244, 0.5931694174247189105, 0.5760571175274613595, 0.59311095680238809025, 0.57600856420647612925, 0.5930525325012551967, 0.5759600415549065843, 0.59299414449226755375, 0.5759115495495467871, 0.5929357927464117697, 0.5758630881672157593, 0.5928774772347136405, 0.57581465738475743, 0.5928191979282380524, 0.575766257179040583, 0.59276095479808888655, 0.57571788752695880585, 0.59270274781540892344, 0.5756695484054304381, 0.5926445769513797471, 0.5756212397913985196, 0.5925864421772216507, 0.57557296166183074003, 0.5925283434641935414, 0.57552471399371938704, 0.59247028078359284574, 0.57547649676408129586, 0.5924122541067554163, 0.57542830994995779886, 0.5923542634050554381, 0.5753801535284146745, 0.5922963086499053342, 0.5753320274765420979, 0.5922383898127556744, 0.5752839317714545898, 0.59218050686509508086, 0.5752358663902909673, 0.5921226597784501376, 0.57518783131021429403, 0.59206484852438529665, 0.5751398265084118306, 0.5920070730745027881, 0.5750918519620949847, 0.5919493334004425275, 0.5750439076484992627, 0.5918916294738820254, 0.57499599354488422005, 0.5918339612665362973, 0.5749481096285334128, 0.59177632875015777217, 0.5749002558767543485, 0.5917187318965362031, 0.5748524322668784381, 0.59166117067749857713, 0.57480463877626094724, 0.5916036450649090268, 0.5747568753822809483, 0.5915461550306687404, 0.57470914206234127264, 0.59148870054671587277, 0.5746614387938684622, 0.5914312815850254583, 0.57461376555431272227, 0.5913738981176093213, 0.57456612232114787395, 0.5913165501165159895, 0.57451850907187130673, 0.59125923755383060556, 0.57447092578400393116, 0.59120196040167484003, 0.57442337243509013233, 0.59114471863220680514, 0.5743758490026977227, 0.5910875122176209677, 0.5743283554644178954, 0.59103034113014806336, 0.57428089179786517794, 0.5909732053420550095, 0.57423345798067738596, 0.5909161048256448213, 0.57418605399051557626, 0.590859039553256524, 0.57413867980506400225, 0.59080200949726507107, 0.57409133540203006655, 0.59074501463008125663, 0.57404402075914427646, 0.5906880549241516323, 0.5739967358541601977, 0.590631130351958423, 0.57394948066485440937, 0.59057424088601944237, 0.5739022551690264587, 0.59051738649888801025, 0.5738550593444988158, 0.59046056716315286885, 0.57380789316911682933, 0.59040378285143809984, 0.5737607566207486809, 0.59034703353640304106, 0.57371364967728534083, 0.59029031919074220525, 0.5736665723166405241, 0.59023363978718519727, 0.57361952451675064557, 0.590176995298496632, 0.57357250625557477627, 0.59012038569747605376, 0.57352551751109459875, 0.59006381095695785395, 0.57347855826131436405, 0.59000727104981119046, 0.5734316284842608474, 0.5899507659489399075, 0.57338472815798330497, 0.58989429562728245437, 0.57333785726055343034, 0.58983786005781180554, 0.5732910157700653119, 0.58978145921353538163, 0.57324420366463538836, 0.58972509306749496785, 0.5731974209224024073, 0.58966876159276663676, 0.5731506675215273815, 0.5896124647624606683, 0.5731039434401935467, 0.5895562025497214709, 0.573057248656606319, 0.58949997492772750337, 0.5730105831489932522, 0.58944378186969119626, 0.572963946895603996, 0.5893876233488588738, 0.57291733987471025373, 0.58933149933851067693, 0.5728707620646057404, 0.58927540981196048514, 0.5728242134436061411, 0.58921935474255583974, 0.57277769399004906917, 0.5891633341036778665, 0.57273120368229402474, 0.58910734786874119963, 0.57268474249872235345, 0.589051396011193905, 0.572638310417737205, 0.58899547850451740364, 0.5725919074177634925, 0.5889395953222263965, 0.5725455334772478509, 0.588883746437868789, 0.5724991885746585967, 0.58882793182502561467, 0.5724528726884856871, 0.58877215145731096084, 0.5724065857972406795, 0.58871640530837189423, 0.572360327879456691, 0.58866069335188838524, 0.5723140989136883582, 0.58860501556157323406, 0.572267898878511797, 0.5885493719111719968, 0.5722217277525245628, 0.58849376237446291164, 0.5721755855143456103, 0.5884381869252568251, 0.5721294721426152543, 0.58838264553739711864, 0.5720833876159951294, 0.5883271381847596357, 0.5720373319131681511, 0.58827166484125260854, 0.5719913050128384763, 0.5882162254808165865, 0.57194530689373146383, 0.5881608200774243621, 0.5718993375345936357, 0.5881054486050809007, 0.57185339691419263835, 0.5880501110378232679, 0.57180748501131720303, 0.58799480734972055663, 0.57176160180477710786, 0.58793953751487381787, 0.5717157472734031389, 0.58788430150741598804, 0.57166992139604705196, 0.5878290993015118189, 0.57162412415158153417, 0.5877739308713578067, 0.5715783555189001658, 0.5877187961911821212, 0.57153261547691738164, 0.58766369523524453525, 0.57148690400456843427, 0.5876086279778363559, 0.57144122108080935515, 0.5875535943932803546, 0.57139556668461691783, 0.58749859445593069637, 0.57134994079498859924, 0.58744362814017287176, 0.57130434339094254365, 0.5873886954204236272, 0.57125877445151752404, 0.58733379627113089616, 0.5712132339557729055, 0.5872789306667737306, 0.5711677218827886086, 0.587224098581862233, 0.57112223821166507154, 0.587169299990937488, 0.5710767829215232139, 0.58711453486857149373, 0.57103135599150439954, 0.58705980318936709487, 0.5709859574007704006, 0.5870051049279579156, 0.5709405871285033603, 0.58695044005900829086, 0.5708952451539057569, 0.5868958085572132006, 0.57084993145620036784, 0.5868412103972982028, 0.5708046460146302326, 0.5867866455540193659, 0.57075938880845861775, 0.58673211400216320334, 0.57071415981696898054, 0.5866776157165466075, 0.5706689590194649332, 0.5866231506720167831, 0.570623786395270207, 0.5865687188434511814, 0.5705786419237286173, 0.58651432020575743553, 0.5705335255842040276, 0.58645995473387329443, 0.5704884373560803144, 0.5864056224027665576, 0.5704433772187613318, 0.5863513231874350109, 0.5703983451516708767, 0.5862970570629063616, 0.5703533411342526536, 0.58624282400423817323, 0.57030836514597023986, 0.5861886239865178026, 0.57026341716630705065, 0.58613445698486233446, 0.5702184971747663048, 0.5860803229744185187, 0.5701736051508709895, 0.5860262219303627057, 0.57012874107416382675, 0.5859721538279007836, 0.5700839049242072383, 0.5859181186422681152, 0.5700390966805833116, 0.58586411634872947394, 0.56999431632289376603, 0.58581014692257898165, 0.5699495638307599184, 0.58575621033914004675, 0.5699048391838226497, 0.58570230657376530064, 0.56986014236174237034, 0.5856484356018365354, 0.56981547334419898736, 0.58559459739876464285, 0.5697708321108918705, 0.5855407919399895515, 0.56972621864153981847, 0.58548701920098016554, 0.56968163291588102604, 0.5854332791572343039, 0.56963707491367305045, 0.58537957178427863793, 0.5695925446146927784, 0.5853258970576686307, 0.56954804199873639313, 0.58527225495298847694, 0.56950356704561934086, 0.58521864544585104075, 0.56945911973517629836, 0.58516506851189779674, 0.56941470004726114077, 0.58511152412679876905, 0.5693703079617469075, 0.58505801226625247114, 0.56932594345852577123, 0.585004532905985846, 0.56928160651750900426, 0.5849510860217542067, 0.569237297118626947, 0.58489767158934117614, 0.569193015241828975, 0.5848442895845586286, 0.56914876086708346745, 0.58479093998324663023, 0.5691045339743777748, 0.58473762276127337986, 0.56906033454371818665, 0.5846843378945351503, 0.56901616255512990015, 0.58463108535895623, 0.56897201798865698857, 0.584577865130488865, 0.5689279008243623685, 0.58452467718511319885, 0.56888381104232776963, 0.58447152149883721707, 0.5688397486226537023, 0.58441839804769668727, 0.5687957135454594268, 0.5843653068077551033, 0.5687517057908829216, 0.5843122477551036255, 0.56870772533908085225, 0.5842592208658610256, 0.56866377217022854066, 0.58420622611617362797, 0.56861984626451993376, 0.5841532634822152536, 0.56857594760216757264, 0.58410033294018716264, 0.5685320761634025617, 0.584047434466317998, 0.5684882319284745381, 0.58399456803686372955, 0.56844441487765164116, 0.5839417336281075966, 0.5684006249912204817, 0.5838889312163600532, 0.5683568622494861115, 0.5838361607779587109, 0.5683131266327719934, 0.58378342228926828394, 0.56826941812141997033, 0.58373071572668053296, 0.5682257366957902358, 0.5836780410666142105, 0.56818208233626130377, 0.5836253982855150049, 0.5681384550232299781, 0.5835727873598554856, 0.56809485473711132333, 0.583520208266135048, 0.5680512814583386345, 0.5834676609808798595, 0.56800773516736340757, 0.5834151454806428037, 0.56796421584465530975, 0.58336266174200342735, 0.5679207234707021499, 0.58331020974156788455, 0.56787725802600984927, 0.5832577894559688844, 0.567833819491102412, 0.5832054008618656361, 0.56779040784652189583, 0.5831530439359437952, 0.56774702307282838305, 0.58310071865491541006, 0.5677036651505999509, 0.5830484249955188687, 0.5676603340604326437, 0.5829961629345188459, 0.56761702978294044256, 0.5829439324487062494, 0.56757375229875523746, 0.58289173351489816696, 0.5675305015885267979, 0.5828395661099378141, 0.5674872776329227449, 0.58278743021069448146, 0.5674440804126285218, 0.58273532579406348166, 0.567400909908347366, 0.58268325283696609837, 0.56735776610080028046, 0.5826312113163495323, 0.5673146489707260057, 0.582579201209186851, 0.56727155849888099084, 0.58252722249247693614, 0.5672284946660393662, 0.58247527514324443223, 0.5671854574529929147, 0.58242335913853969417, 0.5671424468405510439, 0.58237147445543873715, 0.5670994628095407584, 0.58231962107104318484, 0.56705650534080663175, 0.582267798962480219, 0.56701357441521077864, 0.58221600810690252734, 0.5669706700136328273, 0.58216424848148825346, 0.5669277921169698918, 0.5821125200634409466, 0.56688494070613654484, 0.5820608228299895111, 0.56684211576206478976, 0.5820091567583881553, 0.5667993172657040337, 0.5819575218259163424, 0.5667565451980210598, 0.58190591800987873973, 0.5667137995400000009, 0.5818543452876051699, 0.5666710802726423112, 0.58180280363645055995, 0.56662838737696673996, 0.58175129303379489255, 0.56658572083400930464, 0.5816998134570431564, 0.56654308062482326334, 0.58164836488362529687, 0.56650046673047908856, 0.58159694729099616685, 0.5664578791320644401, 0.5815455606566354783, 0.5664153178106841389, 0.5814942049580477527, 0.5663727827474601398, 0.5814428801727622731, 0.5663302739235315055, 0.5813915862783330351, 0.5662877913200543797, 0.5813403232523386984, 0.56624533491820196136, 0.5812890910723825394, 0.56620290469916447775, 0.58123788971609240177, 0.5661605006441491586, 0.5811867191611206495, 0.56611812273438020984, 0.581135579385144119, 0.56607577095109878754, 0.58108447036586407045, 0.566033445275562972, 0.5810333920810061415, 0.5659911456890477418, 0.58098234450832030013, 0.5659488721728449477, 0.5809313276255807952, 0.5659066247082632874, 0.58088034141058611173, 0.56586440327662827944, 0.5808293858411589227, 0.56582220785928223747, 0.58077846089514604237, 0.56578003843758424487, 0.58072756655041837955, 0.56573789499291012955, 0.58067670278487089146, 0.5656957775066524379, 0.5806258695764225366, 0.56565368596022041, 0.58057506690301622883, 0.56561162033503995406, 0.5805242947426187915, 0.56556958061255362125, 0.5804735530732209107, 0.56552756677422058075, 0.58042284187283708967, 0.5654855788015165944, 0.58037216111950560324, 0.56544361667593399186, 0.58032151079128845223, 0.5654016803789816457, 0.5802708908662713173, 0.5653597698921849466, 0.58022030132256351414, 0.5653178851970857782, 0.58016974213829794823, 0.5652760262752424928, 0.5801192132916310691, 0.5652341931082298868, 0.58006871476074282673, 0.56519238567763917556, 0.5800182465238366248, 0.56515060396507796933, 0.5799678085591392774, 0.56510884795217024854, 0.57991740084490096374, 0.56506711762055633977, 0.5798670233593951841, 0.56502541295189289094, 0.57981667608091871506, 0.5649837339278528476, 0.5797663589877915656, 0.5649420805301254284, 0.57971607205835693285, 0.56490045274041610096, 0.57966581527098115826, 0.564858850540446558, 0.5796155886040536832, 0.56481727391195469336, 0.5795653920359870063, 0.5647757228366945779, 0.5795152255452166391, 0.5647341972964364358, 0.57946508911020106223, 0.56469269727296662024, 0.57941498270942168287, 0.56465122274808759106, 0.5793649063213827913, 0.5646097737036178893, 0.5793148599246115172, 0.56456835012139211505, 0.5792648434976577881, 0.5645269519832609032, 0.5792148570190942845, 0.5644855792710909, 0.57916490046751639893, 0.5644442319667647399, 0.57911497382154219235, 0.56440291005218102215, 0.579065077059812352, 0.5643616135092542872, 0.5790152101609901485, 0.56432034231991499393, 0.57896537310376139444, 0.5642790964661094962, 0.5789155658668344016, 0.56423787592980001996, 0.5788657884289399394, 0.5641966806929646401, 0.5788160407688311925, 0.5641555107375972573, 0.5787663228652837189, 0.5641143660457075755, 0.57871663469709540897, 0.56407324659932107875, 0.57866697624308644377, 0.56403215238047900865, 0.5786173474820992531, 0.56399108337123834154, 0.5785677483929984742, 0.56395003955367176563, 0.5785181789546709109, 0.56390902090986765857, 0.57846863914602549255, 0.5638680274219300649, 0.57841912894599323275, 0.5638270590719786736, 0.5783696483335271885, 0.5637861158421487958, 0.5783201972876024199, 0.56374519771459134185, 0.57827077578721594895, 0.5637043046714727998, 0.5782213838113867194, 0.5636634366949752123, 0.5781720213391555559, 0.5636225937672961551, 0.57812268834958512384, 0.5635817758706487147, 0.5780733848217598902, 0.5635409829872614662, 0.5780241107347860816, 0.56350021509937845115, 0.5779748660677916456, 0.5634594721892591559, 0.57792565079992621056, 0.5634187542391784896, 0.57787646491036104594, 0.56337806123142676216, 0.57782730837828902255, 0.56333739314830966254, 0.5777781811829245726, 0.56329674997214823726, 0.57772908330350365144, 0.56325613168527886843, 0.57768001471928369676, 0.56321553827005325243, 0.5776309754095435911, 0.5631749697088383779, 0.57758196535358362084, 0.5631344259840165046, 0.5775329845307254386, 0.56309390707798514175, 0.5774840329203120237, 0.5630534129731570269, 0.5774351105017076439, 0.56301294365196010436, 0.57738621725429781646, 0.5629724990968375038, 0.5773373531574892688, 0.56293207929024751947, 0.5772885181907099016, 0.5628916842146635884, 0.57723971233340874893, 0.56285131385257426977, 0.5771909355650559412, 0.56281096818648322384, 0.577142187865142666, 0.56277064719890919055, 0.5770934692131811307, 0.56273035087238596885, 0.57704477958870452467, 0.5626900791894623958, 0.5769961189712669805, 0.5626498321327023256, 0.57694748734044353704, 0.5626096096846846086, 0.5768988846758301016, 0.56256941182800307123, 0.5768503109570434124, 0.56252923854526649434, 0.57680176616372100105, 0.5624890898190985936, 0.57675325027552115545, 0.5624489656321379979, 0.5767047632721228818, 0.56240886596703822955, 0.5766563051332258687, 0.56236879080646768356, 0.5766078758385504494, 0.56232874013310960704, 0.5765594753678375643, 0.5622887139296620789, 0.5765111037008487256, 0.56224871217883798965, 0.57646276081736597856, 0.5622087348633650211, 0.57641444669719186714, 0.5621687819659856262, 0.576366161320149396, 0.56212885346945700826, 0.57631790466608199386, 0.5620889493565511017, 0.5762696767148534781, 0.5620490696100545513, 0.57622147744634801794, 0.56200921421276869275, 0.5761733068404700987, 0.56196938314750953205, 0.57612516487714448497, 0.561929576397107726, 0.5760770515363161856, 0.56188979394440856206, 0.57602896679795041697, 0.5618500357722719389, 0.5759809106420325676, 0.5618103018635723461, 0.5759328830485681633, 0.5617705922011988451, 0.57588488399758282985, 0.56173090676805504875, 0.57583691346912225877, 0.5616912455470591021, 0.575788971443252172, 0.5616516085211436632, 0.5757410579000582856, 0.56161199567325588216, 0.5756931728196462754, 0.5615724069863573835, 0.5756453161821417416, 0.5615328424434242456, 0.5755974879676901741, 0.5614933020274469815, 0.57554968815645691723, 0.56145378572143051963, 0.5755019167286271345, 0.56141429350839418434, 0.57545417366440577457, 0.56137482537137167683, 0.5754064589440175363, 0.56133538129341105594, 0.5753587725477068346, 0.56129596125757471914, 0.57531111445573776493, 0.56125656524693938297, 0.57526348464839407, 0.5612171932445960643, 0.575215883105979105, 0.56117784523365006133, 0.57516830980881580333, 0.56113852119722093477, 0.575120764737246643, 0.56109922111844248813, 0.57507324787163361155, 0.56105994498046274984, 0.5750257591923581732, 0.561020692766443954, 0.5749782986798212349, 0.5609814644595625214, 0.5749308663144431115, 0.5609422600430090407, 0.57488346207666349324, 0.5609030794999882508, 0.5748360859469414122, 0.5608639228137190207, 0.5747887379057552077, 0.56082478996743433185, 0.57474141793360249397, 0.56078568094438125915, 0.5746941260110001261, 0.56074659572782095267, 0.57464686211848416754, 0.5607075343010286193, 0.5745996262366098567, 0.56066849664729350377, 0.5745524183459515727, 0.5606294827499188709, 0.57450523842710280435, 0.5605904925922219868, 0.5744580864606761163, 0.5605515261575341011, 0.5744109624273031161, 0.56051258342920042796, 0.5743638663076344216, 0.56047366439058012835, 0.5743167980823396281, 0.5604347690250462922, 0.574269757732107277, 0.56039589731598591943, 0.5742227452376448216, 0.5603570492467999023, 0.5741757605796785955, 0.56031822480090300763, 0.57412880373895378033, 0.5602794239617238584, 0.57408187469623437373, 0.56024064671270491604, 0.5740349734323031572, 0.5602018930373024625, 0.5739880999279616632, 0.5601631629189865823, 0.5739412541640301442, 0.5601244563412411448, 0.57389443612134754033, 0.5600857732875637861, 0.5738476457807714477, 0.56004711374146589183, 0.5738008831231780865, 0.56000847768647257904, 0.5737541481294622693, 0.55996986510612267875, 0.57370744078053737, 0.5599312759839687182, 0.57366076105733529146, 0.5598927103035769033, 0.57361410894080643524, 0.5598541680485271014, 0.57356748441191966916, 0.55981564920241282335, 0.57352088745166229707, 0.55977715374884120625, 0.5734743180410400265, 0.5597386816714329959, 0.5734277761610769386, 0.5597002329538225298, 0.57338126179281545644, 0.5596618075796577192, 0.57333477491731631463, 0.55962340553260003274, 0.5732883155156585285, 0.55958502679632447823, 0.5732418835689393624, 0.559546671354519586, 0.57319547905827430035, 0.55950833919088739145, 0.57314910196479701405, 0.5594700302891434183, 0.5731027522696593333, 0.559431744633016661, 0.5730564299540312153, 0.55939348220624956804, 0.57301013499910071413, 0.55935524299259802483, 0.5729638673860739504, 0.55931702697583133687, 0.57291762709617508146, 0.5592788341397322126, 0.5728714141106462706, 0.5592406644680967463, 0.5728252284107476571, 0.5592025179447344019, 0.57277906997775732687, 0.5591643945534679955, 0.57273293879297128154, 0.55912629427813367904, 0.5726868348377034092, 0.5590882171025809234, 0.572640758093285455, 0.5590501630106725014, 0.57259470854106699013, 0.55901213198628447196, 0.57254868616241538353, 0.55897412401330616227, 0.5725026909387157714, 0.55893613907564015225, 0.57245672285137102857, 0.55889817715720225774, 0.5724107818818017384, 0.5588602382419215136, 0.5723648680114461636, 0.5588223223137401573, 0.5723189812217602169, 0.55878442935661361306, 0.572273121494217432, 0.55874655935451047486, 0.5722272888103089345, 0.55870871229141249047, 0.57218148315154341297, 0.55867088815131454464, 0.57213570449944708935, 0.55863308691822464324, 0.5720899528355636908, 0.5585953085761638969, 0.5720442281414544201, 0.5585575531091665045, 0.57199853039869792747, 0.55851982050127973745, 0.57195285958889028206, 0.5584821107365639234, 0.5719072156936449423, 0.5584444237990924299, 0.57186159869459272855, 0.5584067596729516485, 0.57181600857338179416, 0.5583691183422409786, 0.57177044531167759626, 0.55833149979107281173, 0.5717249088911628687, 0.5582939040035725153, 0.57167939929353759314, 0.55825633096387841675, 0.5716339165005189708, 0.5582187806561417877, 0.5715884604938413945, 0.5581812530645268279, 0.5715430312552564203, 0.5581437481732106499, 0.57149762876653274074, 0.55810626596638326257, 0.57145225300945615447, 0.55806880642824755595, 0.5714069039658295409, 0.55803136954301928505, 0.57136158161747283067, 0.55799395529492705454, 0.571316285946222979, 0.5579565636682123028, 0.5712710169339339377, 0.55791919464712928653, 0.5712257745624766271, 0.5578818482159450651, 0.5711805588137389092, 0.55784452435893948493, 0.57113536966962555986, 0.557807223060405164, 0.5710902071120582419, 0.5577699443046474766, 0.5710450711229754769, 0.5577326880759845374, 0.5709999616843326186, 0.55769545435874718633, 0.57095487877810182574, 0.5576582431372789735, 0.5709098223862720346, 0.55762105439593614325, 0.57086479249084893194, 0.5575838881190876191, 0.5708197890738549284, 0.55754674429111498866, 0.57077481211732913126, 0.55750962289641248806, 0.57072986160332731724, 0.5574725239193869869, 0.5706849375139219072, 0.55743544734445797336, 0.5706400398312019373, 0.55739839315605753845, 0.5705951685372730336, 0.5573613613386303611, 0.5705503236142573855, 0.55732435187663369354, 0.57050550504429371903, 0.55728736475453734565, 0.5704607128095372698, 0.55725039995682367026, 0.57041594689215975724, 0.55721345746798754797, 0.5703712072743493583, 0.5571765372725363725, 0.5703264939383106808, 0.55713963935499003535, 0.5702818068662647373, 0.5571027636998809113, 0.57023714604044891904, 0.5570659102917538434, 0.5701925114431169696, 0.55702907911516612793, 0.5701479030565389596, 0.5569922701546875002, 0.5701033208630012599, 0.556955483394900119, 0.57005876484480651596, 0.55691871882039855235, 0.5700142349842736219, 0.5568819764157897631, 0.56996973126373769544, 0.5568452561656930935, 0.5699252536655500511, 0.55680855805474025095, 0.56988080217207817484, 0.55677188206757529347, 0.5698363767657056991, 0.55673522818885461507, 0.5697919774288323769, 0.5566985964032469312, 0.5697476041438740556, 0.5566619866954332641, 0.56970325689326265303, 0.5566253990501069284, 0.5696589356594461301, 0.556588833451973517, 0.56961464042488846825, 0.55655228988575088605, 0.5695703711720696415, 0.556515768336169141, 0.5695261278834855928, 0.55647926878797062213, 0.5694819105416482084, 0.5564427912259098899, 0.5694377191290852927, 0.5564063356347537113, 0.56939355362834054395, 0.5563699019992810448, 0.5693494140219735284, 0.5563334903042830269, 0.5693053002925596562, 0.5562971005345629574, 0.569261212422690156, 0.55626073267493628534, 0.56921715039497205053, 0.5562243867102305949, 0.5691731141920281319, 0.5561880626252855911, 0.56912910379649693635, 0.556151760404953086, 0.5690851191910327208, 0.55611548003409698427, 0.5690411603583054372, 0.55607922149759326976, 0.56899722728100070876, 0.55604298478032999074, 0.5689533199418198055, 0.5560067698672072463, 0.5689094383234796189, 0.55597057674313717264, 0.5688655824087126394, 0.55593440539304392854, 0.5688217521802669306, 0.55589825580186368205, 0.56877794762090610553, 0.55586212795454459633, 0.5687341687134093031, 0.55582602183604681596, 0.5686904154405711632, 0.55578993743134245297, 0.5686466877852018032, 0.5557538747254155732, 0.56860298573012679367, 0.55571783370326218235, 0.5685593092581871348, 0.55568181434989021255, 0.568515658352239232, 0.5556458166503195086, 0.5684720329951548732, 0.5556098405895818141, 0.56842843316982120364, 0.55557388615272075794, 0.5683848588591407036, 0.55553795332479184076, 0.56834131004603116353, 0.55550204209086242134, 0.5682977867134256617, 0.5554661524360117029, 0.56825428884427253966, 0.55543028434533071977, 0.56821081642153537894, 0.5553944378039223237, 0.56816736942819297837, 0.5553586127969011707, 0.56812394784723933003, 0.5553228093093937073, 0.5680805516616835962, 0.5552870273265381569, 0.56803718085455008586, 0.5552512668334845072, 0.56799383540887823197, 0.5552155278153944961, 0.56795051530772256804, 0.55517981025744159867, 0.56790722053415270483, 0.55514411414481101365, 0.5678639510712533077, 0.55510843946269965035, 0.5678207069021240737, 0.55507278619631611553, 0.5677774880098797083, 0.55503715433088069965, 0.5677342943776499023, 0.55500154385162536416, 0.5676911259885793098, 0.55496595474379372824, 0.5676479828258275248, 0.5549303869926410555, 0.5676048648725690591, 0.55489484058343424084, 0.5675617721119933185, 0.55485931550145179764, 0.5675187045273045816, 0.55482381173198384427, 0.567475662101721976, 0.55478832926033209126, 0.5674326448184794566, 0.5547528680718098285, 0.5673896526608257832, 0.55471742815174191176, 0.5673466856120244967, 0.5546820094854647503, 0.5673037436553538994, 0.55464661205832629335, 0.56726082677410703017, 0.55461123585568601765, 0.5672179349515916431, 0.5545758808629149142, 0.5671750681711301859, 0.5545405470653954756, 0.5671322264160597766, 0.55450523444852168323, 0.5670894096697321826, 0.5544699429976989945, 0.56704661791551379783, 0.5544346726983443298, 0.56700385113678562125, 0.55439942353588605995, 0.5669611093169432345, 0.55436419549576399326, 0.56691839243939678, 0.5543289885634293629, 0.56687570048757093935, 0.5542938027243448146, 0.5668330334449049119, 0.55425863796398439326, 0.566790391294852392, 0.55422349426783353086, 0.5667477740208815482, 0.55418837162138903375, 0.566705181606475001, 0.5541532700101590697, 0.5666626140351298016, 0.55411818941966315603, 0.56662007129035741086, 0.5540831298354321464, 0.566577553355683676, 0.5540480912430082185, 0.5665350602146488115, 0.5540130736279448619, 0.5664925918508073758, 0.5539780769758068648, 0.56645014824772825106, 0.5539431012721703025, 0.5664077293889946213, 0.5539081465026225245, 0.5663653352582039514, 0.55387321265276214183, 0.5663229658389679658, 0.55383829970819901554, 0.5662806211149126272, 0.55380340765455424336, 0.56623830106967811595, 0.553768536477460148, 0.5661960056869188084, 0.55373368616256026474, 0.5661537349503032557, 0.55369885669550932895, 0.566111488843514164, 0.55366404806197326403, 0.56606926735024837196, 0.5536292602476291693, 0.5660270704542168314, 0.55359449323816530735, 0.565984898139144585, 0.5535597470192810923, 0.5659427503887707469, 0.5535250215766870774, 0.56590062718684848073, 0.55349031689610494317, 0.56585852851714497994, 0.5534556329632674847, 0.5658164543634414463, 0.5534209697639186002, 0.5657744047095330699, 0.55338632728381327856, 0.5657323795392290083, 0.55335170550871758745, 0.56569037883635236617, 0.55331710442440866136, 0.5656484025847401749, 0.5532825240166746894, 0.5656064507682433717, 0.5532479642713149034, 0.5655645233707267801, 0.55321342517413956606, 0.5655226203760690884, 0.5531789067109699588, 0.56548074176816283084, 0.5531444088676383704, 0.5654388875309143661, 0.55310993162998808425, 0.56539705764824385776, 0.5530754749838733671, 0.56535525210408525416, 0.553041038915159457, 0.5653134708823862675, 0.5530066234097225517, 0.5652717139671083556, 0.55297222845344979665, 0.56522998134222669997, 0.55293785403223927327, 0.56518827299173018654, 0.55290350013199998706, 0.5651465888996213862, 0.55286916673865185636, 0.56510492904991653425, 0.5528348538381256998, 0.565063293426645511, 0.5528005614163632256, 0.56502168201385182173, 0.55276628945931701896, 0.5649800947955925767, 0.5527320379529505313, 0.5649385317559384724, 0.55269780688323806784, 0.5648969928789737709, 0.5526635962361647766, 0.5648554781487962807, 0.55262940599772663653, 0.5648139875495173372, 0.552595236153930446, 0.56477252106526178274, 0.55256108669079381123, 0.56473107868016794736, 0.55252695759434513494, 0.5646896603783876297, 0.55249284885062360464, 0.5646482661440860772, 0.5524587604456791813, 0.56460689596144196696, 0.5524246923655725879, 0.564565549814647386, 0.55239064459637529777, 0.56452422768790781223, 0.55235661712416952356, 0.56448292956544209557, 0.5523226099350482057, 0.5644416554314824388, 0.552288623015115001, 0.5644004052702743773, 0.5522546563504842711, 0.56435917906607676117, 0.55222070992728107146, 0.5643179768031617358, 0.55218678373164114013, 0.5642767984658147228, 0.55215287774971088603, 0.56423564403833440054, 0.55211899196764737785, 0.5641945135050326859, 0.5520851263716183334, 0.5641534068502347157, 0.5520512809478021075, 0.5641123240582788265, 0.5520174556823876811, 0.564071265113516537, 0.55198365056157465056, 0.5640302300003125289, 0.5519498655715732157, 0.56398921870304462783, 0.55191610069860416927, 0.56394823120610378515, 0.5518823559288988855, 0.5639072674938940589, 0.55184863124869930936, 0.563866327550832596, 0.5518149266442579451, 0.56382541136134961215, 0.5517812421018378452, 0.56378451890988837494, 0.5517475776077125995, 0.56374365018090518407, 0.5517139331481663244, 0.5637028051588693541, 0.5516803087094936513, 0.5636619838282631949, 0.55164670427799971633, 0.5636211861735819942, 0.55161311984000014854, 0.5635804121793339986, 0.55157955538182105965, 0.5635396618300403957, 0.55154601088979903287, 0.5634989351102352954, 0.5515124863502811119, 0.5634582320044657123, 0.5514789817496247904, 0.56341755249729154716, 0.5514454970741980007, 0.563376896573285569, 0.55141203231037910315, 0.56333626421703339617, 0.5513785874445568755, 0.56329565541313347983, 0.55134516246313050163, 0.56325507014619708454, 0.5513117573525095613, 0.563214508400848271, 0.551278372099114019, 0.56317397016172387786, 0.55124500668937421327, 0.5631334554134735041, 0.5512116611097308461, 0.5630929641407594907, 0.5511783353466349721, 0.563052496328256903, 0.55114502938654798796, 0.5630120519606535134, 0.55111174321594162163, 0.5629716310226497833, 0.5510784768212979218, 0.56293123349895884483, 0.5510452301891092469, 0.56289085937430648383, 0.55101200330587825517, 0.5628505086334311224, 0.55097879615811789345, 0.5628101812610838007, 0.55094560873235138686, 0.5627698772420281597, 0.55091244101511222827, 0.5627295965610404239, 0.5508792929929441678, 0.56268933920290938337, 0.55084616465240120186, 0.56264910515243637626, 0.55081305598004756327, 0.56260889439443527205, 0.55077996696245771043, 0.562568706913732454, 0.5507468975862163169, 0.56252854269516680086, 0.55071384783791826114, 0.56248840172358967126, 0.55068081770416861575, 0.56244828398386488533, 0.5506478071715826374, 0.5624081894608687074, 0.550614816226785756, 0.56236811813948982903, 0.55058184485641356484, 0.5623280700046293521, 0.55054889304711180986, 0.5622880450412007719, 0.5505159607855363797, 0.56224804323412995953, 0.5504830480583532947, 0.56220806456835514467, 0.5504501548522386973, 0.56216810902882689946, 0.5504172811538788416, 0.5621281766005081209, 0.5503844269499700826, 0.5620882672683740138, 0.5503515922272188667, 0.5620483810174120745, 0.5503187769723417209, 0.56200851783262207375, 0.55028598117206524285, 0.5619686776990160394, 0.55025320481312609076, 0.5619288606016182406, 0.5502204478822709727, 0.56188906652546516987, 0.55018771036625663696, 0.56184929545560552736, 0.5501549922518498619, 0.5618095473771002037, 0.5501222935258274454, 0.56176982227502226375, 0.5500896141749761953, 0.56173012013445692946, 0.5500569541860929188, 0.5616904409405015635, 0.55002431354598441284, 0.56165078467826565304, 0.5499916922414674534, 0.5616111513328707927, 0.5499590902593687866, 0.561571540889450669, 0.54992650758652511733, 0.5615319533331510426, 0.54989394420978310015, 0.56149238864912973286, 0.549861400115999329, 0.5614528468225566015, 0.5498288752920403273, 0.56141332783861353574, 0.5497963697247825379, 0.56137383168249443223, 0.54976388340111231325, 0.56133435833940518094, 0.54973141630792590545, 0.5612949077945636493, 0.54969896843212945635, 0.5612554800331996648, 0.54966653976063898756, 0.5612160750405549998, 0.5496341302803803906, 0.5611766928018833553, 0.5496017399782894175, 0.5611373333024503451, 0.5495693688413116703, 0.56109799652753347866, 0.5495370168564025914, 0.5610586824624221466, 0.54950468401052745415, 0.56101939109241760295, 0.54947237029066135273, 0.5609801224028329514, 0.5494400756837891923, 0.560940876378993127, 0.54940780017690567954, 0.5609016530062348823, 0.5493755437570153127, 0.56086245226990677005, 0.549343306411132372, 0.5608232741553691282, 0.54931108812628090966, 0.56078411864799406383, 0.5492788888894947407, 0.56074498573316543747, 0.5492467086878174329, 0.56070587539627884753, 0.5492145475083022971, 0.5606667876227416138, 0.5491824053380123779, 0.5606277223979727632, 0.5491502821640204437, 0.56058867970740301284, 0.54911817797340897735, 0.5605496595364747553, 0.5490860927532701663, 0.5605106618706420425, 0.54905402649070589327, 0.56047168669537057067, 0.54902197917282772655, 0.56043273399613766425, 0.5489899507867569103, 0.5603938037584322612, 0.5489579413196243558, 0.56035489596775489693, 0.5489259507585706306, 0.5603160106096176891, 0.5488939790907459503, 0.5602771476695443222, 0.5488620263033101682, 0.56023830713307003253, 0.54883009238343276657, 0.56019948898574159284, 0.5487981773182928467, 0.5601606932131172967, 0.54876628109507911936, 0.5601219198007669433, 0.54873440370098989594, 0.56008316873427182255, 0.5487025451232330783, 0.56004443999922469985, 0.54867070534902615014, 0.56000573358122980046, 0.5486388843655961671, 0.5599670494659027955, 0.54860708216017974785, 0.5599283876388707854, 0.5485752987200230643, 0.55988974808577228644, 0.54854353403238183235, 0.559851130792257214, 0.54851178808452130296, 0.55981253574398686935, 0.5484800608637162524, 0.55977396292663392284, 0.54844835235725097325, 0.55973541232588240087, 0.5484166625524192651, 0.5596968839274276698, 0.5483849914365244252, 0.5596583777169764212, 0.5483533389968792392, 0.55961989368024665743, 0.54832170522080597236, 0.5595814318029676766, 0.5482900900956363595, 0.55954299207088005724, 0.5482584936087115964, 0.5595045744697356449, 0.54822691574738233097, 0.5594661789852975362, 0.54819535649900865315, 0.5594278056033400646, 0.54816381585096008664, 0.55938945430964878626, 0.5481322937906155792, 0.5593511250900204643, 0.5481007903053634937, 0.55931281793026305506, 0.5480693053826015992, 0.55927453281619569334, 0.5480378390097370618, 0.5592362697336486782, 0.5480063911741864352, 0.5591980286684634573, 0.5479749618633756521, 0.5591598096064926136, 0.54794355106474001516, 0.559121612533599851, 0.54791215876572418734, 0.5590834374356599789, 0.547880784953782184, 0.55904528429855889843, 0.5478494296163773627, 0.5590071531081935884, 0.54781809274098241494, 0.55896904385047209034, 0.5477867743150793573, 0.5589309565113134947, 0.5477554743261595219, 0.5588928910766479264, 0.54772419276172354776, 0.55885484753241653047, 0.5476929296092813722, 0.55881682586457145803, 0.54766168485635222124, 0.5587788260590758518, 0.5476304584904646014, 0.5587408481019038326, 0.54759925049915629015, 0.5587028919790404849, 0.5475680608699743277, 0.5586649576764818419, 0.5475368895904750076, 0.5586270451802348729, 0.54750573664822386826, 0.55858915447631746844, 0.54747460203079568406, 0.55855128555075842656, 0.54744348572577445635, 0.55851343838959743843, 0.54741238772075340467, 0.558475612978885075, 0.54738130800333495825, 0.5584378093046827725, 0.54735024656113074703, 0.5584000273530628188, 0.54731920338176159277, 0.55836226711010833977, 0.54728817845285750057, 0.55832452856191328494, 0.54725717176205765005, 0.5582868116945824144, 0.54722618329701038664, 0.55824911649423128435, 0.5471952130453732128, 0.55821144294698623353, 0.54716426099481277936, 0.55817379103898436985, 0.5471333271330048769, 0.55813616075637355606, 0.5471024114476344271, 0.55809855208531239693, 0.5470715139263954739, 0.55806096501197022453, 0.5470406345569911753, 0.5580233995225270858, 0.5470097733271337941, 0.55798585560317372786, 0.54697893022454468993, 0.5579483332401115854, 0.5469481052369543103, 0.55791083241955276626, 0.5469172983521021819, 0.5578733531277200388, 0.5468865095577369025, 0.55783589535084681787, 0.5468557388416161321, 0.5577984590751771516, 0.5468249861915065842, 0.55776104428696570754, 0.54679425159518401766, 0.55772365097247775993, 0.54676353504043322796, 0.55768627911798917597, 0.54673283651504803876, 0.5576489287097864024, 0.54670215600683129333, 0.5576115997341664525, 0.5466714935035948463, 0.5575742921774368923, 0.5466408489931595548, 0.5575370060259158278, 0.5466102224633552706, 0.5574997412659318916, 0.546579613902020831, 0.5574624978838242296, 0.54654902329700405094, 0.55742527586594248836, 0.5465184506361617146, 0.55738807519864680065, 0.5464878959073595663, 0.55735089586830777354, 0.54645735909847230273, 0.5573137378613064749, 0.5464268401973835649, 0.55727660116403442017, 0.54639633919198592877, 0.5572394857628935599, 0.54636585607018089793, 0.55720239164429626565, 0.5463353908198788946, 0.55716531879466531826, 0.54630494342899925164, 0.55712826720043389403, 0.5462745138854702039, 0.5570912368480455516, 0.5462441021772288804, 0.5570542277239542201, 0.54621370829222129576, 0.557017239814624185, 0.54618333221840234196, 0.5569802731065300763, 0.54615297394373578, 0.55694332758615685495, 0.5461226334561942319, 0.5569064032399998004, 0.5460923107437591725, 0.5568695000545644978, 0.5460620057944209206, 0.5568326180163668249, 0.54603171859617863164, 0.5567957571119329398, 0.5460014491370402888, 0.55675891732779926786, 0.5459711974050226955, 0.55672209865051248935, 0.5459409633881514665, 0.55668530106662952634, 0.5459107470744610202, 0.5566485245627175305, 0.54588054845199457035, 0.5566117691253538702, 0.5458503675088041181, 0.55657503474112611774, 0.54582020423295044334, 0.5565383213966320376, 0.54579005861250309763, 0.5565016290784795732, 0.545759930635540395, 0.55646495777328683455, 0.5457298202901494045, 0.55642830746768208564, 0.54569972756442594196, 0.55639167814830373213, 0.5456696524464745617, 0.55635506980180030897, 0.54563959492440854886, 0.55631848241483046756, 0.5456095549863499116, 0.55628191597406296447, 0.5455795326204293721, 0.5562453704661766477, 0.5455495278147863596, 0.55620884587786044494, 0.5455195405575690017, 0.5561723421958133515, 0.5454895708369341167, 0.5561358594067444175, 0.54545961864104720564, 0.55609939749737273643, 0.54542968395808244416, 0.5560629564544274319, 0.54539976677622267466, 0.55602653626464764575, 0.54536986708365939834, 0.55599013691478252645, 0.5453399848685927674, 0.5559537583915912162, 0.5453101201192315767, 0.5559174006818428389, 0.5452802728237932566, 0.5558810637723164886, 0.5452504429705038642, 0.5558447476498012165, 0.5452206305475980763, 0.5558084523010960198, 0.5451908355433191807, 0.5557721777130098287, 0.5451610579459190691, 0.5557359238723614954, 0.5451312977436582287, 0.5556996907659797813, 0.54510155492480573495, 0.5556634783807033454, 0.5450718294776392428, 0.5556272867033807318, 0.54504212139044498, 0.5555911157208703588, 0.5450124306515177384, 0.55555496542004050595, 0.54498275724916086664, 0.55551883578776930286, 0.54495310117168626246, 0.5554827268109447172, 0.5449234624074143643, 0.5554466384764645422, 0.5448938409446741444, 0.5554105707712363858, 0.5448642367718031005, 0.5553745236821776585, 0.54483464987714724803, 0.5553384971962155609, 0.54480508024906111305, 0.5553024913002870735, 0.5447755278759077238, 0.5552665059813389436, 0.5447459927460586034, 0.55523054122632767375, 0.544716474847893762, 0.55519459702221951066, 0.54468697416980168917, 0.55515867335599043306, 0.5446574907001793464, 0.55512277021462614056, 0.5446280244274321592, 0.5550868875851220415, 0.5445985753399740096, 0.55505102545448324185, 0.54456914342622722856, 0.55501518380972453335, 0.544539728674622588, 0.55497936263787038145, 0.5445103310735992937, 0.554943561925954915, 0.54448095061160497755, 0.5549077816610219141, 0.54445158727709568963, 0.55487202183012479816, 0.54442224105853589093, 0.55483628242032661507, 0.5443929119443984459, 0.5548005634187000299, 0.5443635999231646148, 0.554764864812327313, 0.54433430498332404583, 0.5547291865883003289, 0.54430502711337476815, 0.5546935287337205247, 0.54427576630182318405, 0.5546578912356989192, 0.5442465225371840612, 0.55462227408135609093, 0.5442172958079805259, 0.5545866772578221671, 0.54418808610274405465, 0.5545511007522368132, 0.5441588934100144675, 0.5545155445517492201, 0.5441297177183399201, 0.5544800086435180941, 0.5441005590162768965, 0.55444449301471164526, 0.54407141729239020145, 0.55440899765250757625, 0.544042292535252953, 0.55437352254409307114, 0.54401318473344657546, 0.5543380676766647841, 0.5439840938755607915, 0.5543026330374288288, 0.54395501995019361516, 0.55426721861360076714, 0.54392596294595134426, 0.5542318243924055976, 0.5438969228514485526, 0.55419645036107774476, 0.5438678996553080836, 0.55416109650686104813, 0.5438388933461610419, 0.55412576281700875097, 0.5438099039126467868, 0.5540904492787834897, 0.5437809313434129243, 0.55405515587945728236, 0.5437519756271153003, 0.5540198826063115181, 0.54372303675241799283, 0.5539846294466369461, 0.5436941147079933051, 0.55394939638773366434, 0.54366520948252175803, 0.5539141834169111099, 0.5436363210646920831, 0.5538789905214880462, 0.54360744944320121467, 0.55384381768879255357, 0.5435785946067542832, 0.5538086649061620181, 0.5435497565440646079, 0.55377353216094312094, 0.5435209352438536892, 0.55373841944049182677, 0.54349213069485120185, 0.5537033267321733737, 0.5434633428857949874, 0.55366825402336226317, 0.5434345718054310474, 0.55363320130144224777, 0.54340581744251353576, 0.55359816855380632135, 0.5433770797858047516, 0.5535631557678567082, 0.5433483588240751325, 0.55352816293100485265, 0.5433196545461032465, 0.55349319003067140783, 0.5432909669406757863, 0.55345823705428622577, 0.54326229599658756045, 0.5534233039892883464, 0.5432336417026414875, 0.55338839082312598673, 0.543205004047648588, 0.5533534975432565308, 0.5431763830204279783, 0.5533186241371465191, 0.54314777860980686247, 0.5532837705922716374, 0.54311919080462052565, 0.55324893689611670707, 0.5430906195937123271, 0.55321412303617567395, 0.54306206496593369294, 0.55317932899995159873, 0.543033526910144109, 0.5531445547749566454, 0.54300500541521111366, 0.5531098003487120712, 0.5429765004700102913, 0.5530750657087482172, 0.54294801206342526475, 0.55304035084260449675, 0.54291954018434768854, 0.5530056557378293856, 0.5428910848216772417, 0.5529709803819804109, 0.54286264596432162066, 0.5529363247626241421, 0.5428342236011965326, 0.55290168886733617997, 0.54280581772122568813, 0.55286707268370114565, 0.5427774283133407947, 0.55283247619931267207, 0.542749055366481549, 0.55279789940177339214, 0.54272069886959563035, 0.55276334227869492887, 0.54269235881163869394, 0.5527288048176978859, 0.5426640351815743635, 0.55269428700641183653, 0.5426357279683742247, 0.552659788832475314, 0.54260743716101781785, 0.55262531028353580097, 0.54257916274849263136, 0.5525908513472497198, 0.54255090471979409457, 0.552556412011282422, 0.5425226630639255708, 0.5525219922633081786, 0.5424944377698983509, 0.5524875920910101698, 0.54246622882673164583, 0.55245321148208047506, 0.54243803622345258006, 0.55241885042422006296, 0.5424098599490961847, 0.55238450890513878097, 0.5423816999927053906, 0.5523501869125553462, 0.5423535563433310215, 0.5523158844341973346, 0.54232542899003178725, 0.5522816014578011713, 0.54229731792187427686, 0.5522473379711121208, 0.5422692231279329517, 0.552213093961884277, 0.5422411445972901392, 0.55217886941788055347, 0.542213082319036025, 0.55214466432687267283, 0.5421850362822686472, 0.5521104786766411577, 0.54215700647609388894, 0.55207631245497532075, 0.54212899288962547196, 0.55204216564967325447, 0.54210099551198494977, 0.55200803824854182176, 0.54207301433230170064, 0.5519739302393966459, 0.5420450493397129214, 0.5519398416100621007, 0.54201710052336362006, 0.5519057723483713014, 0.54198916787240660956, 0.5518717224421660939, 0.54196125137600250076, 0.55183769187929704563, 0.54193335102331969617, 0.5518036806476234364, 0.54190546680353438247, 0.5517696887350132471, 0.5418775987058305247, 0.551735716129343152, 0.54184974671939985913, 0.55170176281849850824, 0.54182191083344188664, 0.55166782879037334523, 0.54179409103716386585, 0.55163391403287035677, 0.54176628731978080685, 0.5516000185339008905, 0.54173849967051546436, 0.5515661422813849381, 0.5417107280785983313, 0.55153228526325112673, 0.54168297253326763154, 0.5514984474674367082, 0.5416552330237693141, 0.5514646288818875508, 0.5416275095393570461, 0.5514308294945581288, 0.5415998020692922062, 0.55139704929341151323, 0.541572110602843878, 0.55136328826641936275, 0.54154443512928884347, 0.5513295464015619138, 0.54151677563791157655, 0.5512958236868279714, 0.54148913211800423634, 0.5512621201102148996, 0.54146150455886666057, 0.55122843565972861224, 0.5414338929498063593, 0.55119477032338356343, 0.54140629728013850815, 0.5511611240892027381, 0.5413787175391859416, 0.551127496945217643, 0.5413511537162791471, 0.55109388887946829746, 0.54132360580075625787, 0.5510602998800032231, 0.5412960737819630467, 0.5510267299348794359, 0.54126855764925291964, 0.550993179032162436, 0.5412410573919869091, 0.5509596471599261988, 0.5412135729995336676, 0.55092613430625316564, 0.54118610446126946145, 0.55089264045923423446, 0.54115865176657816383, 0.55085916560696875094, 0.541131214904851249, 0.550825709737564499, 0.5411037938654877851, 0.5507922728391376917, 0.54107638863789442844, 0.5507588548998129622, 0.5410489992114854165, 0.55072545590772335446, 0.54102162557568256177, 0.5506920758510103145, 0.54099426771991524556, 0.5506587147178236807, 0.54096692563362041103, 0.55062537249632167503, 0.54093959930624255716, 0.5505920491746708941, 0.54091228872723373265, 0.55055874474104630014, 0.5408849938860535289, 0.5505254591836312115, 0.5408577147721690739, 0.550492192490617294, 0.5408304513750550261, 0.55045894465020455224, 0.540803203684193568, 0.55042571565060131986, 0.54077597168907439937, 0.5503925054800242512, 0.54074875537919473144, 0.55035931412669831186, 0.54072155474405928053, 0.55032614157885677026, 0.54069436977318026123, 0.55029298782474118874, 0.54066720045607738053, 0.55025985285260141383, 0.54064004678227783174, 0.5502267366506955683, 0.54061290874131628725, 0.5501936392072900418, 0.54058578632273489355, 0.55016056051065948223, 0.5405586795160832638, 0.55012750054908678696, 0.5405315883109184721, 0.5500944593108630935, 0.5405045126968050475, 0.55006143678428777135, 0.5404774526633149669, 0.5500284329576684129, 0.54045040820002764975, 0.5499954478193208246, 0.54042337929652995104, 0.54996248135756901844, 0.5403963659424161555, 0.5499295335607452027, 0.5403693681272879715, 0.54989660441718977405, 0.5403423858407545244, 0.5498636939152513083, 0.54031541907243235044, 0.54983080204328655137, 0.54028846781194539103, 0.5497979287896604118, 0.54026153204892498597, 0.5497650741427459505, 0.5402346117730098674, 0.5497322380909243734, 0.5402077069738461539, 0.5496994206225850225, 0.5401808176410873442, 0.54966662172612536664, 0.54015394376439431064, 0.54963384138995099375, 0.54012708533343529364, 0.54960107960247560186, 0.54010024233788589523, 0.54956833635212099046, 0.54007341476742907284, 0.54953561162731705205, 0.54004660261175513336, 0.54950290541650176384, 0.540019805860561727, 0.5494702177081211789, 0.53999302450355384094, 0.5494375484906294176, 0.53996625853044379366, 0.54940489775248865957, 0.53993950793095122823, 0.5493722654821691348, 0.53991277269480310704, 0.5493396516681491154, 0.53988605281173370496, 0.54930705629891490713, 0.5398593482714846037, 0.54927447936296084073, 0.5398326590638046854, 0.5492419208487892642, 0.5398059851784501272, 0.54920938074491053326, 0.53977932660518439435, 0.54917685903984300423, 0.5397526833337782349, 0.5491443557221130248, 0.5397260553540096733, 0.5491118707802549259, 0.53969944265566400447, 0.5490794042028110139, 0.5396728452285337876, 0.549046955978331561, 0.5396462630624188404, 0.54901452609537479855, 0.539619696147126233, 0.54898211454250690723, 0.53959314447247028196, 0.54894972130830201024, 0.53956660802827254416, 0.54891734638134216385, 0.53954008680436181116, 0.5488849897502173496, 0.5395135807905741027, 0.5488526514035254661, 0.5394870899767526611, 0.5488203313298723211, 0.53946061435274794536, 0.54878802951787162233, 0.5394341539084176249, 0.5487557459561449708, 0.5394077086336265742, 0.54872348063332185134, 0.53938127851824686575, 0.5486912335380396248, 0.5393548635521577654, 0.5486590046589435201, 0.53932846372524572554, 0.5486267939846866262, 0.53930207902740437984, 0.5485946015039298838, 0.5392757094485345368, 0.548562427205342077, 0.53924935497854417394, 0.54853027107759982575, 0.53922301560734843225, 0.54849813310938757725, 0.5391966913248696102, 0.5484660132893975984, 0.5391703821210371573, 0.5484339116063299673, 0.5391440879857876692, 0.54840182804889256555, 0.539117808909064881, 0.5483697626058010702, 0.53909154488081966166, 0.54833771526577894563, 0.53906529589101000855, 0.5483056860175574355, 0.5390390619296010407, 0.5482736748498755549, 0.53901284298656499403, 0.54824168175148008293, 0.53898663905188121494, 0.54820970671112555337, 0.53896045011553615414, 0.5481777497175742483, 0.5389342761675233618, 0.54814581075959618926, 0.538908117197843481, 0.5481138898259691298, 0.538881973196504242, 0.548081986905478547, 0.5388558441535204568, 0.54805010198691763454, 0.538829730058914013, 0.5480182350590872937, 0.53880363090271386837, 0.5479863861107961269, 0.53877754667495604465, 0.54795455513086042856, 0.538751477365683622, 0.54792274210810417784, 0.53872542296494673353, 0.54789094703135903127, 0.53869938346280255915, 0.547859169889464314, 0.5386733588493153199, 0.5478274106712670131, 0.53864734911455627235, 0.547795669365621769, 0.5386213542486037029, 0.54776394596139086805, 0.5385953742415429216, 0.5477322404474442348, 0.5385694090834662573, 0.5477005528126594243, 0.5385434587644730512, 0.54766888304592161433, 0.5385175232746696513, 0.5476372311361235973, 0.53849160260416940676, 0.54760559707216577394, 0.5384656967430926627, 0.5475739808429561439, 0.5384398056815667536, 0.54754238243741029955, 0.53841392940972599855, 0.5475108018444514174, 0.5383880679177116946, 0.5474792390530102505, 0.5383622211956721121, 0.5474476940520251215, 0.53833638923376248833, 0.54741616683044191477, 0.53831057202214502235, 0.5473846573772140687, 0.53828476955098886897, 0.547353165681302568, 0.5382589818104701333, 0.5473216917316759364, 0.5382332087907718653, 0.547290235517310229, 0.5382074504820840536, 0.547258797027189025, 0.5381817068746036207, 0.5472273762503034198, 0.53815597795853441676, 0.54719597317565201784, 0.5381302637240872139, 0.54716458779224092445, 0.5381045641614797013, 0.54713322008908373955, 0.538078879260936479, 0.5471018700552015493, 0.5380532090126890526, 0.54707053767962291846, 0.53802755340697582757, 0.54703922295138388395, 0.53800191243404210363, 0.54700792585952794664, 0.53797628608414006963, 0.546976646393106064, 0.5379506743475287974, 0.5469453845411766432, 0.5379250772144742365, 0.5469141402928055331, 0.53789949467524920875, 0.54688291363706601714, 0.53787392672013340254, 0.5468517045630388064, 0.5378483733394133674, 0.5468205130598120316, 0.5378228345233825085, 0.54678933911648123584, 0.53779731026234108113, 0.54675818272214936816, 0.537771800546596185, 0.54672704386592677484, 0.5377463053664617592, 0.5466959225369311934, 0.53772082471225857614, 0.54666481872428774447, 0.53769535857431423635, 0.54663373241712892483, 0.5376699069429631634, 0.5466026636045946002, 0.5376444698085465976, 0.5465716122758319982, 0.53761904716141259116, 0.54654057841999570036, 0.53759363899191600254, 0.546509562026247636, 0.53756824529041849096, 0.546478563083757074, 0.5375428660472885109, 0.5464475815817006162, 0.53751750125290130697, 0.5464166175092621903, 0.5374921508976389083, 0.54638567085563304217, 0.5374668149718901228, 0.54635474161001172915, 0.53744149346605053216, 0.5463238297616041127, 0.53741618637052248627, 0.54629293529962335153, 0.537390893675715098, 0.54626205821328989416, 0.5373656153720442375, 0.54623119849183147186, 0.53734035144993252703, 0.54620035612448309206, 0.53731510189980933543, 0.5461695311004870305, 0.5372898667121107728, 0.54613872340909282475, 0.53726464587727968514, 0.5461079330395572666, 0.5372394393857656492, 0.5460771599811443959, 0.53721424722802496636, 0.54604640422312549256, 0.5371890693945206582, 0.54601566575477907033, 0.5371639058757224609, 0.54598494456539086887, 0.5371387566621068194, 0.5459542406442538482, 0.53711362174415688245, 0.5459235539806681801, 0.53708850111236249757, 0.5458928845639412423, 0.53706339475722020504, 0.54586223238338761106, 0.53703830266923323306, 0.545831597428329054, 0.5370132248389114925, 0.54580097968809452403, 0.53698816125677157116, 0.54577037915202015117, 0.53696311191333672883, 0.5457397958094492368, 0.5369380767991368919, 0.54570922964973224634, 0.5369130559047086483, 0.54567868066222680203, 0.5368880492205952414, 0.54564814883629767645, 0.5368630567373465658, 0.54561763416131678567, 0.5368380784455191616, 0.54558713662666318234, 0.53681311433567620897, 0.5455566562217230485, 0.53678816439838752294, 0.5455261929358896894, 0.5367632286242295484, 0.5454957467585635262, 0.5367383070037853548, 0.54546531767915208927, 0.5367133995276446306, 0.5454349056870700117, 0.5366885061864036784, 0.5454045107717390219, 0.5366636269706654096, 0.54537413292258793754, 0.536638761871039339, 0.54534377212905265833, 0.53661391087814157996, 0.54531342838057615945, 0.5365890739825948387, 0.54528310166660848475, 0.53656425117502840977, 0.5452527919766067402, 0.53653944244607816995, 0.5452224993000350867, 0.536514647786386574, 0.5451922236263647342, 0.5364898671866026487, 0.5451619649450739341, 0.5364651006373819883, 0.54513172324564797326, 0.5364403481293867489, 0.54510149851757916725, 0.53641560965328564356, 0.54507129075036685336, 0.53639088519975393696, 0.545041099933517384, 0.53636617475947344016, 0.5450109260565441206, 0.53634147832313250575, 0.5449807691089674263, 0.5363167958814260226, 0.54495062908031465963, 0.53629212742505541066, 0.54492050596012016827, 0.53626747294472861574, 0.5448903997379252819, 0.53624283243116010463, 0.54486031040327830596, 0.53621820587507085975, 0.5448302379457345149, 0.5361935932671883741, 0.5448001823548561457, 0.5361689945982466464, 0.54477014362021239174, 0.53614440985898617555, 0.5447401217313793956, 0.53611983904015395595, 0.5447101166779402428, 0.5360952821325034719, 0.5446801284494849552, 0.5360707391267946931, 0.54465015703561048504, 0.5360462100137940693, 0.54462020242592070786, 0.5360216947842745252, 0.5445902646100264164, 0.5359971934290154554, 0.5445603435775453135, 0.53597270593880271916, 0.5445304393181020065, 0.5359482323044286359, 0.5445005518213280003, 0.5359237725166919795, 0.54447068107686169124, 0.5358993265663979738, 0.5444408270743483604, 0.53587489444435828693, 0.5444109898034401669, 0.53585047614139102697, 0.5443811692537961422, 0.53582607164832073664, 0.5443513654150821839, 0.53580168095597838804, 0.54432157827697104797, 0.5357773040552013777, 0.5442918078291423438, 0.53575294093683352225, 0.54426205406128252723, 0.5357285915917250523, 0.54423231696308489437, 0.53570425601073260835, 0.5442025965242495751, 0.5356799341847192353, 0.5441728927344835268, 0.53565562610455437785, 0.5441432055835005285, 0.53563133176111387486, 0.54411353506102117373, 0.53560705114527995524, 0.54408388115677286487, 0.53558278424794123233, 0.54405424386048980667, 0.5355585310599926993, 0.54402462316191299997, 0.5355342915723357237, 0.54399501905079023547, 0.53551006577587804324, 0.5439654315168760877, 0.53548585366153376004, 0.54393586054993190804, 0.5354616552202233366, 0.5439063061397258195, 0.5354374704428735899, 0.54387676827603270995, 0.53541329932041768715, 0.54384724694863422585, 0.5353891418437951406, 0.54381774214731876636, 0.5353649980039518025, 0.5437882538618814766, 0.5353408677918398605, 0.5437587820821242424, 0.5353167511984178325, 0.54372932679785568316, 0.53529264821465056195, 0.5436998879988911465, 0.5352685588315092127, 0.5436704656750527017, 0.5352444830399712642, 0.5436410598161691333, 0.5352204208310205068, 0.54361167041207593555, 0.53519637219564703635, 0.5435822974526153061, 0.53517233712484725, 0.54355294092763613956, 0.5351483156096238411, 0.5435236008269940221, 0.53512430764098579375, 0.5434942771405512246, 0.53510031320994837895, 0.54346496985817669735, 0.5350763323075331489, 0.54343567896974606305, 0.5350523649247679326, 0.54340640446514161164, 0.5350284110526868308, 0.5433771463342522938, 0.5350044706823302112, 0.5433479045669737151, 0.5349805438047447038, 0.5433186791532081299, 0.5349566304109831957, 0.543289470082864435, 0.5349327304921048266, 0.5432602773458581645, 0.53490884403917498413, 0.54323110093211148317, 0.53488497104326529816, 0.54320194083155318014, 0.5348611114954536373, 0.5431727970341186639, 0.5348372653868241027, 0.54314366952974995543, 0.5348134327084670247, 0.54311455830839568294, 0.5347896134514789567, 0.54308546336001107537, 0.53476580760696267123, 0.5430563846745579569, 0.5347420151660271549, 0.5430273222420047406, 0.53471823611978760364, 0.54299827605232642286, 0.5346944704593654177, 0.5429692460955045775, 0.53467071817588819745, 0.5429402323615273497, 0.5346469792604897379, 0.54291123484038944985, 0.5346232537043100245, 0.54288225352209214816, 0.53459954149849522827, 0.542853288396643269, 0.5345758426341977008, 0.54282433945405718416, 0.53455215710257596997, 0.5427954066843548077, 0.53452848489479473437, 0.54276649007756359004, 0.5345048260020248598, 0.5427375896237175119, 0.53448118041544337355, 0.54270870531285707853, 0.53445754812623346005, 0.5426798371350293141, 0.53443392912558445587, 0.54265098508028775565, 0.5344103234046918458, 0.5426221491386924478, 0.53438673095475725704, 0.54259332930030993597, 0.5343631517669884554, 0.5425645255552132618, 0.5343395858325993401, 0.54253573789348195656, 0.53431603314280993935, 0.5425069663052020357, 0.5342924936888464054, 0.5424782107804659931, 0.53426896746194101013, 0.54244947130937279546, 0.5342454544533321404, 0.54242074788202787616, 0.534221954654264293, 0.54239204048854312986, 0.53419846805598807, 0.54236334911903690703, 0.534174994649760175, 0.54233467376363400777, 0.53415153442684340735, 0.5423060144124656761, 0.5341280873785066578, 0.54227737105566959524, 0.5341046534960249044, 0.5422487436833898805, 0.5340812327706792071, 0.5422201322857770747, 0.53405782519375670373, 0.5421915368529881419, 0.5340344307565506049, 0.54216295737518646223, 0.5340110494503601898, 0.54213439384254182637, 0.5339876812664908012, 0.54210584624523042895, 0.5339643261962538409, 0.5420773145734348639, 0.5339409842309667656, 0.5420487988173441186, 0.53391765536195308155, 0.54202029896715356837, 0.5338943395805423404, 0.5419918150130649703, 0.53387103687807013465, 0.54196334694528645865, 0.53384774724587809265, 0.5419348947540325383, 0.5338244706753138748, 0.54190645842952408004, 0.5338012071577311679, 0.5418780379619883144, 0.5337779566844896814, 0.54184963334165882646, 0.5337547192469551426, 0.54182124455877554985, 0.53373149483649929176, 0.541792871603584762, 0.5337082834444998782, 0.54176451446633907806, 0.53368508506234065523, 0.5417361731372974455, 0.5336618996814113757, 0.54170784760672513865, 0.53363872729310778744, 0.5416795378648937532, 0.5336155678888316287, 0.5416512439020812008, 0.5335924214599906239, 0.5416229657085717035, 0.5335692879979984789, 0.54159470327465578814, 0.53354616749427487597, 0.54156645659063028134, 0.5335230599402454704, 0.5415382256467983038, 0.53349996532734188497, 0.54151001043346926473, 0.5334768836470017057, 0.54148181094095885633, 0.53345381489066847773, 0.54145362715958904895, 0.5334307590497917001, 0.5414254590796880852, 0.53340771611582682194, 0.5413973066915904745, 0.53338468608023523784, 0.54136916998563698806, 0.533361668934484283, 0.54134104895217465334, 0.5333386646700472289, 0.5413129435815567486, 0.53331567327840327913, 0.5412848538641427972, 0.53329269475103756445, 0.54125677979029856323, 0.5332697290794411388, 0.5412287213503960452, 0.53324677625511097414, 0.5412006785348134709, 0.53322383626954995676, 0.5411726513339352925, 0.5332009091142668825, 0.5411446397381521808, 0.5331779947807764521, 0.5411166437378610199, 0.533155093260599267, 0.5410886633234649024, 0.53313220454526182474, 0.54106069848537312344, 0.53310932862629651486, 0.54103274921400117597, 0.53308646549524161385, 0.5410048154997707449, 0.53306361514364128146, 0.5409768973331097026, 0.5330407775630455557, 0.54094899470445210267, 0.5330179527450103486, 0.54092110760423817563, 0.5329951406810974418, 0.5408932360229143227, 0.53297234136287448204, 0.5408653799509331115, 0.5329495547819149772, 0.5408375393787532704, 0.53292678092979829137, 0.5408097142968396833, 0.5329040197981096405, 0.5407819046956633842, 0.53288127137844008846, 0.5407541105657015527, 0.532858535662386542, 0.54072633189743750766, 0.53283581264155174703, 0.540698568681360703, 0.5328131023075442837, 0.5406708209079667225, 0.53279040465197856217, 0.54064308856775727366, 0.53276771966647481853, 0.54061537165124018384, 0.5327450473426591102, 0.54058767014892939384, 0.53272238767216331136, 0.5405599840513449537, 0.532699740646625109, 0.54053231334901301737, 0.53267710625768799836, 0.5405046580324658371, 0.53265448449700127846, 0.5404770180922417586, 0.53263187535622004816, 0.54044939351888521625, 0.5326092788270052013, 0.5404217843029467276, 0.53258669490102342284, 0.54039419043498288815, 0.532564123569947184, 0.54036661190555636676, 0.5325415648254547387, 0.5403390487052359001, 0.5325190186592301185, 0.54031150082459628795, 0.53249648506296312845, 0.54028396825421838763, 0.5324739640283493432, 0.54025645098468910943, 0.5324514555470901021, 0.54022894900660141116, 0.53242895961089250546, 0.5402014623105542937, 0.5324064762114694097, 0.54017399088715279515, 0.5323840053405394236, 0.54014653472700798647, 0.5323615469898269035, 0.540119093820736966, 0.5323391011510619493, 0.54009166815896285493, 0.5323166678159804003, 0.54006425773231479174, 0.53229424697632383057, 0.54003686253142792764, 0.5322718386238395449, 0.54000948254694342124, 0.53224944275028057456, 0.5399821177695084338, 0.53222705934740567265, 0.5399547681897761242, 0.5322046884069793106, 0.53992743379840564403, 0.5321823299207716732, 0.53990011458606213245, 0.53215998388055865464, 0.53987281054341671133, 0.53213765027812185424, 0.53984552166114648035, 0.53211532910524857225, 0.53981824792993451196, 0.53209302035373180555, 0.5397909893404698468, 0.5320707240153702434, 0.5397637458834474881, 0.53204844008196826326, 0.53973651754956839747, 0.53202616854533592636, 0.5397093043295394893, 0.53200390939728897394, 0.53968210621407362653, 0.53198166262964882256, 0.5396549231938896155, 0.53195942823424256026, 0.5396277552597122009, 0.5319372062029029419, 0.53960060240227206103, 0.5319149965274683854, 0.53957346461230580283, 0.53189279919978296724, 0.53954634188055595724, 0.5318706142116964185, 0.53951923419777097417, 0.5318484415550641205, 0.5394921415547052176, 0.5318262812217471007, 0.53946506394211896107, 0.53180413320361202834, 0.5394380013507783822, 0.5317819974925312105, 0.53941095377145555876, 0.53175987408038258785, 0.539383921194928463, 0.53173776295904973056, 0.53935690361198095744, 0.5317156641204218336, 0.53932990101340278947, 0.53169357755639371334, 0.53930291338998958715, 0.53167150325886580297, 0.5392759407325428541, 0.53164944121974414844, 0.539248983031869965, 0.5316273914309404043, 0.5392220402787841603, 0.5316053538843718294, 0.53919511246410454185, 0.53158332857196128305, 0.53916819957865606827, 0.5315613154856372207, 0.53914130161326954976, 0.53153931461733368974, 0.5391144185587816435, 0.5315173259589903255, 0.53908755040603484907, 0.53149534950255234713, 0.53906069714587750367, 0.5314733852399705534, 0.5390338587691637774, 0.53145143316320131856, 0.5390070352667536682, 0.53142949326420658827, 0.53898022662951299786, 0.53140756553495387555, 0.5389534328483134066, 0.5313856499674162566, 0.5389266539140323485, 0.53136374655357236673, 0.53889988981755308756, 0.53134185528540639633, 0.5388731405497646918, 0.53131997615490808627, 0.5388464061015620295, 0.5312981091540727249, 0.5388196864638457644, 0.5312762542749011427, 0.53879298162752235047, 0.53125441150939970927, 0.5387662915835040281, 0.5312325808495803284, 0.53873961632270881875, 0.53121076228746043455, 0.53871295583606052063, 0.53118895581506298865, 0.53868631011448870396, 0.53116716142441647373, 0.53865967914892870657, 0.53114537910755489137, 0.53863306293032162906, 0.53112360885651775747, 0.53860646144961433, 0.5311018506633500978, 0.5385798746977594219, 0.53108010452010244435, 0.5385533026657152661, 0.5310583704188308314, 0.5385267453444459684, 0.53103664835159679114, 0.53850020272492137447, 0.5310149383104673497, 0.53847367479811706487, 0.5309932402875150233, 0.53844716155501435136, 0.530971554274817814, 0.5384206629866002715, 0.5309498802644592057, 0.5383941790838675844, 0.5309282182485281606, 0.5383677098378147663, 0.5309065682191191142, 0.5383412552394460058, 0.5308849301683319722, 0.5383148152797711994, 0.5308633040882721062, 0.53828838994980594693, 0.5308416899710503494, 0.53826197924057154733, 0.53082008780878299297, 0.53823558314309499366, 0.530798497593591782, 0.5382092016484089688, 0.53077691931760391124, 0.5381828347475518411, 0.5307553529729520213, 0.53815648243156765957, 0.53073379855177419496, 0.53813014469150614963, 0.5307122560462139525, 0.53810382151842270864, 0.53069072544842024817, 0.5380775129033784011, 0.53066920675054746637, 0.53805121883743995466, 0.53064769994475541727, 0.53802493931167975536, 0.53062620502320933304, 0.5379986743171758431, 0.5306047219780798638, 0.5379724238450119073, 0.5305832508015430739, 0.5379461878862772825, 0.5305617914857804376, 0.53791996643206694395, 0.53054034402297883555, 0.5378937594734815029, 0.53051890840533055035, 0.53786756700162720286, 0.530497484625033263, 0.5378413890076159142, 0.5304760726742900487, 0.5378152254825651303, 0.5304546725453093731, 0.5377890764175979634, 0.5304332842303050882, 0.5377629418038431394, 0.5304119077214964287, 0.5377368216324349945, 0.53039054301110800764, 0.5377107158945134697, 0.5303691900913698129, 0.53768462458122410753, 0.53034784895451720304, 0.5376585476837180469, 0.53032651959279090336, 0.5376324851931520186, 0.53030520199843700216, 0.5376064371006883417, 0.53028389616370694663, 0.5375804033974949187, 0.53026260208085753935, 0.5375543840747452314, 0.5302413197421509337, 0.53752837912361833627, 0.53022004913985463066, 0.5375023885352988601, 0.5301987902662414744, 0.5374764123009769961, 0.5301775431135896489, 0.5374504504118484993, 0.5301563076741826735, 0.53742450285911468234, 0.53013508394030939925, 0.53739856963398241057, 0.5301138719042640053, 0.5373726507276640986, 0.53009267155834599475, 0.5373467461313777059, 0.5300714828948601906, 0.5373208558363467317, 0.53005030590611673246, 0.53729497983380021184, 0.53002914058443107207, 0.53726911811497271327, 0.53000798692212396974, 0.5372432706711043308, 0.5299868449115214905, 0.53721743749344068227, 0.52996571454495500035, 0.5371916185732329047, 0.5299445958147611619, 0.53716581390173764947, 0.5299234887132819312, 0.5371400234702170786, 0.5299023932328645537, 0.53711424726993886033, 0.5298813093658615598, 0.5370884852921761646, 0.5298602371046307619, 0.53706273752820765915, 0.52983917644153525006, 0.5370370039693175053, 0.52981812736894338847, 0.53701128460679535374, 0.5297970898792288111, 0.53698557943193633987, 0.52977606396477041863, 0.53695988843604108024, 0.52975504961795237394, 0.53693421161041566804, 0.52973404683116409873, 0.53690854894637166865, 0.5297130555968002695, 0.5368829004352261158, 0.5296920759072608139, 0.5368572660683015076, 0.52967110775495090686, 0.5368316458369258017, 0.5296501511322809667, 0.53680603973243241157, 0.5296292060316666513, 0.5367804477461602023, 0.5296082724455288546, 0.5367548698694534863, 0.52958735036629370267, 0.53672930609366201914, 0.5295664397863925496, 0.53670375641014099574, 0.5295455406982619744, 0.5366782208102510459, 0.52952465309434377654, 0.53665269928535823, 0.52950377696708497254, 0.5366271918268340355, 0.52948291230893779213, 0.5366016984260553719, 0.5294620591123596743, 0.53657621907440456767, 0.5294412173698132641, 0.5365507537632693651, 0.5294203870737664081, 0.5365253024840429171, 0.5293995682166921514, 0.53649986522812378266, 0.52937876079106873316, 0.53647444198691592254, 0.52935796478937958325, 0.5364490327518286956, 0.52933718020411331867, 0.53642363751427685464, 0.5293164070277637392, 0.53639825626568054184, 0.52929564525282982455, 0.53637288899746528553, 0.5292748948718157297, 0.53634753570106199545, 0.52925415587723078167, 0.53632219636790695887, 0.5292334282615894758, 0.53629687098944183664, 0.52921271201741147195, 0.53627155955711365895, 0.52919200713722159066, 0.53624626206237482163, 0.52917131361354980947, 0.5362209784966830815, 0.5291506314389312596, 0.5361957088515015534, 0.5291299606059062214, 0.53617045311829870463, 0.5291093011070201215, 0.5361452112885483522, 0.5290886529348235287, 0.5361199833537296585, 0.5290680160818721502, 0.536094769305327127, 0.5290473905407268282, 0.53606956913483059846, 0.52902677630395353585, 0.536044382833735247, 0.52900617336412337376, 0.5360192103935415758, 0.52898558171381256646, 0.53599405180575541334, 0.5289650013456024582, 0.5359689070618879095, 0.52894443225207951007, 0.5359437761534555313, 0.52892387442583529533, 0.5359186590719800594, 0.5289033278594664968, 0.53589355580898858354, 0.52888279254557490227, 0.5358684663560134989, 0.5288622684767674014, 0.5358433907045925023, 0.52884175564565598176, 0.53581832884626858773, 0.5288212540448577254, 0.5357932807725900429, 0.52880076366699480515, 0.5357682464751104454, 0.5287802845046944806, 0.53574322594538865814, 0.52875981655058909494, 0.53571821917498882573, 0.5287393597973160711, 0.53569322615548037086, 0.5287189142375179081, 0.53566824687843799, 0.5286984798638421773, 0.53564328133544164963, 0.5286780566689415192, 0.5356183295180765823, 0.52865764464547363924, 0.53559339141793328276, 0.52863724378610130444, 0.5355684670266075039, 0.52861685408349233983, 0.53554355633570025326, 0.52859647553031962475, 0.53551865933681778894, 0.52857610811926108914, 0.53549377602157161515, 0.5285557518429997103, 0.5354689063815784795, 0.52853540669422350835, 0.5354440504084603679, 0.528515072665625544, 0.53541920809384450165, 0.52849474974990391375, 0.5353943794293633331, 0.52847443793976174684, 0.5353695644066545418, 0.52845413722790720147, 0.5353447630173610309, 0.5284338476070534614, 0.535319975253130923, 0.5284135690699187323, 0.5352952011056175566, 0.52839330160922623773, 0.53527044056647948205, 0.5283730452177042163, 0.53524569362738045795, 0.5283527998880859174, 0.53522096027998944677, 0.52833256561310959807, 0.53519624051598061197, 0.5283123423855185193, 0.5351715343270333134, 0.52829213019806094213, 0.5351468417048321035, 0.5282719290434901246, 0.5351221626410667242, 0.52825173891456431803, 0.53509749712743210227, 0.528231559804046763, 0.5350728451556283463, 0.5282113917047056865, 0.53504820671736074214, 0.52819123460931429793, 0.53502358180433974983, 0.52817108851065078544, 0.53499897040828099936, 0.52815095340149831294, 0.5349743725209052869, 0.52813082927464501576, 0.5349497881339385715, 0.5281107161228839979, 0.5349252172391119708, 0.5280906139390133278, 0.5349006598281617575, 0.52807052271583603524, 0.5348761158928293554, 0.52805044244616010775, 0.53485158542486133625, 0.52803037312279848685, 0.5348270684160094149, 0.52801031473856906476, 0.534802564858030447, 0.5279902672862946806, 0.53477807474268642426, 0.52797023075880311735, 0.5347535980617444706, 0.52795020514892709775, 0.5347291348069768395, 0.5279301904495042812, 0.5347046849701609091, 0.5279101866533772601, 0.5346802485430791789, 0.52789019375339355606, 0.53465582551751926645, 0.5278702117424056171, 0.5346314158852739032, 0.5278502406132708132, 0.53460701963814093065, 0.52783028035885143375, 0.5345826367679232973, 0.52781033097201468345, 0.53455826726642905464, 0.5277903924456326788, 0.5345339111254713533, 0.527770464772582445, 0.5345095683368684392, 0.52775054794574591206, 0.5344852388924436505, 0.5277306419580099116, 0.53446092278402541334, 0.5277107468022661731, 0.5344366200034472389, 0.5276908624714113208, 0.53441233054254771865, 0.52767098895834686973, 0.53438805439317052184, 0.5276511262559792225, 0.534363791547164391, 0.5276312743572196661, 0.5343395419963831387, 0.52761143325498436794, 0.53431530573268564414, 0.52759160294219437256, 0.5342910827479358486, 0.52757178341177559826, 0.53426687303400275286, 0.52755197465665883345, 0.534242676582760413, 0.5275321766697797336, 0.534218493386087937, 0.5275123894440788173, 0.53419432343586948085, 0.52749261297250146297, 0.5341701667239942451, 0.5274728472479979055, 0.53414602324235647165, 0.5274530922635232329, 0.53412189298285543915, 0.52743334801203738257, 0.53409777593739546055, 0.52741361448650513804, 0.53407367209788587876, 0.52739389167989612553, 0.5340495814562410633, 0.5273741795851848104, 0.5340255040043804066, 0.52735447819535049397, 0.5340014397342283208, 0.5273347875033773097, 0.53397738863771423333, 0.52731510750225422035, 0.53395335070677258456, 0.52729543818497501376, 0.5339293259333428231, 0.5272757795445383001, 0.5339053143093694027, 0.5272561315739475084, 0.5338813158268017792, 0.5272364942662108827, 0.53385733047759440596, 0.5272168676143414791, 0.53383335825370673103, 0.527197251611357162, 0.5338093991471031934, 0.52717764625028060095, 0.53378545314975321944, 0.52715805152413926715, 0.5337615202536312193, 0.52713846742596543, 0.53373760045071658376, 0.5271188939487961539, 0.53371369373299368017, 0.5270993310856732946, 0.53368980009245184934, 0.5270797788296434961, 0.5336659195210854017, 0.52706023717375818677, 0.533642052010893614, 0.5270407061110735766, 0.5336181975538807258, 0.5270211856346506536, 0.5335943561420559361, 0.52700167573755517996, 0.53357052776743339924, 0.5269821764128576893, 0.53354671242203222223, 0.5269626876536334831, 0.53352291009787646074, 0.5269432094529626271, 0.53349912078699511547, 0.5269237418039299484, 0.5334753444814221295, 0.52690428469962503145, 0.5334515811731963837, 0.52688483813314221534, 0.5334278308543616941, 0.52686540209758059007, 0.5334040935169668081, 0.52684597658604399335, 0.533380369153065401, 0.5268265615916410071, 0.53335665775471607263, 0.5268071571074849542, 0.5333329593139823436, 0.5267877631266938951, 0.53330927382293265255, 0.5267683796423906246, 0.5332856012736403519, 0.52674900664770266855, 0.5332619416581837049, 0.52672964413576228005, 0.5332382949686458821, 0.5267102920997064366, 0.5332146611971149576, 0.52669095053267683674, 0.5331910403356839063, 0.5266716194278198965, 0.5331674323764505997, 0.52665229877828674624, 0.53314383731151780314, 0.526632988577233227, 0.53312025513299317195, 0.52661368881781988786, 0.533096685832989248, 0.5265943994932119818, 0.5330731294036234568, 0.52657512059657946317, 0.5330495858370181037, 0.5265558521210969838, 0.53302605512530037046, 0.52653659405994388996, 0.5330025372606023122, 0.52651734640630421886, 0.53297903223506085335, 0.52649810915336669553, 0.53295554004081778493, 0.52647888229432472965, 0.532932060670019761, 0.52645966582237641176, 0.5329085941148182951, 0.52644045973072451055, 0.53288514036736975705, 0.5264212640125764692, 0.5328616994198353695, 0.526402078661144402, 0.53283827126438120425, 0.5263829036696450916, 0.53281485589317817994, 0.52636373903129998514, 0.53279145329840205716, 0.52634458473933519115, 0.5327680634722334367, 0.5263254407869814766, 0.5327446864068577548, 0.52630630716747426305, 0.5327213220944652805, 0.5262871838740536239, 0.5326979705272511126, 0.52626807089996428086, 0.5326746316974151756, 0.52624896823845560055, 0.5326513055971622167, 0.5262298758827815917, 0.53262799221870180263, 0.52621079382620090134, 0.5326046915542483162, 0.52619172206197681183, 0.53258140359602095285, 0.52617266058337723795, 0.53255812833624371764, 0.5261536093836747227, 0.5325348657671454212, 0.526134568456146435, 0.53251161588095967755, 0.52611553779407416593, 0.53248837866992489994, 0.5260965173907443257, 0.5324651541262842979, 0.52607750723944794036, 0.532441942242285874, 0.5260585073334806484, 0.53241874301018242004, 0.5260395176661426977, 0.53239555642223151456, 0.52602053823073894223, 0.53237238247069551875, 0.5260015690205788389, 0.53234922114784157405, 0.52598261002897644424, 0.5323260724459415978, 0.5259636612492504111, 0.53230293635727228107, 0.5259447226747239856, 0.5322798128741150845, 0.52592579429872500377, 0.5322567019887562358, 0.5259068761145858886, 0.53223360369348672545, 0.52588796811564364627, 0.532210517980602305, 0.5258690702952398637, 0.5321874448424034823, 0.5258501826467207046, 0.53216438427119551886, 0.52583130516343690663, 0.53214133625928842684, 0.5258124378387437784, 0.53211830079899696556, 0.5257935806660011957, 0.53209527788264063817, 0.52577473363857359903, 0.5320722675025436887, 0.52575589674982998957, 0.53204926965103509847, 0.52573706999314392664, 0.5320262843204485831, 0.5257182533618935243, 0.5320033115031225895, 0.5256994468494614481, 0.5319803511914002919, 0.5256806504492349119, 0.53195740337762958977, 0.5256618641546056749, 0.5319344680541631034, 0.52564308795897003805, 0.5319115452133581717, 0.5256243218557288413, 0.5318886348475768482, 0.5256055658382874601, 0.53186573694918589853, 0.5255868199000558027, 0.53184285151055679674, 0.52556808403444830606, 0.53181997852406572247, 0.52554935823488393374, 0.5317971179820935574, 0.52553064249478617214, 0.53177426987702588226, 0.52551193680758302734, 0.5317514342012529738, 0.52549324116670702246, 0.5317286109471698013, 0.5254745555655951935, 0.53170580010717602356, 0.5254558799976890872, 0.53168300167367598584, 0.52543721445643475743, 0.53166021563907871634, 0.52541855893528276194, 0.5316374419957979236, 0.52539991342768815966, 0.5316146807362519928, 0.52538127792711050696, 0.5315919318528639829, 0.525362652427013855, 0.5315691953380616235, 0.52534403692086674626, 0.5315464711842773114, 0.52532543140214221176, 0.5315237593839481079, 0.52530683586431776744, 0.5315010599295157352, 0.52528825030087541146, 0.5314783728134265737, 0.5252696747053016209, 0.53145569802813165855, 0.5252511090710873487, 0.5314330355660866768, 0.5252325533917280205, 0.53141038541975196385, 0.5252140076607235313, 0.5313877475815925009, 0.5251954718715782427, 0.5313651220440779113, 0.5251769460178009795, 0.5313425087996824577, 0.52515843009290502717, 0.531319907840885039, 0.5251399240904081276, 0.531297319160169187, 0.5251214280038324773, 0.53127474275002306364, 0.5251029418267047233, 0.53125217860293945756, 0.5250844655525559606, 0.5312296267114157813, 0.5250659991749217287, 0.53120708706795406783, 0.5250475426873420089, 0.531184559665060968, 0.525029096083361221, 0.53116204449524774686, 0.52501065935652822, 0.531139541551030281, 0.52499223250039629343, 0.5311170508249290554, 0.524973815508523158, 0.5310945723094691602, 0.5249554083744709564, 0.53107210599718028776, 0.5249370110918062547, 0.5310496518805967295, 0.5249186236541000386, 0.5310272099522573731, 0.52490024605492771094, 0.53100478020470569897, 0.52488187828786908834, 0.53098236263048977784, 0.5248635203465083982, 0.53095995722216226674, 0.52484517222443427553, 0.53093756397228040713, 0.52482683391523975993, 0.530915182873406021, 0.52480850541252229276, 0.5308928139181055079, 0.52479018670988371363, 0.53087045709894984264, 0.5247718778009302577, 0.530848112408514571, 0.5247535786792725525, 0.53082577983937980816, 0.52473528933852561486, 0.5308034593841302342, 0.52471700977230884797, 0.5307811510353550922, 0.524698739974246038, 0.5307588547856481849, 0.5246804799379653515, 0.5307365706276078712, 0.524662229657099332, 0.53071429855383706395, 0.5246439891252848974, 0.53069203855694322634, 0.52462575833616333624, 0.5306697906295383689, 0.5246075372833803053, 0.53064755476423904697, 0.5245893259605858262, 0.5306253309536663572, 0.5245711243614342827, 0.530603119190445935, 0.5245529324795844173, 0.5305809194672079509, 0.52453475030869932833, 0.5305587317765871083, 0.524516577842446467, 0.5305365561112226401, 0.5244984150744976344, 0.53051439246375830546, 0.52448026199852897843, 0.5304922408268423875, 0.52446211860822099075, 0.5304701011931276896, 0.52444398489725850375, 0.53044797355527153305, 0.5244258608593306876, 0.53042585790593575366, 0.5244077464881310473, 0.530403754237786699, 0.5243896417773574195, 0.530381662543495225, 0.52437154672071196944, 0.5303595828157366938, 0.52435346131190118833, 0.5303375150471909701, 0.52433538554463588995, 0.5303154592305424185, 0.52431731941263120774, 0.5302934153584799005, 0.5242992629096065921, 0.53027138342369677153, 0.5242812160292858071, 0.53024936341889087785, 0.52426317876539692736, 0.5302273553367645542, 0.52424515111167233544, 0.53020535917002461996, 0.5242271330618487184, 0.5301833749113823769, 0.5242091246096670655, 0.5301614025535536061, 0.5241911257488726644, 0.5301394420892585649, 0.52417313647321509885, 0.5301174935112219839, 0.5241551567764482452, 0.5300955568121730644, 0.5241371866523302698, 0.53007363198484547506, 0.5241192260946236259, 0.53005171902197734926, 0.5241012750970950506, 0.53002981791631128205, 0.52408333365351556204, 0.53000792866059432746, 0.5240654017576604561, 0.52998605124757799516, 0.524047479403309304, 0.529964185670018248, 0.5240295665842459488, 0.5299423319206754988, 0.52401166329425850275, 0.52992048999231460775, 0.5239937695271393442, 0.52989865987770487927, 0.52397588527668511466, 0.5298768415696200591, 0.523958010536696716, 0.52985503506083833164, 0.5239401453009793072, 0.529833240344142317, 0.5239222895633423017, 0.52981145741231906764, 0.5239044433175993642, 0.5297896862581600665, 0.52388660655756840806, 0.52976792687446122316, 0.523868779277071592, 0.52974617925402287146, 0.5238509614699353173, 0.52972444338964976625, 0.52383315312999022484, 0.5297027192741510812, 0.52381535425107119243, 0.52968100690034040527, 0.52379756482701733135, 0.5296593062610357401, 0.5237797848516719839, 0.529637617349059497, 0.5237620143188827203, 0.5296159401572384948, 0.52374425322250133566, 0.5295942746784039559, 0.52372650155638384724, 0.52957262090539150427, 0.52370875931439049133, 0.529550978831041162, 0.5236910264903857206, 0.5295293484481973469, 0.5236733030782382009, 0.5295077297497088698, 0.52365558907182080856, 0.52948612272842893113, 0.5236378844650106275, 0.52946452737721511837, 0.52362018925168894615, 0.5294429436889294036, 0.5236025034257412543, 0.5294213716564381398, 0.523584826981057241, 0.529399811272612059, 0.52356715991153079097, 0.52937826253032626867, 0.52354950221105998187, 0.52935672542246024947, 0.5235318538735470814, 0.5293351999418978521, 0.5235142148928985445, 0.52931368608152729467, 0.5234965852630250106, 0.5292921838342411597, 0.52347896497784129996, 0.5292706931929363913, 0.52346135403126641195, 0.5292492141505142929, 0.52344375241722352135, 0.5292277466998805238, 0.52342616012963997553, 0.5292062908339450966, 0.52340857716244729213, 0.52918484654562237445, 0.52339100350958115516, 0.52916341382783106824, 0.5233734391649814132, 0.52914199267349423377, 0.523355884122592076, 0.5291205830755392691, 0.52333833837636131145, 0.5290991850268979113, 0.5233208019202414431, 0.5290777985205062346, 0.52330327474818894717, 0.5290564235493046466, 0.5232857568541644495, 0.5290350601062378864, 0.5232682482321327228, 0.5290137081842550209, 0.52325074887606268405, 0.5289923677763094425, 0.5232332587799273911, 0.52897103887535886684, 0.52321577793770404017, 0.52894972147436532917, 0.5231983063433739632, 0.5289284155662951819, 0.52318084399092262436, 0.52890712114411909207, 0.523163390874339618, 0.5288858382008120385, 0.52314594698761866494, 0.5288645667293533087, 0.5231285123247576103, 0.52884330672272649663, 0.5231110868797584205, 0.5288220581739194996, 0.52309367064662718003, 0.52880082107592451553, 0.52307626361937408925, 0.5287795954217380408, 0.52305886579201346106, 0.52875838120436086646, 0.5230414771585637183, 0.5287371784167980763, 0.5230240977130473906, 0.528715987052059044, 0.5230067274494911122, 0.52869480710315743004, 0.5229893663619256184, 0.5286736385631111795, 0.5229720144443857431, 0.5286524814249425189, 0.5229546716909104161, 0.52863133568167795367, 0.5229373380955426599, 0.52861020132634826545, 0.52292001365232958725, 0.52858907835198850933, 0.522902698355322398, 0.52856796675163801125, 0.5228853921985763767, 0.5285468665183403648, 0.5228680951761508891, 0.5285257776451434291, 0.52285080728210938023, 0.52850470012509932614, 0.52283352851051937093, 0.52848363395126443746, 0.52281625885545245525, 0.52846257911669940215, 0.5227989983109842978, 0.5284415356144691137, 0.5227817468711946306, 0.5284205034376427171, 0.5227645045301672505, 0.52839948257929360704, 0.52274727128199001647, 0.52837847303249942425, 0.5227300471207548467, 0.5283574747903420534, 0.52271283204055771546, 0.52833648784590762047, 0.52269562603549865127, 0.5283155121922864894, 0.52267842909968173303, 0.5282945478225732602, 0.5226612412272150877, 0.52827359472986676586, 0.52264406241221088767, 0.52825265290727006946, 0.52262689264878534767, 0.5282317223478904623, 0.5226097319310587223, 0.5282108030448394606, 0.52259258025315530295, 0.5281898949912328028, 0.5225754376092034151, 0.5281689981801904471, 0.5225583039933354158, 0.52814811260483656867, 0.5225411793996876905, 0.52812723825829955757, 0.5225240638224006506, 0.52810637513371201516, 0.52250695725561873057, 0.52808552322421075217, 0.522489859693490385, 0.5280646825229367856, 0.5224727711301680862, 0.5280438530230353365, 0.52245569155980832124, 0.5280230347176558268, 0.52243862097657158894, 0.52800222759995187716, 0.5224215593746223977, 0.5279814316630813043, 0.5224045067481292622, 0.527960646900206118, 0.522387463091264701, 0.52793987330449251876, 0.5223704283982052336, 0.527919110869110895, 0.52235340266313137764, 0.5278983595872358206, 0.5223363858802276463, 0.5278776194520460522, 0.5223193780436825456, 0.5278568904567245266, 0.52230237914768857144, 0.5278361725944583581, 0.522285389186442207, 0.52781546585843883574, 0.52226840815414392013, 0.5277947702418614212, 0.5222514360449981602, 0.5277740857379257456, 0.52223447285321335595, 0.5277534123398356071, 0.522217518573001912, 0.52773275004079896845, 0.522200573198580207, 0.5277120988340279542, 0.5221836367241685903, 0.52769145871273884815, 0.5221667091439913792, 0.52767082967015209075, 0.5221497904522768566, 0.5276502116994922766, 0.52213288064325726805, 0.52762960479398815175, 0.522115979711168819, 0.52760900894687261096, 0.52209908765025167234, 0.52758842415138269556, 0.5220822044547499452, 0.5275678504007595905, 0.52206533011891170673, 0.52754728768824862156, 0.5220484646369889749, 0.5275267360070992535, 0.52203160800323771456, 0.5275061953505650866, 0.52201476021191783373, 0.5274856657119038551, 0.5219979212572931817, 0.5274651470843774235, 0.52198109113363154595, 0.52744463946125178487, 0.52196426983520464955, 0.52742414283579705774, 0.52194745735628814837, 0.52740365720128748385, 0.5219306536911616285, 0.5273831825510014256, 0.5219138588341086035, 0.52736271887822136316, 0.5218970727794165117, 0.5273422661762338923, 0.5218802955213767135, 0.5273218244383297214, 0.52186352705428448886, 0.5273013936578036695, 0.52184676737243903405, 0.5272809738279546633, 0.52183001647014345975, 0.5272605649420857344, 0.5218132743417047878, 0.52724016699350401735, 0.52179654098143394875, 0.5272197799755207469, 0.521779816383645779, 0.5271994038814512551, 0.52176310054265901843, 0.5271790387046149692, 0.5217463934527963075, 0.5271586844383354092, 0.5217296951083841845, 0.5271383410759401845, 0.52171300550375308304, 0.5271180086107609923, 0.5216963246332373294, 0.52709768703613361473, 0.52167965249117513967, 0.52707737634539791606, 0.5216629890719086174, 0.5270570765318978407, 0.52164633436978375053, 0.52703678758898141014, 0.52162968837915040904, 0.5270165095100007209, 0.52161305109436234225, 0.5269962422883119416, 0.52159642250977717603, 0.5269759859172753106, 0.5215798026197564101, 0.5269557403902551338, 0.5215631914186654158, 0.52693550570061978153, 0.5215465889008734328, 0.5269152818417416865, 0.521529995060753567, 0.52689506880699734124, 0.5215134098926827875, 0.5268748665897672954, 0.52149683339104192423, 0.52685467518343615325, 0.52148026555021566514, 0.5268344945813925716, 0.5214637063645925537, 0.52681432477702925653, 0.52144715582856498596, 0.52679416576374296183, 0.5214306139365292083, 0.52677401753493448587, 0.5214140806828853145, 0.52675388008400866906, 0.52139755606203724327, 0.52673375340437439185, 0.5213810400683927756, 0.52671363748944457177, 0.5213645326963635319, 0.5266935323326361612, 0.5213480339403649698, 0.52667343792737014496, 0.5213315437948163812, 0.5266533542670715376, 0.5213150622541408895, 0.5266332813451693809, 0.5212985893127654477, 0.5266132191550967418, 0.52128212496512083484, 0.5265931676902907094, 0.52126566920564165405, 0.5265731269441923929, 0.5212492220287663296, 0.52655309691024691895, 0.5212327834289371043, 0.52653307758190342905, 0.52121635340060003727, 0.5265130689526150774, 0.52119993193820500076, 0.5264930710158390283, 0.52118351903620567804, 0.5264730837650364536, 0.52116711468905956034, 0.5264531071936725302, 0.5211507188912279446, 0.5264331412952164381, 0.5211343316371759308, 0.52641318606314135717, 0.52111795292137241917, 0.5263932414909244653, 0.52110158273829010767, 0.5263733075720469355, 0.5210852210824054896, 0.5263533842999939341, 0.52106886794819885073, 0.52633347166825461755, 0.52105252333015426695, 0.52631356967032213046, 0.5210361872227596013, 0.52629367829969360307, 0.52101985962050650187, 0.5262737975498701488, 0.5210035405178903988, 0.5262539274143568616, 0.5209872299094105019, 0.5262340678866628141, 0.5209709277895697982, 0.52621421896030105424, 0.52095463415287504875, 0.52619438062878860394, 0.520938348993836787, 0.52617455288564645577, 0.52092207230696931526, 0.52615473572439957096, 0.5209058040867907028, 0.526134929138576877, 0.520889544327822783, 0.52611513312171126515, 0.52087329302459115074, 0.52609534766733958774, 0.52085705017162516, 0.5260755727690026563, 0.5208408157634579211, 0.52605580842024523875, 0.52082458979462629837, 0.5260360546146160571, 0.52080837225967090707, 0.526016311345667785, 0.52079216315313611166, 0.5259965786069570454, 0.52077596246957002244, 0.525976856392044408, 0.5207597702035244933, 0.5259571446944943872, 0.52074358634955511944, 0.5259374435078754394, 0.5207274109022212344, 0.5259177528257599606, 0.5207112438560859076, 0.52589807264172428407, 0.52069508520571594195, 0.52587840294934867796, 0.5206789349456818711, 0.525858743742217343, 0.5206627930705579569, 0.52583909501391840985, 0.52064665957492218726, 0.52581945675804393694, 0.5206305344533562728, 0.52579982896818990826, 0.52061441770044564534, 0.52578021163795623036, 0.52059830931077945423, 0.52576060476094673064, 0.5205822092789505648, 0.5257410083307691545, 0.52056611759955555524, 0.52572142234103516313, 0.52055003426719471417, 0.52570184678536033133, 0.52053395927647203825, 0.52568228165736414494, 0.52051789262199522955, 0.52566272695066999825, 0.52050183429837569305, 0.5256431826589051921, 0.5204857843002285338, 0.52562364877570093123, 0.52046974262217255526, 0.52560412529469232185, 0.52045370925883025566, 0.5255846122095183694, 0.5204376842048278261, 0.52556510951382197637, 0.5204216674547951483, 0.52554561720124993963, 0.52040565900336579114, 0.52552613526545294807, 0.52038965884517700923, 0.52550666370008558066, 0.5203736669748697396, 0.52548720249880630353, 0.5203576833870885995, 0.525467751655277468, 0.5203417080764818839, 0.52544831116316530813, 0.52032574103770156304, 0.5254288810161399384, 0.52030978226540327945, 0.52540946120787535137, 0.52029383175424634634, 0.5253900517320494152, 0.52027788949889374403, 0.52537065258234387147, 0.5202619554940121183, 0.5253512637524443327, 0.52024602973427177746, 0.5253318852360402803, 0.52023011221434668994, 0.5253125170268250619, 0.52021420292891448186, 0.5252931591184958891, 0.52019830187265643447, 0.52527381150475383533, 0.5201824090402574815, 0.5252544741793038332, 0.52016652442640620696, 0.5252351471358546725, 0.5201506480257948426, 0.5252158303681189977, 0.52013477983311926536, 0.5251965238698133057, 0.5201189198430789944, 0.5251772276346579433, 0.5201030680503771897, 0.52515794165637710525, 0.5200872244497206486, 0.5251386659286988316, 0.52007138903581980364, 0.5251194004453550056, 0.52005556180338872037, 0.5251001452000813513, 0.52003974274714509415, 0.5250809001866174311, 0.5200239318618102486, 0.52506166539870664366, 0.52000812914210913244, 0.52504244083009622164, 0.5199923345827703172, 0.52502322647453722926, 0.5199765481785259949, 0.52500402232578455985, 0.51996076992411197526, 0.52498482837759693364, 0.51994499981426768365, 0.52496564462373689584, 0.5199292378437361582, 0.52494647105797081365, 0.51991348400726404767, 0.52492730767406887483, 0.5198977382996016088, 0.52490815446580508445, 0.51988200071550270395, 0.5248890114269572632, 0.51986627124972479853, 0.52486987855130704515, 0.51985054989702895854, 0.52485075583263987505, 0.5198348366521798485, 0.52483164326474500644, 0.5198191315099457282, 0.524812540841415499, 0.5198034344650984511, 0.5247934485564482167, 0.5197877455124134613, 0.52477436640364382526, 0.51977206464666979134, 0.52475529437680678955, 0.51975639186265005983, 0.52473623246974537224, 0.5197407271551404686, 0.5247171806762716304, 0.51972507051893080077, 0.5246981389902014141, 0.519709421948814418, 0.52467910740535436356, 0.51969378143958825804, 0.52466008591555390753, 0.51967814898605283245, 0.5246410745146272601, 0.5196625245830122242, 0.5246220731964054194, 0.5196469082252740849, 0.5246030819547231646, 0.5196312999076496327, 0.5245841007834190543, 0.5196156996249536497, 0.5245651296763354234, 0.5196001073720044796, 0.52454616862731838175, 0.5195845231436240252, 0.52452721763021781134, 0.5195689469346377461, 0.52450827667888736436, 0.51955337873987465606, 0.5244893457671844604, 0.5195378185541673208, 0.52447042488897028525, 0.5195222663723518554, 0.52445151403810978723, 0.5195067221892679219, 0.52443261320847167633, 0.5194911859997587272, 0.5244137223939284208, 0.51947565779867102006, 0.52439484158835624595, 0.51946013758085508914, 0.52437597078563513096, 0.51944462534116476066, 0.5243571099796488075, 0.5194291210744573955, 0.5243382591642847568, 0.51941362477559388706, 0.52431941833343420763, 0.51939813643943865914, 0.52430058748099213437, 0.51938265606085966296, 0.52428176660085725423, 0.5193671836347283753, 0.5242629556869320255, 0.51935171915591979566, 0.52424415473312264524, 0.51933626261931244416, 0.5242253637333390467, 0.51932081401978835917, 0.5242065826814948976, 0.5193053733522330944, 0.5241878115715075974, 0.5192899406115357172, 0.5241690503972982756, 0.51927451579258880566, 0.5241502991527917889, 0.51925909889028844653, 0.5241315578319167195, 0.51924368989953423266, 0.5241128264286053729, 0.51922828881522926046, 0.5240941049367937751, 0.5192128956322801282, 0.5240753933504216712, 0.5191975103455969325, 0.5240566916634325227, 0.519182132950093267, 0.5240379998697735048, 0.51916676344068621947, 0.5240193179633955056, 0.51915140181229636935, 0.5240006459382531224, 0.51913604805984778576, 0.5239819837883046605, 0.5191207021782680246, 0.5239633315075121304, 0.5191053641624881268, 0.523944689089841246, 0.5190900340074426153, 0.52392605652926142214, 0.5190747117080694933, 0.52390743381974577263, 0.5190593972593102413, 0.5238888209552711078, 0.5190440906561098151, 0.5238702179298179324, 0.5190287918934166434, 0.5238516247373704435, 0.5190135009661826253, 0.52383304137191652824, 0.51899821786936312806, 0.52381446782744776175, 0.51898294259791698463, 0.52379590409795940463, 0.51896767514680649147, 0.523777350177450401, 0.5189524155109974058, 0.5237588060599233764, 0.51893716368545894386, 0.52374027173938463557, 0.51892191966516377794, 0.52372174720984416007, 0.51890668344508803444, 0.52370323246531560617, 0.5188914550202112911, 0.5236847274998163029, 0.51887623438551657535, 0.5236662323073672498, 0.51886102153599036115, 0.52364774688199311426, 0.5188458164666225671, 0.5236292712177222301, 0.51883061917240655403, 0.52361080530858659487, 0.51881542964833912284, 0.52359234914862186794, 0.51880024788942051165, 0.5235739027318673684, 0.51878507389065439383, 0.5235554660523660723, 0.51876990764704787574, 0.52353703910416461134, 0.518754749153611494, 0.52351862188131327016, 0.5187395984053592137, 0.5235002143778659842, 0.5187244553973084254, 0.52348181658788033786, 0.5187093201244799435, 0.523463428505417562, 0.5186941925818980033, 0.52344505012454253177, 0.51867907276459025907, 0.523426681439323765, 0.51866396066758778155, 0.52340832244383341936, 0.5186488562859250558, 0.5233899731321472907, 0.5186337596146399784, 0.5233716334983448104, 0.51861867064877385585, 0.5233533035365090439, 0.5186035893833714015, 0.52333498324072668794, 0.51858851581348073385, 0.52331667260508806866, 0.51857344993415337386, 0.5232983716236871395, 0.51855839174044424274, 0.52328008029062147906, 0.5185433412274116596, 0.52326179859999228885, 0.51852829839011733943, 0.5232435265459043911, 0.5185132632236263901, 0.523225264122466227, 0.5184982357230073107, 0.52320701132378985424, 0.5184832158833319892, 0.5231887681439909448, 0.51846820369967569963, 0.52317053457718878294, 0.5184531991671171002, 0.5231523106175062633, 0.518438202280738231, 0.52313409625906988836, 0.51842321303562451156, 0.52311589149600976664, 0.5184082314268647384, 0.52309769632245961025, 0.51839325744955108303, 0.5230795107325567333, 0.51837829109877908956, 0.52306133472044204913, 0.51836333236964767234, 0.5230431682802600686, 0.5183483812572591138, 0.52302501140615889803, 0.5183334377567190619, 0.5230068640922902367, 0.51831850186313652796, 0.522988726332809375, 0.5183035735716238846, 0.52297059812187519245, 0.5182886528772968631, 0.5229524794536501551, 0.5182737397752745513, 0.5229343703223003141, 0.5182588342606793914, 0.5229162707219953029, 0.51824393632863717735, 0.5228981806469083357, 0.5182290459742770529, 0.52288010009121620495, 0.51821416319273150915, 0.5228620290490992794, 0.51819928797913638246, 0.5228439675147415022, 0.5181844203286308517, 0.52282591548233038836, 0.5181695602363574366, 0.52280787294605702295, 0.51815470769746199524, 0.522789839900116059, 0.5181398627070937214, 0.52277181633870571545, 0.5181250252604051428, 0.5227538022560277747, 0.5181101953525521186, 0.52273579764628758077, 0.5180953729786938372, 0.52271780250369403743, 0.51808055813399281385, 0.5226998168224596058, 0.51806575081361488856, 0.5226818405968003022, 0.5180509510127292238, 0.5226638738209356963, 0.518036158726508302, 0.5226459164890889089, 0.5180213739501279234, 0.5226279685954866098, 0.51800659667876720454, 0.5226100301343590162, 0.51799182690760857444, 0.5225921010999398897, 0.5179770646318377738, 0.52257418148646653483, 0.51796230984664385196, 0.52255627128817979727, 0.51794756254721916497, 0.5225383704993240609, 0.5179328227287593732, 0.52252047911414724654, 0.5179180903864634392, 0.5225025971269008093, 0.5179033655155336251, 0.52248472453183973684, 0.51788864811117549106, 0.5224668613232225473, 0.51787393816859789234, 0.52244900749531128693, 0.5178592356830129773, 0.5224311630423715283, 0.5178445406496361854, 0.52241332795867236825, 0.51782985306368624454, 0.5223955022384864257, 0.5178151729203851691, 0.52237768587608983964, 0.5178005002149582575, 0.5223598788657622668, 0.5177858349426340905, 0.52234208120178688025, 0.51777117709864452796, 0.5223242928784503665, 0.5177565266782247078, 0.5223065138900429242, 0.5177418836766130427, 0.5222887442308582616, 0.51772724808905121847, 0.52227098389519359465, 0.51771261991078419186, 0.52225323287734964493, 0.5176979991370601881, 0.5222354911716306377, 0.5176833857631306986, 0.5222177587723442995, 0.51766877978425047903, 0.5222000356738018569, 0.51765418119567754683, 0.5221823218703180332, 0.5176395899926731792, 0.5221646173562110477, 0.5176250061705019106, 0.5221469221258026128, 0.517610429724431531, 0.522129236173417932, 0.5175958606497330831, 0.5221115594933856986, 0.51758129894168086055, 0.52209389208003809243, 0.51756674459555240543, 0.52207623392771077914, 0.51755219760662850634, 0.52205858503074290715, 0.5175376579701931959, 0.52204094538347710613, 0.5175231256815337489, 0.5220233149802594848, 0.51750860073594067947, 0.52200569381543962896, 0.5174940831287077396, 0.52198808188337059914, 0.5174795728551319167, 0.52197047917840892935, 0.51746506991051343093, 0.52195288569491462427, 0.5174505742901557336, 0.52193530142725115734, 0.51743608598936550476, 0.5219177263697854692, 0.5174216050034526509, 0.5219001605168879651, 0.51740713132773030297, 0.5218826038629325133, 0.5173926649575148139, 0.52186505640229644297, 0.5173782058881257566, 0.5218475181293605418, 0.5173637541148859219, 0.52182998903850905456, 0.5173493096331213159, 0.5218124691241296805, 0.5173348724381611583, 0.52179495838061357193, 0.51732044252533788, 0.52177745680235533175, 0.5173060198899871206, 0.5217599643837530113, 0.51729160452744772685, 0.5217424811192081091, 0.5172771964330617499, 0.521725007003125568, 0.5172627956021744436, 0.5217075420299137739, 0.51724840203013426176, 0.52169008619398455305, 0.5172340157122928564, 0.52167263948975317047, 0.5172196366440050757, 0.52165520191163832793, 0.51720526482062896106, 0.5216377734540621618, 0.51719090023752574593, 0.521620354111450241, 0.5171765428900598528, 0.52160294387823156537, 0.5171621927735988917, 0.5215855427488385631, 0.5171478498835136573, 0.5215681507177070893, 0.5171335142151781276, 0.52155076777927642363, 0.51711918576396946097, 0.5215333939279892684, 0.5171048645252679944, 0.5215160291582917465, 0.5170905504944572414, 0.52149867346463339983, 0.5170762436669238895, 0.5214813268414671866, 0.5170619440380577985, 0.5214639892832494801, 0.5170476516032519978, 0.52144666078444006557, 0.5170333663579026849, 0.52142934133950214, 0.51701908829740922266, 0.52141203094290230856, 0.5170048174171741373, 0.52139472958911058304, 0.5169905537126031166, 0.5213774372726003802, 0.51697629717910500703, 0.52136015398784851956, 0.5169620478120918125, 0.52134287972933522154, 0.51694780560697869135, 0.5213256144915441051, 0.5169335705591839549, 0.52130835826896218635, 0.5169193426641290648, 0.52129111105607987603, 0.51690512191723863114, 0.5212738728473909779, 0.5168909083139404102, 0.5212566436373926867, 0.5168767018496653023, 0.5212394234205855858, 0.51686250251984735, 0.52122221219147364587, 0.51684831031992373537, 0.5212050099445642225, 0.5168341252453347783, 0.5211878166743680541, 0.5168199472915239341, 0.5211706323753992606, 0.51680577645393779157, 0.52115345704217534044, 0.5167916127280260708, 0.52113629066921716965, 0.51677745610924162106, 0.5211191332510489992, 0.5167633065930404184, 0.52110198478219845326, 0.51674916417488156396, 0.5210848452571965275, 0.5167350288502272816, 0.5210677146705775865, 0.5167209006145429157, 0.5210505930168793623, 0.51670677946329692925, 0.5210334802906429526, 0.5166926653919609016, 0.52101637648641281803, 0.5166785583960095263, 0.520999281598736781, 0.5166644584709206089, 0.5209821956221660232, 0.51665036561217506523, 0.5209651185512550841, 0.51663627981525691896, 0.5209480503805618586, 0.5166222010756532993, 0.52093099110464759547, 0.5166081293888544394, 0.5209139407180768948, 0.5165940647503536738, 0.5208968992154177069, 0.5165800071556474365, 0.5208798665912413294, 0.51656595660023525886, 0.5208628428401224061, 0.5165519130796197675, 0.52084582795663892475, 0.5165378765893066819, 0.52082882193537221487, 0.51652384712480481295, 0.52081182477090694617, 0.51650982468162606, 0.5207948364578311264, 0.51649580925528540946, 0.5207778569907360995, 0.5164818008413009324, 0.52076088636421654363, 0.5164677994351937823, 0.5207439245728704691, 0.51645380503248819345, 0.5207269716112992169, 0.51643981762871147813, 0.5207100274741074561, 0.51642583721939402534, 0.52069309215590318257, 0.5164118638000692979, 0.52067616565129771626, 0.51639789736627383086, 0.5206592479549057002, 0.51638393791354722946, 0.5206423390613450979, 0.5163699854374321665, 0.52062543896523719183, 0.5163560399334743809, 0.5206085476612065811, 0.51634210139722267537, 0.52059166514388117996, 0.5163281698242289139, 0.52057479140789221527, 0.51631424521004802054, 0.52055792644787422547, 0.51630032755023797626, 0.52054107025846505776, 0.516286416840359818, 0.5205242228343058668, 0.5162725130759776355, 0.52050738417004111246, 0.51625861625265857016, 0.52049055426031855807, 0.5162447263659728123, 0.52047373309978926836, 0.51623084341149359935, 0.5204569206831076078, 0.5162169673847972139, 0.5204401170049312384, 0.5162030982814629813, 0.52042332205992111787, 0.51618923609707326774, 0.52040653584274149796, 0.5161753808272134783, 0.520389758348059922, 0.5161615324674720548, 0.5203729895705472236, 0.51614769101344047355, 0.52035622950487752454, 0.5161338564607132436, 0.5203394781457282326, 0.51612002880488790443, 0.52032273548778004, 0.516106208041565024, 0.5203060015257169213, 0.51609239416634819666, 0.5202892762542261315, 0.5160785871748440409, 0.52027255966799820433, 0.51606478706266219783, 0.52025585176172694997, 0.5160509938254153285, 0.5202391525301094539, 0.5160372074587191121, 0.5202224619678460739, 0.5160234279581922441, 0.5202057800696404391, 0.51600965531945643377, 0.5201891068301994476, 0.5159958895381364024, 0.5201724422442332648, 0.51598213060985988134, 0.5201557863064553214, 0.5159683785302576099, 0.5201391390115823116, 0.51595463329496333275, 0.5201225003543341908, 0.5159408948996137988, 0.52010587032943417457, 0.5159271633398487585, 0.52008924893160873577, 0.51591343861131096205, 0.5200726361555876035, 0.5158997207096461571, 0.5200560319961037606, 0.5158860096305030872, 0.52003943644789344205, 0.51587230536953348917, 0.52002284950569613314, 0.51585860792239209146, 0.52000627116425456747, 0.51584491728473661203, 0.51998970141831472516, 0.5158312334522277563, 0.51997314026262583076, 0.5158175564205292149, 0.5199565876919403516, 0.51580388618530766196, 0.5199400437010139959, 0.515790222742232753, 0.51992350828460571073, 0.5157765660869771227, 0.5199069814374776802, 0.5157629162152163832, 0.5198904631543953237, 0.5157492731226291216, 0.5198739534301272941, 0.5157356368048968985, 0.51985745225944547535, 0.51572200725770424534, 0.5198409596371249813, 0.51570838447673866307, 0.5198244755579441535, 0.5156947684576906196, 0.51980800001668455904, 0.51568115919625354804, 0.51979153300813098925, 0.5156675566881238443, 0.5197750745270714577, 0.5156539609290008657, 0.51975862456829719784, 0.51564037191458692857, 0.51974218312660266185, 0.51562678964058730605, 0.5197257501967855182, 0.5156132141027102265, 0.51970932577364665014, 0.5155996452966668711, 0.51969290985199015375, 0.5155860832181713723, 0.519676502426623336, 0.5155725278629408112, 0.519660103492356713, 0.51555897922669521606, 0.5196437130440040081, 0.51554543730515756014, 0.51962733107638214977, 0.51553190209405375935, 0.5196109575843112703, 0.5155183735891126709, 0.51959459256261470355, 0.5155048517860660907, 0.5195782360061189834, 0.5154913366806487517, 0.5195618879096538413, 0.51547782826859832176, 0.51954554826805220504, 0.51546432654565540153, 0.51952921707615019675, 0.5154508315075635228, 0.51951289432878713085, 0.5154373431500691461, 0.5194965800208055123, 0.51542386146892165907, 0.5194802741470510351, 0.515410386459873374, 0.5194639767023725796, 0.51539691811867952654, 0.51944768768162221176, 0.5153834564410982728, 0.51943140707965518033, 0.51537000142289068823, 0.5194151348913299157, 0.51535655305982076504, 0.5193988711115080276, 0.51534311134765541036, 0.5193826157350543034, 0.5153296762821644445, 0.5193663687568367066, 0.5153162478591205986, 0.51935013017172637433, 0.51530282607429951294, 0.51933389997459761625, 0.5152894109234797347, 0.51931767816032791215, 0.51527600240244271615, 0.51930146472379791044, 0.5152626005069728126, 0.5192852596598914261, 0.5152492052328572804, 0.51926906296349543886, 0.51523581657588627504, 0.5192528746295000917, 0.51522243453185284927, 0.5192366946527986889, 0.5152090590965529507, 0.5192205230282876937, 0.5151956902657854204, 0.51920435975086672703, 0.5151823280353519904, 0.51918820481543856584, 0.51516897240105728213, 0.5191720582169091404, 0.51515562335870880405, 0.51915591995018753346, 0.5151422809041169501, 0.5191397900101859779, 0.5151289450330949974, 0.51912366839181985507, 0.5151156157414591046, 0.5191075550900076928, 0.5151022930250283095, 0.51909145009967116384, 0.5150889768796245275, 0.51907535341573508375, 0.5150756673010725493, 0.5190592650331274094, 0.5150623642852000391, 0.5190431849467792368, 0.51504906782783753275, 0.5190271131516247998, 0.5150357779248184355, 0.5190110496426014675, 0.5150224945719790203, 0.5189949944146497433, 0.51500921776515842576, 0.51897894746271326253, 0.51499594750019865423, 0.5189629087817387906, 0.51498268377294456957, 0.51894687836667622183, 0.5149694265792438957, 0.5189308562124785769, 0.5149561759149472142, 0.5189148423141020013, 0.5149429317759079626, 0.51889883666650576374, 0.5149296941579824324, 0.51888283926465225395, 0.5149164630570297669, 0.5188668501035069813, 0.5149032384689119596, 0.5188508691780385727, 0.5148900203894938521, 0.51883489648321877094, 0.5148768088146431321, 0.5188189320140224328, 0.51486360374023033143, 0.5188029757654275273, 0.5148504051621288243, 0.51878702773241513395, 0.5148372130762148252, 0.5187710879099694408, 0.51482402747836738697, 0.5187551562930777428, 0.514810848364468399, 0.5187392328767304399, 0.51479767573040258487, 0.51872331765592103543, 0.5147845095720575011, 0.5187074106256461339, 0.5147713498853235347, 0.51869151178090544, 0.51475819666609390145, 0.5186756211167017558, 0.51474504991026464375, 0.51865973862804097956, 0.514731909613734629, 0.51864386430993210413, 0.51471877577240554737, 0.51862799815738721453, 0.5147056483821819101, 0.5186121401654214867, 0.51469252743897104754, 0.5185962903290531854, 0.514679412938683107, 0.5185804486433036625, 0.51466630487723105116, 0.51856461510319735563, 0.51465320325053065585, 0.5185487897037617855, 0.51464010805450050837, 0.5185329724400275549, 0.51462701928506200547, 0.5185171633070283467, 0.5146139369381393514, 0.51850136229980092186, 0.51460086100965955584, 0.5184855694133851178, 0.5145877914955524325, 0.5184697846428238468, 0.5145747283917505965, 0.5184540079831630939, 0.51456167169418946306, 0.5184382394294519153, 0.5145486213988072453, 0.5184224789767424368, 0.51453557750154495237, 0.5184067266200898515, 0.5145225399983463874, 0.51839098235455241846, 0.514509508885158146, 0.5183752461751914607, 0.5144964841579296139, 0.5183595180770713637, 0.51448346581261296525, 0.5183437980552595732, 0.51447045384516316074, 0.5183280861048265939, 0.51445744825153794555, 0.5183123822208459874, 0.51444444902769784786, 0.5182966863983943705, 0.5144314561696061761, 0.51828099863255141356, 0.51441846967322901816, 0.51826531891839983844, 0.51440548953453523834, 0.5182496472510254169, 0.5143925157494964766, 0.5182339836255169692, 0.5143795483140871457, 0.5182183280369663615, 0.5143665872242844299, 0.51820268048046850504, 0.51435363247606828273, 0.51818704095112135384, 0.51434068406542142525, 0.5181714094440259028, 0.51432774198832934416, 0.5181557859542861864, 0.5143148062407802899, 0.51814017047700927654, 0.51430187681876527486, 0.51812456300730528133, 0.5142889537182780711, 0.5181089635402873426, 0.51427603693531520905, 0.51809337207107163483, 0.51426312646587597505, 0.51807778859477736285, 0.5142502223059624098, 0.5180622131065267604, 0.51423732445157930665, 0.5180466456014450885, 0.5142244328987342092, 0.5180310860746606333, 0.5142115476434374098, 0.51801553452130470447, 0.5141986686817019474, 0.51799999093651163387, 0.51418579600954360617, 0.5179844553154187732, 0.514172929622980913, 0.51796892765316649256, 0.5141600695180351361, 0.51795340794489817875, 0.5141472156907302829, 0.5179378961857602333, 0.5141343681370930981, 0.51792239237090207114, 0.51412152685315306205, 0.51790689649547611836, 0.5141086918349423887, 0.51789140855463781064, 0.51409586307849602373, 0.5178759285435455918, 0.51408304057985164284, 0.5178604564573609116, 0.51407022433504964974, 0.5178449922912482245, 0.5140574143401331741, 0.51782953604037498753, 0.5140446105911480702, 0.51781408769991165857, 0.5140318130841429146, 0.51779864726503169485, 0.5140190218151690046, 0.51778321473091155127, 0.5140062367802803559, 0.5177677900927306781, 0.5139934579755337015, 0.51775237334567152006, 0.5139806853969884889, 0.51773696448491951375, 0.5139679190407068792, 0.5177215635056630868, 0.5139551589027537446, 0.5177061704030936554, 0.51394240497919666666, 0.5176907851724056228, 0.5139296572661059348, 0.51767540780879637804, 0.51391691575955454364, 0.5176600383074662933, 0.5139041804556181922, 0.5176446766636187231, 0.5138914513503752814, 0.51762932287246000203, 0.5138787284399069121, 0.51761397692919944324, 0.51386601172029688374, 0.5175986388290493366, 0.5138533011876316922, 0.517583308567224947, 0.51384059683800052784, 0.5175679861389445128, 0.513827898667495274, 0.51755267153942924394, 0.51381520667221050483, 0.51753736476390332016, 0.51380252084824348373, 0.5175220658075938895, 0.51378984119169416136, 0.5175067746657310664, 0.5137771676986651736, 0.5174914913335479301, 0.5137645003652618401, 0.51747621580628052295, 0.51375183918759216215, 0.5174609480791678484, 0.5137391841617668211, 0.5174456881474518697, 0.51372653528389917635, 0.517430436006377508, 0.5137138925501052634, 0.5174151916511926406, 0.5137012559565037923, 0.5173999550771480994, 0.51368862549921614574, 0.51738472627949766886, 0.51367600117436637704, 0.51736950525349808476, 0.5136633829780812084, 0.517354291994409032, 0.5136507709064900292, 0.5173390864974931432, 0.5136381649557248941, 0.51732388875801599726, 0.51362556512192052133, 0.51730869877124611685, 0.51361297140121429044, 0.5172935165324549674, 0.51360038378974624113, 0.5172783420369169554, 0.5135878022836590707, 0.51726317527990942616, 0.5135752268790981329, 0.5172480162567126625, 0.5135626575722114358, 0.51723286496260988327, 0.51355009435914963976, 0.517217721392887241, 0.5135375372360660561, 0.5172025855428338208, 0.5135249861991166449, 0.51718745740774163836, 0.51351244124446001336, 0.51717233698290563846, 0.5134999023682574138, 0.5171572242636236928, 0.5134873695666727423, 0.5171421192451965992, 0.51347484283587253623, 0.51712702192292807894, 0.51346232217202597293, 0.51711193229212477554, 0.5134498075713048678, 0.51709685034809625324, 0.5134372990298836725, 0.517081776086154995, 0.51342479654393947296, 0.5170667095016164008, 0.5134123001096519876, 0.5170516505897987862, 0.51339980972320356604, 0.5170365993460233804, 0.51338732538077918655, 0.5170215557656143246, 0.5133748470785664545, 0.5170065198438986707, 0.5133623748127556011, 0.51699149157620637917, 0.5133499085795394807, 0.51697647095787031715, 0.5133374483751135697, 0.5169614579842262574, 0.5133249941956759644, 0.51694645265061287665, 0.51331254603742737947, 0.516931454952371753, 0.51330010389657114553, 0.5169164648848473651, 0.5132876677693132083, 0.51690148244338709034, 0.51327523765186212613, 0.516886507623341203, 0.5132628135404290684, 0.5168715404200628724, 0.51325039543122781376, 0.5168565808289081618, 0.5132379833204747481, 0.51684162884523602606, 0.5132255772043888634, 0.5168266844644083104, 0.51321317707919175514, 0.5168117476817897486, 0.5132007829411076209, 0.51679681849274796134, 0.5131883947863632589, 0.51678189689265345455, 0.5131760126111880655, 0.5167669828768796177, 0.5131636364118140341, 0.516752076440802722, 0.5131512661844757528, 0.5167371775798019191, 0.513138901925410403, 0.51672228628925923906, 0.51312654363085775754, 0.51670740256455958896, 0.51311419129706017884, 0.51669252640109075085, 0.5131018449202626172, 0.5166776577942433804, 0.51308950449671260894, 0.51666279673941100547, 0.5130771700226602747, 0.51664794323199002366, 0.5130648414943583175, 0.51663309726737970145, 0.51305251890806202156, 0.51661825884098217215, 0.51304020226002924963, 0.5166034279482024344, 0.51302789154652044185, 0.5165886045844483502, 0.51301558676379861376, 0.5165737887451306436, 0.51300328790812935474, 0.51655898042566289897, 0.5129909949757808256, 0.5165441796214615592, 0.51297870796302375815, 0.51652938632794592413, 0.51296642686613145174, 0.5165146005405381489, 0.5129541516813797728, 0.5164998222546632424, 0.5129418824050471526, 0.5164850514657490654, 0.51292961903341458533, 0.516470288169226329, 0.5129173615627656266, 0.516455532360528593, 0.51290510998938639166, 0.51644078403509226433, 0.51289286430956555367, 0.51642604318835659517, 0.5128806245195943416, 0.51641130981576368144, 0.51286839061576653884, 0.5163965839127584613, 0.51285616259437848157, 0.51638186547478871313, 0.51284394045172905643, 0.51636715449730505436, 0.5128317241841196994, 0.5163524509757609394, 0.51281951378785439375, 0.5163377549056126583, 0.51280730925923966805, 0.5163230662823193346, 0.512795110594584595, 0.5163083851013429246, 0.51278291779020078917, 0.51629371135814821486, 0.51277073084240240564, 0.5162790450482028209, 0.51275854974750613777, 0.51626438616697718553, 0.51274637450183121605, 0.5162497347099445774, 0.512734205101699406, 0.516235090672581089, 0.51272204154343500637, 0.51622045405036563515, 0.51270988382336484755, 0.51620582483877995155, 0.51269773193781829, 0.51619120303330859296, 0.5126855858831272221, 0.5161765886294389317, 0.5126734456556260587, 0.51616198162266115583, 0.5126613112516517393, 0.5161473820084682677, 0.5126491826675437265, 0.5161327897823560821, 0.51263705989964400374, 0.51611820493982322496, 0.51262494294429707424, 0.51610362747637113126, 0.51261283179784995897, 0.5160890573875040439, 0.51260072645665219455, 0.5160744946687290118, 0.51258862691705583237, 0.5160599393155558883, 0.51257653317541543596, 0.5160453913234973294, 0.51256444522808807993, 0.51603085068806879256, 0.51255236307143334793, 0.51601631740478853447, 0.5125402867018133308, 0.5160017914691776099, 0.51252821611559262527, 0.51598727287675987, 0.512516151309138332, 0.51597276162306196057, 0.5125040922788200537, 0.5159582577036133203, 0.5124920390210098936, 0.51594376111394617946, 0.51247999153208245385, 0.51592927184959555813, 0.51246794980841483354, 0.5159147899060992645, 0.512455913846386627, 0.5159003152789978936, 0.5124438836423799224, 0.5158858479638348251, 0.5124318591927792997, 0.51587138795615622216, 0.51241984049397182907, 0.51585693525151102957, 0.5124078275423470692, 0.51584248984545097265, 0.51239582033429706545, 0.51582805173353055454, 0.5123838188662163483, 0.5158136209113070558, 0.5123718231345019317, 0.51579919737434053206, 0.51235983313555331106, 0.51578478111819381255, 0.51234784886577246183, 0.5157703721384324987, 0.51233587032156383765, 0.5157559704306249624, 0.5123238974993343689, 0.5157415759903423442, 0.5123119303954934604, 0.51572718881315855196, 0.5122999690064529904, 0.51571280889465025924, 0.51228801332862730845, 0.51569843623039690345, 0.5122760633584332339, 0.51568407081598068454, 0.5122641190922900541, 0.5156697126469865633, 0.5122521805266195227, 0.51565536171900225944, 0.512240247657845858, 0.5156410180276182506, 0.51222832048239574145, 0.5156266815684277702, 0.5122163989966983151, 0.515612352337026806, 0.51220448319718518134, 0.5155980303290140989, 0.51219257308029039993, 0.5155837155399911405, 0.51218066864245048703, 0.5155694079655621724, 0.51216876988010441314, 0.5155551076013341839, 0.5121568767896936016, 0.51554081444291691096, 0.51214498936766192686, 0.51552652848592283403, 0.51213310761045571264, 0.51551224972596717703, 0.51212123151452373075, 0.51549797815866790535, 0.51210936107631719867, 0.51548371377964572436, 0.5120974962922897785, 0.5154694565845240781, 0.5120856371588975747, 0.515455206568929147, 0.5120737836725991332, 0.5154409637284898471, 0.51206193582985543884, 0.51542672805883782785, 0.51205009362712991424, 0.515412499555607471, 0.5120382570608884181, 0.5153982782144358886, 0.51202642612759924313, 0.5153840640309629215, 0.51201460082373311493, 0.51536985700083113795, 0.5120027811457631898, 0.51535565711968583187, 0.51199096709016505354, 0.5153414643831750214, 0.5119791586534167194, 0.51532727878694944713, 0.51196735583199862656, 0.51531310032666257043, 0.5119555586223936384, 0.51529892899797057233, 0.51194376702108704096, 0.5152847647965323515, 0.5119319810245665411, 0.51527060771800952283, 0.511920200629322265, 0.5152564577580664157, 0.5119084258318467565, 0.51524231491237007275, 0.5118966566286349751, 0.5152281791765902478, 0.5118848930161842948, 0.51521405054639940496, 0.5118731349909945019, 0.51519992901747271614, 0.5118613825495677939, 0.51518581458548806044, 0.51184963568840877754, 0.5151717072461260218, 0.51183789440402446705, 0.5151576069950698879, 0.5118261586929242825, 0.5151435138280056483, 0.5118144285516200485, 0.5151294277406219931, 0.51180270397662599215, 0.5151153487286103111, 0.51179098496445874145, 0.51510127678766468853, 0.51177927151163732394, 0.51508721191348190736, 0.51176756361468316465, 0.5150731541017614434, 0.51175586127012008466, 0.51505910334820546555, 0.5117441644744742995, 0.51504505964851883326, 0.5117324732242744172, 0.51503102299840909547, 0.51172078751605143713, 0.5150169933935864892, 0.51170910734633874784, 0.5150029708297639376, 0.51169743271167212576, 0.5149889553026570485, 0.51168576360858973357, 0.5149749468079841132, 0.51167410003363211817, 0.5149609453414661043, 0.5116624419833422094, 0.51494695089882667463, 0.5116507894542653185, 0.51493296347579215535, 0.5116391424429491358, 0.51491898306809155456, 0.51162750094594373006, 0.51490500967145655576, 0.51161586495980154596, 0.51489104328162151633, 0.51160423448107740296, 0.51487708389432346585, 0.51159260950632849343, 0.5148631315053021044, 0.51158099003211438113, 0.51484918611029980136, 0.5115693760549969997, 0.5148352477050615937, 0.5115577675715406506, 0.5148213162853351843, 0.51154616457831200195, 0.51480739184687094055, 0.51153456707188008665, 0.5147934743854218926, 0.51152297504881630074, 0.5147795638967437323, 0.51151138850569440183, 0.51476566037659481095, 0.5114998074390905076, 0.5147517638207361381, 0.51148823184558309384, 0.51473787422493138015, 0.51147666172175299307, 0.5147239915849468586, 0.51146509706418339313, 0.5147101158965515486, 0.5114535378694598348, 0.51469624715551707715, 0.51144198413417021117, 0.51468238535761772184, 0.51143043585490476516, 0.51466853049863040936, 0.51141889302825608844, 0.5146546825743347135, 0.51140735565081911945, 0.51464084158051285424, 0.5113958237191911423, 0.5146270075129496955, 0.51138429722997178433, 0.5146131803674327443, 0.51137277617976301537, 0.5145993601397521487, 0.5113612605651691455, 0.51458554682570069647, 0.51134975038279682373, 0.5145717404210738135, 0.51133824562925503626, 0.51455794092166956235, 0.51132674630115510503, 0.51454414832328864056, 0.5113152523951106859, 0.51453036262173437927, 0.5113037639077377672, 0.5145165838128127414, 0.51129228083565466805, 0.5145028118923323205, 0.51128080317548203656, 0.5144890468561043391, 0.5112693309238428487, 0.5144752886999426469, 0.5112578640773624064, 0.51446153741966371936, 0.5112464026326683356, 0.51444779301108665656, 0.5112349465863905855, 0.51443405547003318105, 0.51122349593516142605, 0.51442032479232763676, 0.5112120506756154471, 0.5144066009737969872, 0.51120061080438955605, 0.51439288401027081425, 0.51118917631812297713, 0.5143791738975813162, 0.5111777472134572488, 0.51436547063156330656, 0.51116632348703622317, 0.5143517742080542124, 0.51115490513550606357, 0.5143380846228940728, 0.51114349215551524333, 0.5143244018719255373, 0.5111320845437145443, 0.51431072595099386465, 0.51112068229675705513, 0.5142970568559469208, 0.51110928541129816924, 0.5142833945826351779, 0.5110978938839955841, 0.5142697391269117122, 0.51108650771150929907, 0.51425609048463220315, 0.5110751268905016136, 0.51424244865165493146, 0.51106375141763712647, 0.5142288136238407776, 0.5110523812895827332, 0.5142151853970532204, 0.51104101650300762526, 0.5142015639671583356, 0.51102965705458328803, 0.51418794933002479416, 0.5110183029409834993, 0.51417434148152386063, 0.51100695415888432784, 0.51416074041752939203, 0.5109956107049641319, 0.51414714613391783595, 0.510984272575903557, 0.5141335586265682295, 0.5109729397683855352, 0.5141199778913621971, 0.5109616122790952829, 0.5141064039241839494, 0.5109502901047202996, 0.514092836720920282, 0.51093897324195036594, 0.5140792762774605734, 0.51092766168747754275, 0.5140657225896967839, 0.5109163554379961689, 0.5140521756535234536, 0.51090505449020285993, 0.51403863546483770147, 0.51089375884079650644, 0.51402510201953922377, 0.5108824684864782726, 0.5140115753135302919, 0.5108711834239515947, 0.5139980553427157517, 0.510859903649922179, 0.51398454210300302135, 0.51084862916109800103, 0.5139710355903020902, 0.5108373599541893032, 0.51395753580052551727, 0.51082609602590859385, 0.5139440427295884293, 0.5108148373729706452, 0.51393055637340852, 0.5108035839920924921, 0.51391707672790604773, 0.51079233587999343046, 0.5139036037890038347, 0.5107810930333950155, 0.51389013755262726496, 0.51076985544902106035, 0.51387667801470428324, 0.5107586231235976343, 0.51386322517116539296, 0.51074739605385306143, 0.5138497790179436555, 0.5107361742365179192, 0.513836339550974688, 0.5107249576683250363, 0.5138229067661966623, 0.51071374634600949163, 0.51380948065955030305, 0.51070254026630861266, 0.51379606122697888657, 0.51069133942596197376, 0.51378264846442823924, 0.5106801438217113945, 0.5137692423678467358, 0.5106689534503009387, 0.51375584293318529846, 0.51065776830847691185, 0.51374245015639739435, 0.51064658839298786064, 0.5137290640334390352, 0.51063541370058457085, 0.5137156845602687749, 0.5106242442280200657, 0.5137023117328477087, 0.5106130799720496045, 0.51368894554713947126, 0.51060192092943068137, 0.51367558599911023533, 0.51059076709692302306, 0.5136622330847287103, 0.51057961847128858785, 0.5136488867999661407, 0.5105684750492915641, 0.5136355471407963045, 0.5105573368276983682, 0.5136222141031955119, 0.51054620380327764365, 0.51360888768314260386, 0.51053507597280025894, 0.5135955678766189503, 0.51052395333303930637, 0.5135822546796084489, 0.51051283588077010057, 0.5135689480880975233, 0.51050172361277017653, 0.51355564809807512235, 0.51049061652581928847, 0.51354235470553271747, 0.5104795146166994084, 0.5135290679064643023, 0.51046841788219472384, 0.51351578769686639047, 0.51045732631909163725, 0.5135025140727380143, 0.5104462399241787638, 0.5134892470300807237, 0.51043515869424693033, 0.51347598656489858415, 0.51042408262608917333, 0.51346273267319817547, 0.51041301171650073777, 0.5134494853509885903, 0.5104019459622790753, 0.51343624459428143284, 0.5103908853602238431, 0.5134230103990908169, 0.51037982990713690203, 0.5134097827614333649, 0.51036877959982231514, 0.5133965616773282061, 0.5103577344350863463, 0.5133833471427969751, 0.5103466944097374584, 0.5133701391538638108, 0.5103356595205863122, 0.51335693770655535436, 0.51032462976444576455, 0.51334374279690074815, 0.51031360513813086693, 0.51333055442093163383, 0.51030258563845886386, 0.51331737257468215143, 0.51029157126224919145, 0.5133041972541889375, 0.5102805620063234759, 0.5132910284554911237, 0.5102695578675055321, 0.5132778661746303354, 0.5102585588426213617, 0.51326471040765069025, 0.51024756492849915204, 0.51325156115059879664, 0.5102365761219692745, 0.513238418399523752, 0.5102255924198642828, 0.513225282150477142, 0.5102146138190189117, 0.5132121523995130383, 0.51020364031627007544, 0.51319902914268799763, 0.5101926719084568662, 0.51318591237606106, 0.5101817085924205526, 0.5131728020956937476, 0.5101707503650045781, 0.5131596982976500628, 0.5101597972230545598, 0.51314660097799648733, 0.5101488491634182864, 0.51313351013280198023, 0.51013790618294571725, 0.5131204257581379767, 0.5101269682784889805, 0.5131073478500783868, 0.51011603544690237177, 0.51309427640469959346, 0.5101051076850423524, 0.5130812114180804517, 0.51009418498976754843, 0.51306815288630228637, 0.5100832673579387485, 0.51305510080544889165, 0.5100723547864189028, 0.51304205517160652867, 0.5100614472720731212, 0.5130290159808639246, 0.5100505448117686722, 0.51301598322931227106, 0.51003964740237498106, 0.5130029569130452228, 0.51002875504076362857, 0.51298993702815889585, 0.5100178677238083493, 0.51297692357075186663, 0.51000698544838503015, 0.51296391653692517004, 0.50999610821137170906, 0.51295091592278229797, 0.50998523600964857337, 0.51293792172442919845, 0.5099743688400979584, 0.51292493393797427355, 0.50996350669960434566, 0.5129119525595283783, 0.509952649585054362, 0.512898977585204819, 0.5099417974933367773, 0.51288600901111935183, 0.50993095042134250377, 0.51287304683339018187, 0.50992010836596459377, 0.5128600910481379606, 0.50990927132409823906, 0.5128471416514857859, 0.50989843929264076853, 0.51283419863955919906, 0.5098876122684916472, 0.51282126200848618456, 0.50987679024855247476, 0.5128083317543971679, 0.5098659732297269839, 0.5127954078734250146, 0.50985516120892103876, 0.5127824903617050284, 0.50984435418304263377, 0.5127695792153749503, 0.5098335521490018919, 0.5127566744305749563, 0.50982275510371106314, 0.512743776003447657, 0.5098119630440845233, 0.5127308839301380952, 0.5098011759670387723, 0.51271799820679374516, 0.5097903938694924329, 0.5127051188295645108, 0.50977961674836624894, 0.51269224579460272433, 0.5097688446005830841, 0.5126793790980631448, 0.50975807742306792014, 0.51266651873610295695, 0.509747315212747856, 0.5126536647048817693, 0.5097365579665521058, 0.512640817000561613, 0.50972580568141199743, 0.51262797561930694036, 0.50971505835426097146, 0.5126151405572846234, 0.50970431598203457896, 0.51260231181066395235, 0.5096935785616704809, 0.51258948937561663446, 0.5096828460901084461, 0.51257667324831679217, 0.50967211856429034986, 0.51256386342494096207, 0.50966139598116017256, 0.5125510599016680933, 0.5096506783376639985, 0.5125382626746795462, 0.5096399656307500135, 0.5125254717401590905, 0.50962925785736850465, 0.5125126870942929044, 0.5096185550144718581, 0.5124999087332695732, 0.5096078570990145577, 0.51248713665328008717, 0.50959716410795318365, 0.512474370850517841, 0.50958647603824641095, 0.5124616113211786315, 0.5095757928868550081, 0.51244885806146065713, 0.50956511465074183563, 0.5124361110675645157, 0.5095544413268718442, 0.5124233703356932035, 0.50954377291221207384, 0.5124106358620521137, 0.5095331094037316521, 0.51239790764284903497, 0.50952245079840179255, 0.51238518567429414976, 0.50951179709319579337, 0.5123724699526000334, 0.5095011482850890363, 0.51235976047398165256, 0.5094905043710589846, 0.5123470572346563634, 0.50947986534808518196, 0.5123343602308439106, 0.50946923121314925074, 0.51232166945876642566, 0.50945860196323489107, 0.51230898491464842577, 0.50944797759532787877, 0.5122963065947168124, 0.5094373581064160645, 0.5122836344952008692, 0.5094267434934893719, 0.51227096861233226187, 0.5094161337535397961, 0.51225830894234503527, 0.5094055288835614026, 0.51224565548147561316, 0.5093949288805503258, 0.51223300822596279624, 0.5093843337415047672, 0.5122203671720477608, 0.5093737434634249943, 0.5122077323159740575, 0.50936315804331333914, 0.5121951036539876098, 0.5093525774781741968, 0.5121824811823367125, 0.50934200176501402376, 0.51216986489727203057, 0.5093314309008413368, 0.51215725479504659745, 0.50932086488266671147, 0.5121446508719158138, 0.5093103037075027805, 0.51213205312413744605, 0.50929974737236423236, 0.5121194615479716251, 0.50928919587426781026, 0.5121068761396808449, 0.50927864921023231027, 0.5120942968955299609, 0.50926810737727857983, 0.5120817238117861885, 0.50925757037242951676, 0.5120691568847191022, 0.50924703819271006753, 0.5120565961106006338, 0.5092365108351472258, 0.512044041485705071, 0.50922598829677003125, 0.5120314930063090562, 0.50921547057460956774, 0.5120189506686915847, 0.50920495766569896253, 0.51200641446913400397, 0.509194449567073384, 0.51199388440392001155, 0.5091839462757700411, 0.51198136046933565403, 0.50917344778882818115, 0.51196884266166932566, 0.5091629541032890891, 0.51195633097721176673, 0.50915246521619608565, 0.5119438254122560625, 0.50914198112459452603, 0.51193132596309764157, 0.50913150182553179856, 0.51191883262603427436, 0.5091210273160573231, 0.5119063453973660721, 0.5091105575932225499, 0.51189386427339548526, 0.5091000926540809578, 0.511881389250427302, 0.5090896324956880533, 0.511868920324768647, 0.5090791771151013688, 0.5118564574927289798, 0.50906872650938046114, 0.51184400075062009395, 0.5090582806755869105, 0.51183155009475611477, 0.50904783961078431886, 0.5118191055214534989, 0.50903740331203830834, 0.5118066670270310324, 0.5090269717764165201, 0.51179423460780982906, 0.509016545000988613, 0.51178180826011332975, 0.50900612298282626176, 0.5117693879802673007, 0.5089957057190031561, 0.5117569737645998316, 0.50898529320659499864, 0.51174456560944133534, 0.5089748854426795043, 0.51173216351112454535, 0.50896448242433639845, 0.51171976746598451554, 0.50895408414864741535, 0.51170737747035861744, 0.5089436906126962973, 0.5116949935205865404, 0.5089333018135687927, 0.5116826156130102887, 0.5089229177483526547, 0.51167024374397418137, 0.5089125384141376403, 0.51165787790982485004, 0.50890216380801550855, 0.51164551810691123816, 0.5088917939270800189, 0.5116331643315845989, 0.5088814287684269306, 0.5116208165801984945, 0.50887106832915400035, 0.5116084748491087943, 0.50886071260636098174, 0.5115961391346736739, 0.50885036159714962337, 0.51158380943325361336, 0.5088400152986236675, 0.51157148574121139603, 0.50882967370788884895, 0.511559168054912107, 0.5088193368220528933, 0.51154685637072313213, 0.50880900463822551584, 0.51153455068501415615, 0.50879867715351841996, 0.51152225099415716164, 0.508788354365045296, 0.5115099572945264274, 0.5087780362699218194, 0.51149766958249852773, 0.50876772286526565013, 0.51148538785445232986, 0.5087574141481964303, 0.51147311210676899376, 0.5087471101158357836, 0.51146084233583197025, 0.50873681076530731363, 0.5114485785380269995, 0.5087265160937366021, 0.51143632070974211, 0.5087162260982512082, 0.5114240688473676169, 0.5087059407759806668, 0.5114118229472961209, 0.5086956601240564869, 0.5113995830059225067, 0.5086853841396121509, 0.51138734901964394186, 0.50867511281978311245, 0.5113751209848598749, 0.50866484616170679544, 0.51136289889797203475, 0.50865458416252259277, 0.5113506827553844285, 0.5086443268193718648, 0.51133847255350334097, 0.50863407412939793783, 0.51132626828873733245, 0.508623826089746103, 0.5113140699574972379, 0.50861358269756361483, 0.5113018775561961655, 0.5086033439499996896, 0.51128969108124949517, 0.5085931098442055047, 0.51127751052907487726, 0.50858288037733419626, 0.5112653358960922313, 0.5085726555465408584, 0.51125316717872374447, 0.50856243534898254183, 0.5112410043733938703, 0.5085522197818182526, 0.51122884747652932736, 0.50854200884220895006, 0.5112166964845590979, 0.5085318025273175464, 0.51120455139391442645, 0.50852160083430890457, 0.5111924122010288185, 0.5085114037603498374, 0.51118027890233803896, 0.50850121130260910585, 0.5111681514942801115, 0.5084910234582574181, 0.5111560299732953161, 0.5084808402244674277, 0.51114391433582618857, 0.50847066159841373235, 0.51113180457831751885, 0.5084604875772728731, 0.5111197006972163498, 0.50845031815822333185, 0.5111076026889719755, 0.5084401533384455313, 0.5110955105500359406, 0.5084299931151218326, 0.51108342427686203805, 0.5084198374854365343, 0.51107134386590630877, 0.5084096864465758713, 0.51105926931362703926, 0.5083995399957280132, 0.5110472006164847612, 0.5083893981300830628, 0.5110351377709422494, 0.5083792608468330552, 0.5110230807734645209, 0.5083691281431719559, 0.5110110296205188333, 0.5083590000162956599, 0.51099898430857468366, 0.50834887646340199026, 0.5109869448341038071, 0.5083387574816906964, 0.5109749111935801754, 0.5083286430683634535, 0.51096288338347999557, 0.50831853322062386035, 0.5109508614002817089, 0.5083084279356774383, 0.51093884524046598907, 0.50829832721073163015, 0.51092683490051574125, 0.50828823104299579867, 0.51091483037691610053, 0.50827813942968122485, 0.51090283166615443064, 0.50826805236800110725, 0.5108908387647203227, 0.5082579698551705602, 0.5108788516691055937, 0.5082478918884066126, 0.5108668703758042853, 0.5082378184649282065, 0.5108548948813126626, 0.5082277495819561959, 0.5108429251821292124, 0.5082176852367133452, 0.51083096127475464233, 0.5082076254264243281, 0.5108190031556918793, 0.50819757014831572634, 0.5108070508214460681, 0.5081875193996160277, 0.5107951042685245703, 0.5081774731775556257, 0.51078316349343696253, 0.5081674314793668173, 0.5107712284926950357, 0.5081573943022838022, 0.5107592992628127931, 0.5081473616435426812, 0.5107473758003064495, 0.5081373335003814552, 0.5107354581016944296, 0.50812730987004002314, 0.51072354616349736667, 0.5081172907497601816, 0.5107116399822381014, 0.50810727613678562293, 0.51069973955444168055, 0.50809726602836193394, 0.5106878448766353554, 0.5080872604217365948, 0.51067595594534858066, 0.5080772593141589773, 0.51066407275711301304, 0.50806726270288034427, 0.5106521953084625099, 0.50805727058515384725, 0.51064032359593312813, 0.5080472829582345261, 0.51062845761606312245, 0.50803729981937930716, 0.51061659736539294455, 0.508027321165847002, 0.5106047428404652413, 0.5080173469948983063, 0.5105928940378248538, 0.50800737730379579807, 0.51058105095401881587, 0.50799741208980393704, 0.51056921358559635287, 0.50798745135018906276, 0.51055738192910888007, 0.5079774950822193933, 0.51054555598111000164, 0.50796754328316502447, 0.5105337357381555094, 0.5079575959502979278, 0.51052192119680338126, 0.50794765308089194977, 0.51051011235361377975, 0.50793771467222281014, 0.5104983092051490512, 0.50792778072156810105, 0.51048651174797372425, 0.5079178512262072851, 0.5104747199786545082, 0.50790792618342169473, 0.51046293389376029196, 0.5078980055904945302, 0.51045115348986214285, 0.50788808944471085894, 0.51043937876353330524, 0.5078781777433576138, 0.51042760971134919885, 0.507868270483723592, 0.5104158463298874181, 0.5078583676630994536, 0.5104040886157277301, 0.5078484692787777203, 0.510392336565452074, 0.5078385753280527744, 0.51038059017564455926, 0.5078286858082208569, 0.5103688494428914643, 0.50781880071658006665, 0.51035711436378123555, 0.50780892005043035896, 0.51034538493490448575, 0.5077990438070735444, 0.5103336611528539931, 0.50778917198381328725, 0.51032194301422469925, 0.5077793045779551042, 0.51031023051561370883, 0.5077694415868063633, 0.5102985236536202876, 0.5077595830076762826, 0.51028682242484586106, 0.50774972883787592874, 0.51027512682589401386, 0.5077398790747182156, 0.5102634368533704876, 0.5077300337155179032, 0.51025175250388317996, 0.50772019275759159646, 0.5102400737740421437, 0.50771035619825774336, 0.51022840066045958455, 0.5077005240348366344, 0.5102167331597498608, 0.5076906962646504009, 0.5102050712685294814, 0.50768087288502301363, 0.51019341498341710494, 0.50767105389328028166, 0.5101817643010335382, 0.50766123928674985127, 0.51017011921800173486, 0.50765142906276120423, 0.51015847973094679445, 0.5076416232186456568, 0.5101468458364959608, 0.50763182175173635844, 0.5101352175312786206, 0.50762202465936829036, 0.5101235948119263026, 0.50761223193887826437, 0.51011197767507267584, 0.5076024435876049217, 0.5101003661173535486, 0.50759265960288873137, 0.51008876013540686714, 0.50758287998207198927, 0.51007715972587271423, 0.5075731047224988166, 0.5100655648853933079, 0.50756333382151515884, 0.5100539756106130003, 0.50755356727646878424, 0.5100423918981782762, 0.50754380508470928277, 0.5100308137447377519, 0.5075340472435880645, 0.51001924114694217383, 0.5075242937504583588, 0.5100076741014444173, 0.50751454460267521253, 0.509996112604899485, 0.5075047997975954893, 0.50998455665396450625, 0.50749505933257786774, 0.5099730062452987349, 0.50748532320498284043, 0.5099614613755635489, 0.5074755914121727127, 0.5099499220414224485, 0.50746586395151160127, 0.5099383882395410551, 0.5074561408203654329, 0.5099268599665871098, 0.50744642201610194325, 0.50991533721923047257, 0.5074367075360906755, 0.50990381999414312043, 0.50742699737770297915, 0.5098923082879991464, 0.50741729153831200886, 0.50988080209747475835, 0.50740759001529272285, 0.5098693014192482775, 0.507397892806021882, 0.50985780625000013734, 0.50738819990787804826, 0.50984631658641288204, 0.50737851131824158394, 0.5098348324251711656, 0.50736882703449464954, 0.5098233537629617502, 0.5073591470540212033, 0.50981188059647350507, 0.5073494713742069996, 0.5098004129223974053, 0.5073397999924395878, 0.5097889507374265304, 0.5073301329061083107, 0.50977749403825606305, 0.5073204701126043039, 0.50976604282158328795, 0.5073108116093204936, 0.5097545970841075905, 0.5073011573936515963, 0.50974315682253045547, 0.50729150746299411697, 0.5097317220335554655, 0.5072818618147463479, 0.50972029271388830053, 0.50727222044630836773, 0.50970886886023673574, 0.5072625833550820396, 0.5096974504693106405, 0.5072529505384710105, 0.5096860375378219777, 0.50724332199388070955, 0.5096746300624848016, 0.5072336977187183473, 0.50966322804001525706, 0.5072240777103929138, 0.50965183146713157803, 0.50721446196631517797, 0.50964044034055408673, 0.50720485048389768576, 0.5096290546570051917, 0.5071952432605547594, 0.50961767441320938706, 0.5071856402937024961, 0.5096062996058932512, 0.5071760415807587664, 0.5095949302317854453, 0.5071664471191432133, 0.50958356628761671204, 0.50715685690627725096, 0.5095722077701198746, 0.50714727093958406317, 0.50956085467602983517, 0.5071376892164886026, 0.50954950700208357403, 0.5071281117344175892, 0.50953816474502014737, 0.5071185384907995089, 0.50952682790158068747, 0.5071089694830646128, 0.5095154964685084002, 0.50709940470864491534, 0.5095041704425485642, 0.5070898441649741935, 0.50949284982044852995, 0.50708028784948798543, 0.5094815345989577179, 0.50707073575962358927, 0.50947022477482761755, 0.50706118789282006155, 0.50945892034481178617, 0.5070516442465182166, 0.5094476213056658476, 0.5070421048181606248, 0.50943632765414749073, 0.5070325696051916115, 0.50942503938701646853, 0.50702303860505725576, 0.5094137565010345968, 0.50701351181520538925, 0.50940247899296575273, 0.5070039892330855948, 0.5093912068595758735, 0.50699447085614920543, 0.5093799400976329555, 0.5069849566818493027, 0.5093686787039070528, 0.5069754467076407161, 0.5093574226751702758, 0.50696594093098002116, 0.5093461720081967901, 0.5069564393493255387, 0.5093349266997628154, 0.5069469419601373334, 0.50932368674664662406, 0.50693744876087721256, 0.50931245214562853964, 0.50692795974900872486, 0.5093012228934909362, 0.5069184749219971595, 0.50928999898701823656, 0.5069089942773095442, 0.50927878042299691135, 0.50689951781241464487, 0.50926756719821547755, 0.5068900455247829637, 0.50925635930946449735, 0.5068805774118867382, 0.50924515675353657703, 0.50687111347119994005, 0.50923395952722636556, 0.5068616537001982739, 0.509222767627330553, 0.50685219809635917584, 0.50921158105064787017, 0.50684274665716181257, 0.50920039979397908663, 0.50683329938008707985, 0.50918922385412700957, 0.5068238562626176016, 0.50917805322789648286, 0.5068144173022377284, 0.50916688791209438556, 0.50680498249643353637, 0.5091557279035296306, 0.5067955518426928262, 0.50914457319901316394, 0.5067861253385051214, 0.50913342379535796276, 0.50677670298136166766, 0.50912227968937903477, 0.50676728476875543126, 0.50911114087789341665, 0.5067578706981810978, 0.5091000073577201728, 0.50674846076713507163, 0.5090888791256803943, 0.50673905497311547375, 0.50907775617859719777, 0.5067296533136221412, 0.50906663851329572333, 0.5067202557861566256, 0.50905552612660313465, 0.50671086238822219213, 0.5090444190153486166, 0.50670147311732381787, 0.50903331717636337444, 0.5066920879709681917, 0.5090222206064806328, 0.50668270694666371135, 0.50901112930253563423, 0.50667333004192048395, 0.50900004326136563784, 0.50666395725425032357, 0.5089889624798099182, 0.50665458858116675076, 0.50897788695470976423, 0.5066452240201849909, 0.50896681668290847777, 0.5066358635688219735, 0.5089557516612513725, 0.50662650722459633015, 0.50894469188658577236, 0.50661715498502839434, 0.50893363735576101106, 0.5066078068476401995, 0.5089225880656284301, 0.50659846280995547816, 0.50891154401304137785, 0.5065891228694996607, 0.5089005051948552084, 0.506579787023799874, 0.50888947160792728016, 0.5065704552703849404, 0.5088784432491169548, 0.5065611276067853767, 0.50886742011528559584, 0.50655180403053339227, 0.50885640220329656755, 0.5065424845391628887, 0.5088453895100152338, 0.50653316913020945815, 0.50883438203230895667, 0.506523857801210382, 0.5088233797670470953, 0.50651455054970463015, 0.50881238271110100453, 0.5065052473732328595, 0.5088013908613440341, 0.5064959482693374128, 0.508790404214651527, 0.50648665323556231753, 0.5087794227679008182, 0.5064773622694532845, 0.50876844651797123394, 0.5064680753685577072, 0.50875747546174409004, 0.5064587925304246599, 0.50874650959610269083, 0.5064495137526048969, 0.5087355489179323279, 0.5064402390326508513, 0.508724593424120279, 0.5064309683681166338, 0.5087136431115558068, 0.50642170175655803143, 0.50870269797713015735, 0.50641243919553250645, 0.5086917580177365594, 0.50640318068259919523, 0.508680823230270223, 0.5063939262153189067, 0.5086698936116283378, 0.50638467579125412177, 0.50865896915871007257, 0.50637542940796899176, 0.5086480498684165735, 0.5063661870630293371, 0.50863713573765096315, 0.5063569487540026465]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the test dataset"
      ],
      "metadata": {
        "id": "RD84h5J-aZTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db2=pipeline(test_df)\n",
        "db2=db2.fillna(dataset.mean(numeric_only=True))\n",
        "db2=db2.to_numpy()"
      ],
      "metadata": {
        "id": "uBG5ISJgOUOa"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y3_cap=np.dot(db2,w)\n",
        "h3=sigmoid(y3_cap)\n",
        "h3 = np.where(h3 > 0.5, 1, 0)\n",
        "\n"
      ],
      "metadata": {
        "id": "EvWLU1LpOzl9"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(h3)"
      ],
      "metadata": {
        "id": "WxMyoMHuO0x1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f29d86e-0106-4810-bc24-25e836118a2c"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['Survived']=h3"
      ],
      "metadata": {
        "id": "U0Z_TTGsP1NR"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5wXZ-ZGUYv7",
        "outputId": "58e7a90e-b803-456c-e606-e57ae0fdf912"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     PassengerId  Pclass                                          Name  \\\n",
            "0            892       3                              Kelly, Mr. James   \n",
            "1            893       3              Wilkes, Mrs. James (Ellen Needs)   \n",
            "2            894       2                     Myles, Mr. Thomas Francis   \n",
            "3            895       3                              Wirz, Mr. Albert   \n",
            "4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
            "..           ...     ...                                           ...   \n",
            "413         1305       3                            Spector, Mr. Woolf   \n",
            "414         1306       1                  Oliva y Ocana, Dona. Fermina   \n",
            "415         1307       3                  Saether, Mr. Simon Sivertsen   \n",
            "416         1308       3                           Ware, Mr. Frederick   \n",
            "417         1309       3                      Peter, Master. Michael J   \n",
            "\n",
            "        Sex   Age  SibSp  Parch              Ticket      Fare Cabin Embarked  \\\n",
            "0      male  34.5      0      0              330911    7.8292   NaN        Q   \n",
            "1    female  47.0      1      0              363272    7.0000   NaN        S   \n",
            "2      male  62.0      0      0              240276    9.6875   NaN        Q   \n",
            "3      male  27.0      0      0              315154    8.6625   NaN        S   \n",
            "4    female  22.0      1      1             3101298   12.2875   NaN        S   \n",
            "..      ...   ...    ...    ...                 ...       ...   ...      ...   \n",
            "413    male   NaN      0      0           A.5. 3236    8.0500   NaN        S   \n",
            "414  female  39.0      0      0            PC 17758  108.9000  C105        C   \n",
            "415    male  38.5      0      0  SOTON/O.Q. 3101262    7.2500   NaN        S   \n",
            "416    male   NaN      0      0              359309    8.0500   NaN        S   \n",
            "417    male   NaN      1      1                2668   22.3583   NaN        C   \n",
            "\n",
            "     Survived  \n",
            "0           0  \n",
            "1           0  \n",
            "2           0  \n",
            "3           0  \n",
            "4           0  \n",
            "..        ...  \n",
            "413         0  \n",
            "414         1  \n",
            "415         0  \n",
            "416         0  \n",
            "417         0  \n",
            "\n",
            "[418 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv('predictions.csv')"
      ],
      "metadata": {
        "id": "wbx_fn6kUalN"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"predictions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_aK7xhV1YqUP",
        "outputId": "883a9981-fc47-4298-a6b5-502fa846cb18"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_de94f557-27b9-46d2-9afe-4d350de59db4\", \"predictions\", 31378)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "50s2M22lZZMF"
      },
      "execution_count": 102,
      "outputs": []
    }
  ]
}